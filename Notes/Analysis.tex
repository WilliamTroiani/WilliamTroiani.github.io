\documentclass[12pt]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{units}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{nicefrac}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{color}
\usepackage{tensor}
\usepackage{tipa}
\usepackage{bussproofs}
\usepackage{ stmaryrd }
\usepackage{ textcomp }
\usepackage{leftidx}
\usepackage{afterpage}
\usepackage{varwidth}

\newcommand\blankpage{
    \null
    \thispagestyle{empty}
    \addtocounter{page}{-1}
    \newpage
    }

\graphicspath{ {images/} }

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[subsection] % reset theorem numbering for each chapter
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{fact}[thm]{Fact}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers
\newtheorem{notation}[thm]{Notation}
\newtheorem{remark}[thm]{Remark}
\newtheorem{condition}[thm]{Condition}
\newtheorem{question}[thm]{Question}
\newtheorem{construction}[thm]{Construction}
\newtheorem{exercise}[thm]{Exercise}
\newtheorem{example}[thm]{Example}
\newtheorem{axiom}[thm]{Axiom}
\newtheorem{observation}[thm]{Observation}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\scr}[1]{\mathscr{#1}}
\newcommand{\call}[1]{\mathcal{#1}}
\newcommand{\psheaf}{\text{\underline{Set}}^{\scr{C}^{\text{op}}}}
\newcommand{\und}[1]{\underline{\hspace{#1 cm}}}
\newcommand{\adj}[1]{\text{\textopencorner}{#1}\text{\textcorner}}
\newcommand{\comment}[1]{}
\newcommand{\lto}{\longrightarrow}
\def\doubleunderline#1{\underline{\underline{#1}}}

\usepackage[margin=1cm]{geometry}

\title{Analysis}
\author{Will Troiani}
\date{August 2020}

\begin{document}

\maketitle
\tableofcontents

\section{Elementary facts}
\begin{defn}
The \textbf{real numbers}, denoted $\bb{R}$, are equivalence classes of Cauchy sequences. 
\end{defn}
\begin{remark}
It can be shown that $\bb{R}$ satisfies the axioms for the real numbers, thus equivalence classes of Cauchy sequences is an appropriate model for the real numbers. Another example is given by Dedekind cuts.
\end{remark}
Throughout we work with the space $\bb{C}$, but the proofs can be amalgamated easily for the real case.
\begin{proposition}
Let $(z_1,z_2,...),(w_1,w_2,...)$ be convergent sequences of complex numbers, then their sum $(z_1 + w_1,z_2 + w_2,...)$, any scalar product $(kz_1,kz_2,...)$, and their product $(z_1w_1,z_2w_2,...)$ are also convergent. 
\end{proposition}
\begin{proof}
Say $z_n\stackrel{n \to \infty}{\lto} c_1$ and $w_n \stackrel{n \to \infty}{\lto} c_2$. Then for $\epsilon > 0$ there exists $N \in \bb{N}$ such that for $n > N$ we have $|z_n - c_1| < \epsilon/2, |w_n - c_2| < \epsilon/2$. Thus
\begin{equation}
|z_n + w_n - (c_1 + c_2)| \leq |z_n - c_1| + |w_n - c_2| \leq \epsilon/2 + \epsilon/2 = \epsilon
\end{equation}
Next, there exists $N>0$ such that for all $n > N$ we have $|z_n - c_1| < \epsilon/|k|$, the second claim follows.

Lastly, there exists $N \in \bb{N}$ such that for all $n > N$ we have
\begin{equation}
|a_n - c_1| < \frac{\epsilon}{2(|c_1| + 1)},\quad\text{and}\quad |b_n -c_2| < \frac{\epsilon}{2(|c_2| + 1)}
\end{equation}
Also, $N$ can be taken so that for $n > N$ we have
\begin{equation}
|z_n| - |c_1| \leq ||z_n| - |c_1|| \leq |z_n - c_1| < 1
\end{equation}
so that
\begin{equation}
|z_n|/(|c_1| + 1) < 1
\end{equation}
Thus,
\begin{align*}
|z_nw_n - c_1c_2| &= |z_nw_n + z_nc_2 - z_nc_2 - c_1c_2|\\
&\leq |z_n||w_n - c_2| + |c_2||z_n - c_1|\\
&\leq \epsilon/2 + \epsilon/2\\
&= \epsilon
\end{align*}
\end{proof}
In other words, convergent sequences forms a $\bb{C}$-algebra.
\begin{cor}\label{cor:sum_convergen_sums}
If $\sum_{i = 1}^\infty z_i$ and $\sum_{i = 1}^\infty w_j$ converge, then so do (for any $k \in \bb{C}$)
\begin{equation}
\sum_{i = 1}^\infty (z_i + w_i),\quad\sum_{i = 1}^\infty kz_i,\quad \text{and}\quad \sum_{i = 1}^\infty z_iw_i
\end{equation}
\end{cor}


\section{Taylor's Theorem}
\subsection{Taylor's Theorem for real analysis}
This Theorem can be found by pressing on the \emph{Intermediate Value Theorem}, which itself is a result of pressing on the completeness property of the real numbers:
\begin{axiom}[Completeness Axiom]
Every non-empty, bounded subset of $\bb{R}$ admits a supremum and an infimum.
\end{axiom}
\begin{defn}
A subset $E \subseteq \bb{R}$ is \textbf{connected} if it cannot be written as the union of two disjoint, open subsets.
\end{defn}
\begin{lemma}
\label{lem:connectedness}
A subset $E \subseteq \bb{R}$ is connected if and only if it satisfies the following property: if $x < y \in E$ and $z \in \bb{R}$ is such that $x < z < y$ then $z \in E$.
\end{lemma}
\begin{proof}
Say $E$ is not connected, say $E = A \cup B$ with $A,B$ disjoint, open subsets of $\bb{R}$ and assume without loss of generality that every element of $B$ is an upper bound of $A$. Let $x \in A, y \in B$ and consider
\[z := \sup (A \cap [x,y])\]
Then $z$ is not in $E$ as if $z \in A$ then $A$ being open implies there exists $\epsilon > 0$ such that $B_\epsilon(z) \subseteq A$, which implies $z < z + \epsilon/2 \in A \cap [x,y]$ contradicting that $z$ is an upper bound. If $z \in B$ then there exists $\epsilon > 0$ such that $z > z - \epsilon/2 \in B$ with $z - \epsilon/2 \not\in A$, contradicting that $z$ is the least upper bound of $A \cap [x,y]$.

Conversely, let $x < z < y$ be such that $x \in E, y \in E, z \not\in E$, then $E = \big((-\infty, z) \cap E\big) \cup \big(E \cap (z,\infty)\big)$.
\end{proof}
\begin{lemma}
\label{lem:connected_continuous}
Let $f: A \lto \bb{R}$ be a continuous function where $A \subseteq \bb{R}$ is any set (the following holds if $A$ is any metric space). Then $A$ connected implies $f(A)$ is connected.
\end{lemma}
\begin{proof}
Say $f(A)$ is not connected, so $f(A) = B \cup C$ with $B,C \supseteq \bb{R}$ open subsets. Then $A = f^{-1}(B) \cup f^{-1}(C)$.
\end{proof}
\begin{thm}[Intermediate Value Theorem]
Let $f: [a,b] \lto \bb{R}$ be a continuous function satisfying $f(a) \leq f(b)$. Then for all $u \in [f(a),f(b)]$ there exists $c \in [a,b]$ such that $f(c) = u$.
\end{thm}
\begin{proof}
By Lemma \ref{lem:connected_continuous} we have that $f([a,b])$ is connected, so the result follows from Lemma \ref{lem:connectedness}.
\end{proof}
\begin{cor}[Mean Value Theorem]
Let $f: [a,b] \lto \bb{R}$ be a function which is differentiable for all $t \in (a,b)$. Then there exists $c \in (a,b)$ such that
\[f'(x) = \frac{f(b) - f(a)}{b-a}\]
\end{cor}
\begin{proof}
\cite{rudin}.
\end{proof}
\begin{thm}[Taylor's Theorem]
Let $f: [a,b] \lto \bb{R}$ be $n$ times differentiable in the interval $(a,b)$, and assume $f^{(n-1)}$ is continuous on some closed interval $[\alpha,\beta] \subseteq [a,b]$. Define:
\[P_n(t,\alpha) := \sum_{m = 0}^{n-1} \frac{f^{(m)}(\alpha)}{m!}(t - \alpha)^m\]
Then there exists $\gamma \in (\alpha,\beta)$ such that
\[f(\beta) = P_{n}(\beta,\alpha) + \frac{f^{(n)}(\gamma)}{n!}(\beta - \alpha)^n\]
\end{thm}
\begin{proof}
Let $M$ be such that
\[f(\beta) = P_n(\beta,\alpha) + M(\beta - \alpha)^n\]
and define
\[g(t) = f(t) - P_n(t,\alpha) - M(t - \alpha)^n\]
so that
\[g^{(n)}(t) = f^{(n)}(t) - n!M\]
Notice that $g^{(n)}(\alpha) = 0$. The last step is to prove by induction on $n$ that there exists $x_n \in (\alpha, \beta)$ such that $g^{(n)}(x_n) = 0$. If $n = 1$ then $g^{(1)}(\alpha) = g'(\alpha) = f'(\alpha) - f'(\alpha) = 0$. Also, $g(\beta) = 0$ by the defining property of $M$ so there exists $x_1 \in (\alpha,\beta)$ such that $g'(x_1) = 0$. Now assume $n > 1$ and the result holds for $n - 1$. Then since there exists $x_{n-1}$ such that $g^{(n-1)}(x_{n-1}) = 0$ we have by the mean value theorem that there exists $x_n \in (\alpha,\beta)$ such that $g^{(n)}(x_n) = 0$.
\end{proof}
\begin{defn}
Given a function $f: [a,b] \lto \bb{R}$ and real number $\alpha \in (a,b)$ the function $P_n(t,\alpha)$ is the \textbf{$n^{\operatorname{th}}$ order Taylor polynomial about $\alpha$}. (Notice the notation is a bit naughty as we suppress the function $f$). 
\end{defn}
\begin{remark}
If a function $f: [a,b] \lto \bb{R}$ can be written as a power series about $\alpha \in (a,b)$:
\[f(x) = \sum_{k = 0}^\infty c_n(x - \alpha)^k\]
then the sequence of Taylor polynomials about $\alpha$ corresponding to $f$, $(P_1(x,\alpha),P_2(x,\alpha),\hdots)$ converges to $f(x)$.
\end{remark}

\section{Hilbert spaces}
\begin{defn}
A \textbf{Hilbert space} is an inner product space which is complete with respect to the norm generated by its inner product.
\end{defn}
\subsection{The space $\ell^2$}
\begin{defn}
Let $\ell^2$ denote the Hilbert product space of \textbf{square summable sequences}, that is, sequences $(z_1,z_2,...)$ of complex numbers such that
\begin{equation}
\sum_{i = 1}^\infty |z_i| < \infty
\end{equation}
Define an inner product on this space by
\begin{equation}
\big\langle (z_1,z_2,...),(w_1,w_2,...)\big\rangle = \sum_{i = 1}^\infty z_i\bar{w}_i
\end{equation}
\end{defn}
We check this is in fact a Hilbert space:
\begin{proof}
We have already seen that $\ell^2$ is a vector space (Corollary \ref{cor:sum_convergen_sums}).  That we have a valid inner product is clear, so it remains to show that this space is complete with respect to this inner product.

Say we had a Cauchy sequence $\big((z_n^1),(z_n^2),...\big)$ consisting of elements in $\ell^2$.  Consider for each $i$ the sequence $(z_i^1,z_i^2,...)$, which we claim is a Cauchy sequence in $\bb{C}$. This is easy to establish as for all $\epsilon > 0$ there exists $N > 0$ such that for $m,k > N$ we have
\begin{equation}
\Big(\sum_{n = 1}^\infty|z_n^m - z_n^k|\Big)^{1/2} < \sqrt{\epsilon}
\end{equation}
and clearly
\begin{equation}
|z_n^m - z_i^k| \leq \sum_{n = 1}^\infty|z_n^m - z_n^k|
\end{equation}
Since $\bb{C}$ is complete, we thus have $(z_i^1,z_i^2,...)$ converges to some complex number, $c_i$ say.

We now claim that $\big((z_n^1),(z_n^2),...\big) \lto (c_1,c_2,...)$.

Let $\epsilon > 0$ and $N > 0$ such that for all $m,k > N$ we have $||(z_n^m) - (z_n^k)|| < \epsilon/\sqrt{2}$. Fix $M > 0$ and consider the finite sum
\begin{equation}
\sum_{n = 1}^M |z_n^m - c_n|^2
\end{equation}
We pick $l$ such that $|z_n^m - z_n^l| < \epsilon/\sqrt{2M}$ for all $n = 1,...,M$. Then we have
\begin{align*}
\sum_{n = 1}^M |z_n^m - c_n|^2 &= \sum_{n = 1}^M |z_n^m + z_n^l - z_n^l - c_n|^2\\
&\leq \sum_{n = 1}^M |z_n^m - z_n^l|^2 + \sum_{n = 1}^M |z_n^l - c_n|^2\\
&< \epsilon^2/2M + \epsilon^2/2\\
&= \epsilon^2
\end{align*}
Since this holds true for arbitrary $M$, we have
\begin{equation}
||z_n^m - c_n||_2 < \epsilon
\end{equation}
proving the claim.
\end{proof}






\section{Operator Theory}
\subsection{Adjoint operator}
We will be chiefly concerned with the Hilbert space $\ell^2$ but we work in a more general setting for now. A \emph{Hilbert space} will always mean over $\bb{C}$. Associated to every operator between Hilbert spaces is an operator between their \emph{dual spaces}:

In general, if $\call{I}$ is any inner product space over $\bb{C}$ and we have two vectors $x,y \in I$ then we can consider the projection of $y$ onto $x$ which is given by
\begin{equation}\label{eq:easy_case}
    \operatorname{Proj}_y(x) := \frac{\langle x, y \rangle}{||y||} \frac{y}{||y||}
\end{equation}
Thus, if $U \subseteq \call{I}$ is a one dimensional subspace spanned by a unit vector $u \in U$ then the projection of any $x \in \call{I}$ onto $u$ is given by the simple formula $\langle x,u\rangle u$. The following Lemma shows what we can say when the subspace is of arbitrary dimension but with $U$ closed:
\begin{lemma}\label{lem:decomposition}
Let $\bb{H}$ be a Hilbert space and $U \subseteq \bb{H}$ a closed subspace. Then
\[\bb{H} = U \oplus U^\perp\]
\end{lemma}
\begin{proof}
We will define a projection
\begin{align*}
    P_U: \bb{H} &\lto U\\
    x &\longmapsto \operatorname{inf}\lbrace ||x - y|| \mid y \in U\rbrace
\end{align*}
We let $d$ denote $\operatorname{inf}\lbrace ||x - y|| \mid y \in U\rbrace$. By definition of $\operatorname{inf}$ there exists a sequence $(x_n)_{n=0}^\infty$ of elements in $U$ such that $\lim_{n\to \infty}||x - x_n|| = d$. Since $U$ is closed it is complete and the norm is continuous so it suffices to show that the sequence $(x_n)_{n=0}^\infty$ is Cauchy. This can be done for example using the parallelogram identity: for all $n,m\geq 0$:
\begin{equation}
    ||x_n - x_m||^2 + ||(x - x_n) + (x - x_m)||^2 = 2||x - x_n||^2 + 2||x - x_m||^2
\end{equation}
As given $\epsilon > 0$ there exists $N \geq 0$ such that $||x - x_n||^2 < d^2 + \epsilon^2/4$, for $n \geq N$. Thus
\begin{align*}
    ||x_n - x_m||^2 &= 2||x - x_n||^2 + 2||x - x_m||^2 - 4||x = ||1/2(x_n + x_m)||^2\\
    &\leq 4d^2 + \epsilon^2 - 4||x - 1/2(x_n + x_m)||^2
\end{align*}
which since $1/2(x_n + x_m) \in C$ we have $d \leq ||x - 1/2(x_n + x_m)||$, proving $(x_n)_{n = 0}^\infty$ is Cauchy. This also shows linearity.

It remains to show $x - P_U(x) \in U^\perp$. To do this, we will consider the family of vectors $c(t) = (1-t)P_U(x) + ty$, ($t \in \bb{R}$) and analyse the derivative of $||x - y_t||^2$ at $t = 0$.

Consider the composition
\begin{align}
    \gamma: \bb{R} &\lto \bb{R}\\
    t &\longmapsto ||x - c(t)||^2
\end{align}
We can write $\gamma$ in a more explicit form:
\begin{align*}
    \gamma(t) &= ||x - P_U(x) + t(y - P_U(x))||^2\\
    &= \big\langle x - P_U(x) + t(y - P_U(x)),x - P_U(x) + t(y - P_U(x))\big\rangle\\
    &= ||x - P_U(x)||^2 - 2t\operatorname{Re}\langle  x - P_U(x),y - P_U(x)\rangle + t^2||y - P_U(x)||
\end{align*}
which is clearly differentiable and has derivative $- 2\operatorname{Re}\langle  x - P_U(x),y - P_U(x)\rangle$ at $t = 0$. Since $P_U(x)$ (which equals $c(0)$) is a minimum of $\gamma(t)$ we have that $\operatorname{Re}\langle  x - P_U(x),y - P_U(x)\rangle = 0$. This holds true for arbitrary $y \in U$ and lastly we have
\[\lbrace y - P_U(x) \mid y \in U \rbrace = U\]
thus for all $y \in U$:
\begin{equation}
    \operatorname{Re}\langle  x - P_U(x),y\rangle = 0
\end{equation}
This shows that $x - P_U(x) \in U^\perp$.
\end{proof}
Given a Hilbert space $\bb{H}$ there is a map
\begin{align}
    \Phi: \bb{H} &\lto \bb{H}^\ast\label{eq:Riesz_map}\\
    b &\longmapsto \langle \und{0.2}, b \rangle
\end{align}
Notice that in order to produce a \emph{linear} functional, it was important we put $b$ in the second argument, we must define $\Phi$ so that $\Phi(b) \neq \langle b, \und{0.2}\rangle$. By anti-linearity of the second argument of the inner product we have that $\Phi$ is anti-linear, and moreover is injective as
\begin{align*}
    \Phi(b) = \Phi(b') &\Longrightarrow \langle \und{0.2}, b\rangle = \langle \und{0.2}, b'\rangle\\
    &\Longrightarrow \forall b'' \in \bb{H}, \langle b'', b - b'\rangle = 0\\
    &\Longrightarrow \text{ in particular, } \langle b-b', b-b'\rangle = 0\\
    &\Longrightarrow b - b' = 0
\end{align*}
In the special case where $\bb{H}$ is finite dimensional, we automatically have that this map is surjective as it is injective, and any anti-linear, injective map between two finite dimensional spaces of equal dimension is automatically surjective. More generally, if $\bb{H}$ has arbitrary dimension, then for any $y \in \bb{H}$ the map $\langle \und{0.2},y\rangle$ is bounded (see Remark \ref{rmk:bounded_inner_product}) so the image of $\Phi$ is contained in the set of continuous linear functionals, the following establishes the reverse inequality:
\begin{thm}[Riesz Representation Theorem]\label{thm:riesz}
Let $\bb{H}$ be a Hilbert space. For every continuous linear functional $\varphi \in \bb{H}^\ast$ there exists a unique element $h_\varphi \in \bb{H}$ such that
\begin{equation}
    \varphi = \langle \und{0.2}, h_\varphi \rangle
\end{equation}
Moreover, we have
\begin{equation}
    ||\varphi||_{\bb{H}^\ast} = ||h_\varphi||_{\bb{H}}
\end{equation}
\end{thm}
We will use the following Lemma:
\begin{lemma}\label{lem:one_dim_kernel}
Let $\bb{H}$ be a Hilbert space and $\varphi \in \bb{H}^\ast$ be non-zero and continuous. Then $(\operatorname{ker}\varphi)^\perp$ is one dimensional.
\end{lemma}
\begin{proof}
Since $\varphi$ is continuous the set $\operatorname{ker}\varphi$ is closed and so by Lemma \ref{lem:decomposition} we have $\bb{H} = \operatorname{ker}\varphi \oplus (\operatorname{ker}\varphi)^\perp$, which since $\varphi\neq 0$ implies there exists $v\neq 0 \in (\operatorname{ker}\varphi)^\perp$, so $\operatorname{dim}(\operatorname{ker}\varphi)^\perp > 0$. Now, say $v_1,v_2 \in (\operatorname{ker}\varphi)^\perp$ so that $\varphi(v_1) \neq 0$ and $\varphi(v_2) \neq 0$. These are complex numbers and so there exists $\lambda \in \bb{C}$ such that
\[0 = \lambda \varphi(v_1) - \varphi(v_2) = \varphi(\lambda v_1 - v_2)\]
which means $\lambda v_1 - v_2 \in \operatorname{ker}\varphi \cap (\operatorname{ker}\varphi)^\perp = \lbrace 0 \rbrace$.
\end{proof}
\begin{proof}[Proof of Theorem \ref{thm:riesz}]
Clearly if $\operatorname{ker}\varphi = \bb{H}$ we can take $h_\varphi = 0$ so assume this is not the case. Since $\varphi$ is continuous its kernel $\operatorname{ker}\varphi$ is a closed subset of $\bb{H}$. Thus, by Lemma \ref{lem:decomposition} the Hilbert space $\bb{H}$ decomposes: $\bb{H} = \operatorname{ker}\varphi \oplus (\operatorname{ker}\varphi)^\ast$. Since $\operatorname{ker}\varphi$ is a proper subset it then follows that there exists a non-zero element $v\neq0 \in (\operatorname{ker}\varphi)^\ast$, by normalising we may assume that $v$ is a unit vector. We will show that $\overline{\varphi(v)}v$ is the appropriate unique choice for $h_\varphi$.

By Lemma \ref{lem:one_dim_kernel} the subspace $(\operatorname{ker}\varphi)^\perp$ is one dimensional, hence we can use formula \eqref{eq:easy_case} for the projection of arbitrary $x$ onto $(\operatorname{ker}\varphi)^\perp$. Observe the following calculation:
\begin{align*}
    \varphi(x) &= \varphi(x - \langle x,v\rangle v + \langle x,v\rangle v)\\
    &= \varphi(x - \langle x,v\rangle v) + \varphi(\langle x,v\rangle v)\\
    &= 0 + \langle x,v\rangle\varphi(v)\\
    &= \langle x, \overline{\varphi(v)}v\rangle
\end{align*}
For uniqueness, say $h_\varphi'$ was another such element. Then
\begin{align*}
    &\forall x \in \bb{H}, \langle x, h_\varphi\rangle = \langle x, h_\varphi'\rangle\\
    &\Longrightarrow \forall x \in \bb{H}, \langle x, h_\varphi - h_\varphi'\rangle = 0\\
    &\Longrightarrow|| h_\varphi - h_\varphi'|| = 0\\
    &\Longrightarrow h_\varphi = h_\varphi'
\end{align*}
For the second claim, we use the Cauchy-Schwartz inequality:
\begin{align*}
    |\varphi(x)| = |\langle x, \overline{\varphi(v)}v\rangle| \leq ||x||||\overline{\varphi(v)}||v|| = ||x|||\varphi(v)|
\end{align*}
and so if $x$ has unit norm $|\varphi(x)| \leq |\varphi(v)|$, in other words, $||\varphi||_{\bb{H}^\ast} \leq |\varphi(v)|$ however $v$ has unit norm itself, so $||\varphi||_{\bb{H}^\ast} = |\varphi(v)|$. The proof is now complete once it is noted that $||h_\varphi||_{\bb{H}} = |\varphi(v)|$.
\end{proof}
\begin{remark}
\label{rmk:bounded_inner_product} Let $y \in \bb{H}$ be an element of a Hilbert space $\bb{H}$ and consider the function $\langle \und{0.2},y\rangle$. This is bounded, as by Cauchy-Schwartz:
\[|\langle x,y\rangle| \leq ||x||||y||\]
thus $|\langle \und{0.2},y\rangle|/||x|| \leq ||y||$ and in fact this is equality as $|\langle y/||y||,y\rangle| = ||y||$.
\end{remark}
Given an operator $u: \bb{H}_1 \lto \bb{H}_2$ there is for each $y \in \bb{H}_2$ an associated linear functional $x \longmapsto \langle u(x),y\rangle$ which we denote by $\langle u(\und{0.2}),y\rangle$. By Theorem \ref{thm:riesz} there is thus an element $y^\ast \in \bb{H}_1$ such that $\langle u(\und{0.2}),y\rangle = \langle \und{0.2}, y^\ast\rangle$. The assignment $y \mapsto y^\ast$ is in fact linear, we show additivity:
\begin{align*}
    \langle u(\und{0.2}), y_1 + y_2\rangle &= \langle \und{0.2}, (y_1 + y_2)^\ast\rangle\\
\end{align*}
and
\begin{align*}
    \langle u(\und{0.2}), y_1 + y_2\rangle &= \langle u(\und{0.2}), y_1\rangle + \langle u(\und{0.2}), y_2\rangle\\
    &= \langle \und{0.2}, y_1^\ast\rangle + \langle \und{0.2}, y_2^\ast\rangle\\
    &= \langle \und{0.2}, y_1^\ast + y_2^\ast\rangle
\end{align*}
which implies $(y_1 + y_2)^\ast = y_1^\ast + y_2^\ast$. We define:
\begin{defn}
The \textbf{adjoint operator} associated to an operator $u: \bb{H}_1 \lto \bb{H}_2$ is the linear map:
\begin{align*}
    u^\ast: \bb{H}_2 &\lto \bb{H}_1\\
    y &\longmapsto y^\ast
\end{align*}
Its existence is established by the Riesz Representation Theorem (\ref{thm:riesz}) and it is uniquely determined by the property:
\begin{equation}
    \forall x\in\bb{H}_1,y\in\bb{H}_2, \langle u(x),y\rangle = \langle x, u^\ast(y)\rangle
\end{equation}
\end{defn}
\begin{remark}
Let $\big(\bb{H}_1,\langle\cdot,\cdot\rangle_B\big), \big(\bb{H}_2,\langle \cdot, \cdot \rangle\big)_C$ be Hilbert spaces and let $u: \bb{H}_1 \lto \bb{H}_2$ be an operator. The \textbf{adjoint} to $u$, denoted $u^\ast$ is the operator:
\begin{align}
    \und{0.2} \circ u: \bb{H}_2^\ast &\lto \bb{H}_1^\ast\\
    \varphi &\longmapsto \varphi \circ u
\end{align}
The following diagram commutes:
\begin{equation}
\begin{tikzcd}
\bb{H}_{2}^{\ast}\arrow[rrrr,"{\und{0.2} \circ u}"]\arrow[ddd,swap,"{\cong}"] & & & & \bb{H}_1^{\ast}\arrow[ddd,"{\cong}"]\\
&\langle\und{0.2},v\rangle\arrow[r,mapsto] & \langle u(\und{0.2}), v\rangle\arrow[r,leftrightarrow] & \langle \und{0.2},v^{\ast}\rangle\\
&v\arrow[r,mapsto]\arrow[u,mapsto] & u^{\ast}(v)\arrow[ur,mapsto]\\
\bb{H}_2\arrow[rrrr,swap,"{u^\ast}"] & & & & \bb{H}_1
\end{tikzcd}
\end{equation}
which explains the overloading of terminology.
\end{remark}
\begin{notation}
Given a complex matrix $A$, the matrix given by conjugating each element $a \in A$ and then transposing the result, ie, the \textbf{conjugate transpose} is denoted $A^{\dagger}$. Due to Proposition \ref{prop:transpose_matrix} below, the conjugate transpose of a matrix is often referred to as the \textbf{adjoint}.
\end{notation}
\begin{proposition}\label{prop:transpose_matrix}
Let $\bb{H}_1,\bb{H}_2$ be finite dimensional, and let $v_1,...,v_n \in \bb{H}_1, w_1,...,w_m\in \bb{H}_2$ be orthonormal bases for $\bb{H}_1,\bb{H}_2$ respectively. If $\varphi: \bb{H}_1 \lto \bb{H}_2$ is a linear transformation and $A$ its matrix representation with respect to these bases, then the matrix representation of the adjoint $\varphi^\ast$ is $A^\dagger$, the conjugate transpose of $A$.
\end{proposition}
\begin{proof}
For each $j = 1,...,m$ write $w_j^\ast = \alpha_1 v_1 + \hdots + \alpha_n v_n$ and each $i = 1,...,n$ write $\varphi(v_i) = \beta_1 w_1 + \hdots + \beta_m w_m$. We calculate:
\begin{equation}
\langle \varphi(v_i), w_j\rangle = \beta_m \langle w_1, w_j \rangle + \hdots + \beta_m \langle w_m, w_j\rangle = \beta_j
\end{equation}
and
\begin{equation}
\langle v_i, w_j^\ast\rangle = \bar{\alpha}_1 \langle v_i, v_1\rangle + \hdots + \bar{\alpha}_n \langle v_i, v_n \rangle = \bar{\alpha}_i
\end{equation}
Since by definition $\langle \varphi(v_i), w_j\rangle = \langle v_i, w_j^\ast\rangle$ the proof is complete.
\end{proof}
\subsection{Hermitian operators}
Throughout, $V$ is a complex vector space.
\begin{defn}
A square, complex matrix $A$ is \textbf{Hermitian} if it is self-adjoint, that is $A^\dagger = A$.

A matrix is \textbf{normal} if $AA^\dagger = A^\dagger A$

An operator $\varphi:V \lto V$ is \textbf{Hermitian} (\textbf{normal}) if a (and hence all) matrix representation(s) of $V$ is Hermitian (normal).
\end{defn}
Clearly, all Hermitian matrices are normal.
\begin{thm}[Spectral decomposition]\label{thm:spectral}
Let $V$ be a finite dimensional, complex vector space and $A$ a matrix representation of an operator on $V$. The matrix $A$ is normal if and only if it is diagonalisable with respect to some orthonormal basis for $V$.
\end{thm}
\begin{proof}
We prove that normal matrices are diagonalisable.

We proceed by induction on the size of the matrix. If the matrix is $1\times 1$ then there is nothing to prove. Now for the inductive step. Let $\lambda$ be an eigenvalue of $A$, and $P$ the matrix which projects onto the $\lambda$-eigenspace. We let $Q$ denote $I - P$, the projector onto the complement subspace.  We notice that
\begin{equation}
A = (P + Q)A(P + Q)= PAP + QAP + PAQ + QAQ
\end{equation}
We have that $QAP = 0$ because $A$ maps the $\lambda$-eigenspace onto itself, and we claim moreover that $PAQ = 0$. To see this, let $v$ be an eigenvector with eigenvalue $\lambda$, then
\begin{equation}
AA^{\dagger}v = A^{\dagger}Av = A^{\dagger} \lambda v = \lambda A^\dagger v
\end{equation}
which means $A^\dagger$ maps the $\lambda$-eigenspace onto itself. This implies $QA^\dagger P = 0$, taking the transpose of which we end at $P A Q = 0$ as claimed.

Thus $A = PAP + QAQ$. The matrix $PAP$ is diagonalisable with respect to some orthonormal basis for $P$. Since $P \cap Q = 0$ it remains to show that $QAQ$ is diagonalisable with respect to some orthonormal basis for $Q$. The space $Q$ has strictly smaller size than $A$ and so this follows by induction once we have shown that $QAQ$ is normal. This is a simple calculation:
\begin{align*}
QAQQA^\dagger Q &= QAQA^\dagger Q\\
&= QA(P + Q)A^\dagger Q\\
&= QAA^\dagger Q\\
&= QA^\dagger AQ\\
&= QA^\dagger (P + Q)AQ\\
&= QA^\dagger QAQ \\
&= QA^\dagger QQ A Q
\end{align*}
\end{proof}
\begin{defn}
A matrix $U$ is \textbf{unitary} if $U^\dagger U = I$. 
\end{defn}
\begin{lemma}
A unitary matrix $U$ satisfies $UU^\dagger = I$.
\end{lemma}
\begin{proof}
We calculate:
\begin{equation}
U^\dagger U = (\langle v_i, v_j\rangle)_{ij}= (\overline{\langle v_j, v_i \rangle})_{ij}= (\langle v_j, v_i \rangle)_{ij}= (\langle v_j, v_i \rangle)_{ji}= UU^\dagger
\end{equation}
where the third and fourth equality follow from the fact that $U^\dagger U = I$.
\end{proof}
Notice that the spectral decomposition (\ref{thm:spectral}) states that the matrix $A$ is such that $A = U^\dagger DU$ for a diagonal matrix $D$ and a unitary matrix $U$.
\begin{cor}
A normal matrix $A$ is Hermitian if and only if its eigenvalues are real.
\end{cor}
\begin{proof}
First notice that if a matrix is Hermitian then for any eigenvector $v$ with eigenvalue $\lambda$:
\begin{equation}
\lambda |v|^2 = \langle \lambda v,v \rangle = \langle Av, v \rangle = \langle v,Av \rangle = \bar{\lambda}|v|^2
\end{equation}

Now we prove the other direction.  Let $D$ be diagonal and $U$ a unitary matrix such that $A = U^{-1}DU$. Then
\begin{equation}
A^\dagger = U^{\dagger}D^\dagger U^{-1^\dagger} = U^{-1}DU = A
\end{equation}
\end{proof}
\begin{defn}
An operator $\varphi: V \lto V$ is \textbf{positive} if:
\begin{equation}
\forall v \in V, \langle v, \varphi v\rangle \geq 0
\end{equation}
which means, $\langle v, \varphi v\rangle$ is real and non-negative. If the inequality is strict, then $\varphi$ is \textbf{positive definite}.
\end{defn}
\begin{example}
Let $A$ be any operator. Then for any $v \in V$:
\begin{equation}
\langle v, A^\dagger Av\rangle = \langle Av, Av \rangle = ||Av||^2 \geq 0
\end{equation}
Thus $A^\dagger A$ is positive.
\end{example}
\begin{proposition}
A positive operator on a finite dimensional vector space is necessarily Hermitian.
\end{proposition}
\begin{proof}
Let $A$ be a matrix representation of the positive operator. Notice the following calculation:
\begin{align*}
0 \leq \langle v, (A - A^\dagger)v\rangle &= \langle (A^\dagger - A)v,v\rangle\\
&= \overline{\langle v, (A^\dagger - A)v\rangle}\\
&= \langle v, (A^\dagger - A)v\rangle\\
&= -\langle v, (A - A^\dagger)v\rangle \geq 0
\end{align*}
and so for all $v \in V$ we have $\langle v, (A - A^\dagger)v\rangle = 0$.

Moreover, we notice that $A - A^\dagger$ is normal and hence diagonalisable, by the Spectral decomposition. It follows from these two observations that $A - A^\dagger = 0$.
\end{proof}
\begin{defn}
Let $A,B$ be matrices, then the \textbf{commutator} is $[A,B] := AB - BA$. The \textbf{anticommutator} is $\lbrace A,B \rbrace = AB + BA$.
\end{defn}
\begin{thm}[Simultaneous Diagonalisation Theorem]
Let $A,B$ be Hermitian operators. Then $[A,B] = 0$ if and only if $A$ and $B$ are simultaneously diagonalisable.
\end{thm}
\begin{proof}
If $A$ and $B$ are simultaneously diagonalisable, then let $U$ be a unitary matrix and $D_1,D_2$ diagonal matrices such that
\begin{equation}
A = U^{-1}D_1U,\qquad B = U^{-1}D_2U
\end{equation}
We then have:
\begin{equation}
AB = U^{-1}D_1UU^{-1}D_2U = U^{-1}D_1D_2U = U^{-1}D_2D_1U = U^{-1}D_2UU^{-1}D_1U = BA
\end{equation}
Conversely, say $[A,B] = 0$. We have that $A$ is Hermitian and so admits a spectral decomposition. Let $a_1,...,a_n$ be the eigenvalues corresponding to this decomposition and let $V_{a_i}$ denote the $a_i$-eigenspace. We first notice that $B$ maps $V_{a_i}$ into itself: for any $v \in V_{a_i}$
\begin{equation}
ABv = BAv = a_{i}Bv
\end{equation}
Now, since $B$ is Hermitian, it follows that $B_{V_{a_i}}: V_{a_i} \lto V_{a_i}$ is and so there exists a spectral decomposition of $B_{V_{a_i}}$ for each vector space $V_{a_i}$. Denote by $b_1^{a_i},...,b_{k_{a_i}}^{a_i}$ an orthonormal basis for $V_{a_i}$. We then have that
\begin{equation}
\lbrace b_1^{a_i},...,b_{k_{a_i}}^{a_i}\rbrace_{i = 1}^n
\end{equation}
is a basis of eigenvectors of both $A$ and $B$ for the whole space $V$.
\end{proof}
There is another decomposition which is often helpful:
\begin{observation}
Let $T: V \lto V$ be a linear operator on an $n$-dimensional vector space $V$. We could ask if $T$ can be factored $T = UT'$ where $U$ is unitary? Say this was possible, then
\begin{equation}
T^\dagger T = T'^\dagger U^\dagger U T'
\end{equation}
so if $T'$ were Hermitian we would have $T^\dagger T = T'^2$ which would imply $T' = \sqrt{T^\dagger T}$, in fact $T^\dagger T$ is Hermitian (indeed it is positive) and thus so is $\sqrt{T^\dagger T}$ and so our assumption that $T'$ be Hermitian is not too much to ask for, and if $U$ were to exist it must be that $T' = \sqrt{T^\dagger T}$. Thus we are prompted to make the following calculation: let $v_1,...,v_n$ be a basis for $V$ such that (we write $P_{v_i}$ for the projection onto $v_i$)
\begin{equation}
\sqrt{T^\dagger T} = \sum_{i = 1}^n \lambda_i P_{v_i}
\end{equation}
then
\begin{equation}
\sqrt{T^\dagger T}v_i \lambda_i
\end{equation}
and indeed we want $U$ such that $\lambda_i Uv_i = Tv_i$. One might suggest defining $Uv_i = Tv_i/\lambda_i$ at this point, however there is no reason for this to be unitary. Instead we define
\begin{equation}
U = \sum_{j = 1}^n Tv_jP_{v_j}/\sqrt{\lambda_j}
\end{equation}
which indeed is unitary.  In fact we read off from this that $\lbrace Tv_1/\sqrt{\lambda_1},...,Tv_n/\sqrt{\lambda_n}\rbrace$ is an orthonormal basis for $V$. Notice however that this assumes $\lambda_i \neq 0$ for all $i$. This can be fixed by doing this process first for all $\lambda_i \neq 0$, and to construct an orthonormal set $\lbrace Tv_1/\sqrt{\lambda_1},...,Tv_j/\sqrt{\lambda_j}\rbrace$ and then extending this to an orthonormal basis for $V$ via the Gram-Schmidt process.

We have proven the first half of:
\begin{thm}[Polar decomposition]
Let $T: V \lto V$ be a linear operator on an $n$-dimensional vector space $V$. Then there exists a unitary operator $U$ and positive operators $J,K$ such that
\begin{equation}
T = UJ = KU
\end{equation}
with $J = \sqrt{A^\dagger A}, K = \sqrt{AA^\dagger}$.
\end{thm}
To obtain $K$ we simply notice
\begin{equation}
A = JU = UJU^\dagger  U
\end{equation}
so we set $K = UJU^\dagger$, which is a positive operator. Then $AA^\dagger = KUU^\dagger K = K^2$.
\end{observation}
If we have such a decomposition $T = UJ$, then $J$ is diagonalisable, being positive, thus $T = USDS^\dagger$ for unitary $S$ and diagonal $D$. Setting $V = S^\dagger$ we obtain:
\begin{cor}[Singular value decomposition]
Let $T: V \lto V$ be a linear operator on an $n$-dimensional vector space, then there exists unitary operators $U,V$ and a diagonal operator $D$ such that
\begin{equation}
T = UDV
\end{equation}
\end{cor}

































































\end{document}
\begin{thebibliography}{99}
\bibitem{rudin} W. Rudin, \emph{Principles of mathematical analysis}.
\end{thebibliography}