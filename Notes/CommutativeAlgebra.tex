\documentclass[12pt]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{units}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{nicefrac}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{color}
\usepackage{tensor}
\usepackage{tipa}
\usepackage{bussproofs}
\usepackage{ stmaryrd }
\usepackage{ textcomp }
\usepackage{leftidx}
\usepackage{afterpage}
\usepackage{varwidth}
\usepackage{physics}

\newcommand\blankpage{
	\null
	\thispagestyle{empty}
	\addtocounter{page}{-1}
	\newpage
}

\graphicspath{ {images/} }

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[subsection] % reset theorem numbering for each chapter
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{fact}[thm]{Fact}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers
\newtheorem{notation}[thm]{Notation}
\newtheorem{remark}[thm]{Remark}
\newtheorem{condition}[thm]{Condition}
\newtheorem{question}[thm]{Question}
\newtheorem{construction}[thm]{Construction}
\newtheorem{exercise}[thm]{Exercise}
\newtheorem{example}[thm]{Example}
\newtheorem{observation}[thm]{Observation}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\scr}[1]{\mathscr{#1}}
\newcommand{\call}[1]{\mathcal{#1}}
\newcommand{\psheaf}{\text{\underline{Set}}^{\scr{C}^{\text{op}}}}
\newcommand{\und}[1]{\underline{\hspace{#1 cm}}}
\newcommand{\adj}[1]{\text{\textopencorner}{#1}\text{\textcorner}}
\newcommand{\comment}[1]{}
\newcommand{\lto}{\longrightarrow}

\usepackage[margin=1cm]{geometry}

\title{Commutative Algebra}
\author{Will Troiani}
\date{August 2020}

\begin{document}
	
	\maketitle
	\tableofcontents
	
	\section{Linear Algebra}
	\subsection{Orthogonal matrices}
	Throughout, $k$ is an algebraically closed field.
	\begin{defn}
		A square matrix $X \in M_n(k)$ is \textbf{orthogonal} if $X^T = X^{-1}$.
	\end{defn}
	These come up when dealing with orthonormal bases:
	\begin{lemma}
		\label{lem:orthonormal} A square matrix $X \in M_n(k)$ is orthonormal if and only if its columns form an orthogonal basis for $k^n$.
	\end{lemma}
	\begin{proof}
		Write $X = [v_1, ..., v_n]$ for vectors $v_i \in k^n$. Then $v_i\cdot v_j = \delta_{ij}$ if and only if $XX^T = I$.
	\end{proof}
	\begin{lemma}
		\label{lem:symm_diag} For every symmetric matrix $X \in M_n(k)$ there exists an orthogonal matrix $U$ such that $U^TXU$ is diagonal.
	\end{lemma}
	\begin{proof}
		We proceed by induction on $n$, the base case is trivial. Say $n > 1$. Since $k$ is algebraically closed we can find an eigenvector $v_1 \in k^n$ of $X$ with eigenvalue $\lambda \in k$ and we can assume this is of unit length too. We extend $v_1$ to an orthonormal basis $\lbrace v_1,...,v_n\rbrace$ of $k^n$ and set $Q = [v_1,...,v_n]$. We then have
		\[Q^TXQ = \begin{pmatrix}
			\lambda & u\\
			0 & X'
		\end{pmatrix}\]
		for some $u \in M_{1 \times n-1}(k)$ and $X' \in M_{n-1}(k)$. Taking the transpose of each side we have:
		\[Q^TX^TQ = \begin{pmatrix}
			\lambda & 0\\
			u & X'^T
		\end{pmatrix} \]
		Since $X$ is symmetric we have $X = X^T$ and so $Q^TXQ = Q^TX^TQ$ which implies $u = 0$ and $X'$ is symmetric. We can apply the inductive hypothesis to $X'$ to get an orthogonal matrix $V \in M_{n-1}(k)$ such that $V^TX'V = D$ is diagonal. We thus have
		\[Q^TXQ = \begin{pmatrix}
			\lambda & 0\\
			0 & VDV^T
		\end{pmatrix} = \begin{pmatrix}
			1 & 0\\
			0 & V
		\end{pmatrix}\begin{pmatrix}
			\lambda & 0\\
			0 & D
		\end{pmatrix}\begin{pmatrix}
			1 & 0\\
			0 & V^T
		\end{pmatrix}\]The result then follows as $Q\begin{pmatrix}
			1 & 0\\
			0 & V
		\end{pmatrix}$ is orthogonal.
	\end{proof}
	
	\subsection{Partial trace}
	Throughout, $V,W$ are finite dimensional vector spaces. First we make an observation, let $\lbrace v_1,...,v_n
	\rbrace$ and $\lbrace w_1,...,w_m\rbrace$ be bases for $V,W$ respectively. Let $\gamma: V \otimes W \lto V \otimes W$ be a linear map. We write
	\begin{align*}
		\gamma(v_i \otimes w_j) &= \alpha_{11,ij}v_1 \otimes w_1 + \hdots + \alpha_{1m,ij}v_1 \otimes w_m\\
		&+ \hdots\\
		&+ \alpha_{n1,ij}v_n \otimes w_1 + \hdots + \alpha_{nm,ij}v_n \otimes w_m
	\end{align*}
	so that if we make the following definitions:
	\begin{align*}
		E_{ij}: V &\lto V & F_{ij}: W &\lto W\\
		v_i &\longmapsto \delta_{ij}v_j & w_i &\longmapsto \delta_{ij}w_j\\
	\end{align*}
	where $\delta_{ij} = 0$ if $i \neq j$ and $\delta_{ij} = 1$ if $i = j$, we have:
	\begin{align*}
		\gamma = \sum_{i = 1}^n\sum_{j = 1}^m\Big(&\alpha_{11,ij}E_{i1}\otimes F_{j1} + \hdots + \alpha_{1m,ij}E_{i1} \otimes F_{jw}\\
		&+ \hdots\\
		&+ \alpha_{n1,ij}E_{in} \otimes F_{j1} + \hdots + \alpha_{nm,ij}E_{in} \otimes F_{jm}\Big)
	\end{align*}
	We arrive at: 
	\begin{observation}\label{obs:basis}
		The set
		\begin{equation}
			\lbrace E_{ik} \otimes F_{jl}\rbrace_{i,k = 1,...,n}^{j,l =1,...,m}
		\end{equation}
	\end{observation}
	forms a basis for $\operatorname{Hom}V \otimes W$. More sophisticatedly, an easy extension of what we have shown yields:
	\begin{lemma}\label{lem:embedding_tensor}
		For arbitrary vector spaces $X,Y,X',Y'$ there exists a natural injection
		\begin{equation}
			\operatorname{Hom}(X,X') \otimes \operatorname{Hom}(Y,Y') \rightarrowtail \operatorname{Hom}(X \otimes Y, X'\otimes Y')
		\end{equation}
		which is an isomorphism if $X,Y,X',Y'$ are finite dimensional.
	\end{lemma}
\begin{remark}
	The map described in Lemma \ref{lem:embedding_tensor} sends a formal tensor element $f \otimes g$ to the tensor product of the two maps $f$ and $g$, which is also denoted $f \otimes g$. Hence we clearly have an injection, and the argument above shows surjectivity in the finite dimensional case.
	
	It follows easily from this presentation that this map is natural in all arguments.
\end{remark}
	\begin{cor}\label{cor:hom_dual}
		There is a natural isomorphism (recall that $V,W$ are finite dimensional).
		\begin{align}
			\operatorname{Hom}(V,W) &\lto V^\ast \otimes W\\
			f &\longmapsto \sum_{i = 1}^n v_i^\ast \otimes f(v_i)
				\end{align}
			where $\lbrace v_1,..., v_n\rbrace$ is ar arbitrary basis for $V$.
	\end{cor}
	\begin{proof}
		By Lemma \ref{lem:embedding_tensor} we have:
		\begin{equation}
			\operatorname{Hom}(V \otimes k, k\otimes W) \cong \operatorname{Hom}(V, k) \otimes \operatorname{Hom}(k, W)
		\end{equation}
	hence we can perform the following calculation, where $s: V \otimes k \lto k \otimes V$ is the swap map, for $i=1,...,n$ we denote by $E_{i}: V \lto k$ is the linear map sending $v_i \longmapsto 1$ and $F_{j}: k \lto V$ the linear map sending $1 \longmapsto v_i$.
	\begin{equation}
		f \longmapsto 1 \otimes f \circ s = \sum_{i = 1}^n E_{i} \otimes f F_{i} \longmapsto \sum_{i = 1}^n v_i^\ast \otimes f(v_i)
	\end{equation}
	\end{proof}
	We will generalise the definition of the trace operator to the definition of a \emph{partial} trace operator, to do this we use the natural isomorphism of Corollary \ref{cor:hom_dual} and combine it with the following adjunction, the \textbf{tensor-hom} adjunction.
	\begin{fact}
		There is an adjunction
		\begin{align}
			\operatorname{Hom}(V \otimes W, U) &\cong \operatorname{Hom}(W, \operatorname{Hom}(V,U))\\
			f &\longmapsto \big(w \mapsto (v \mapsto f(v\otimes w))\big)\\
			\big(v \otimes w \mapsto g(v)(w) \big) &\longmapsfrom g
		\end{align}
		with counit given by the evaluation map (we simply write $h$ for $\operatorname{Hom}$)
		\begin{align*}
			V \otimes h(V,U) &\lto U\\
			v \otimes f &\longmapsto f(v)
		\end{align*}
		and unit given by tensor:
		\begin{align*}
			U &\lto h(V,V \otimes U)\\
			u &\longmapsto (v \mapsto v \otimes u)
		\end{align*}
	\end{fact}
	\begin{cor}
		For finite dimensional vector spaces, there is an adjunction:
		\begin{align}\label{eq:tensor_dual_adjunction}
			\operatorname{Hom}(V \otimes W, U) &\lto \operatorname{Hom}(W, U \otimes V^\ast)\\
			f &\longmapsto \big(w \mapsto \sum_{i = 1}^n v_i^\ast \otimes f(u_i \otimes w)\big)
		\end{align}
	where $\lbrace u_1,...,u_n\rbrace$ is an arbitrary choice of basis for $V$.
	
		The counit of this adjunction is given by
		\begin{align*}
			V \otimes U \otimes V^\ast &\lto U\\
			x \otimes u \otimes y^\ast &\longmapsto y^\ast(x )u
		\end{align*}
		and unit given by (where $v_1,...,v_n$ is an arbitrary basis for $V$)
		\begin{align*}
			W &\lto W \otimes V \otimes V^\ast\\
			w &\longmapsto w \otimes (\sum_{i = 1}^n v_i \otimes v_i^\ast)
		\end{align*}
	\end{cor}
	\begin{proof}
		For existence of the adjunction, we observe the following algebra.
		\begin{align}
			\operatorname{Hom}(V \otimes W, U) &\cong \operatorname{Hom}(W, \operatorname{Hom}(V,U))\\
			&\cong \operatorname{Hom}(W, V^\ast \otimes U )
		\end{align}
		Now we unwind definitions and find the unit and counit. Writing simply $h$ for $\operatorname{Hom}$ the map:
		\begin{equation}
			h(V,V) \lto h(V \otimes k, k \otimes V) \lto h(V,k) \otimes h(k,V) \lto V^\ast \otimes V
		\end{equation}
		acts on $\operatorname{id}_V$ in the following way, where $s$ denotes the \emph{swap map} and $v_1,...,v_n$ is an arbitrary basis for $V$,
		\begin{equation}
			\operatorname{id}_V \longmapsto s \longmapsto \sum_{i = 1}^n v_i^\ast \otimes (1 \mapsto v_i) \longmapsto \sum_{i = 1}^nv_i^\ast \otimes v_i
		\end{equation}
		which describes the unit. The counit is calculated similarly.
	\end{proof}
	With this language, we come up with a description of the trace operator.
	\begin{observation}
		Let $f: V \lto V$ be a linear map. Then the map $k \lto k$ given by $1 \longmapsto \operatorname{Trace}f$ is given by the following composite, where $\eta,\epsilon$ are respectively the unit and counit of the adjunction \eqref{eq:tensor_dual_adjunction} and $v_1,...,v_n$ is an arbitrary basis for $V$:
		\begin{equation}
			\begin{tikzcd}
				k\arrow[r,"{\eta}"] & V \otimes V^\ast\arrow[r, "{f \otimes 1}"] & V \otimes V^\ast\arrow[r,"{\epsilon}"] & k\\
				1\arrow[r,mapsto] & \sum_{i = 1}^n v_i \otimes v_i^\ast\arrow[r,mapsto] & \sum_{i = 1}^n f(v_i) \otimes v_i^\ast\arrow[r,mapsto] & \sum_{i = 1}^n v_i^{\ast}(f(v_i)) = \operatorname{Trace}f
			\end{tikzcd}
		\end{equation}
	\end{observation}
	We use this observation to make a definition:
	\begin{defn}
		Let $f: W \otimes V \lto W \otimes V$ be a linear map. We define the \textbf{partial trace operator} $\operatorname{Trace}_Vf$ as the following composite:
		\begin{equation}\label{eq:partial_trace}
			\begin{tikzcd}
				W\arrow[r,"{1 \otimes \eta}"] & W \otimes V \otimes V^\ast\arrow[r,"{f \otimes 1}"] & W \otimes V \otimes V^\ast\arrow[r,"{1 \otimes \epsilon}"] & W\\
				w\arrow[r,mapsto] & w \otimes (\sum_{i = 1}^n v_i \otimes v_i^\ast) = \sum_{i = 1}^n(w \otimes v_i) \otimes v_i^\ast\arrow[r,mapsto] & \sum_{i = 1}^n f(w \otimes v_i) \otimes v_i^\ast\arrow[r,mapsto] & \hdots
			\end{tikzcd}
		\end{equation}
		the final formula is a bit difficult to write out, in the special case where $f = f_1 \otimes f_2$ for $f_1: W \lto W, f_2: V \lto V$ we obtain
		\begin{equation}
			(\operatorname{Trace}_Vf)(w) = \sum_{i = 1}^n f_1(w)v_i^\ast\big(f_2(v_i)\big) = (\operatorname{Trace}f_2)f_1(w)
		\end{equation}
	\end{defn}
	In fact we can write out a formula for \eqref{eq:partial_trace} if we use Dirac notation. Let $\ket{1},...,\ket{n}$ be a basis for $V$, we think of these as operators $k \lto V$.  We write $\operatorname{id} \otimes \ket{i}$ for the composite $W \lto W \otimes k \lto W \otimes V$.  Also, we denote the multiplication map $W \otimes k \lto W$ defined by $w \otimes x \longmapsto xw$ by $\operatorname{Mult}$. We have
	\begin{align*}
		\operatorname{Trace}_V f &= \epsilon\Big(\sum_{i = 1}^n f(\operatorname{id} \otimes \ket{i}) \otimes \bra{i}\Big)\\
		&= \operatorname{Mult}\Big(\sum_{i = 1}^n (\operatorname{id} \otimes \bra{i})f (\operatorname{id} \otimes \ket{i})\Big)
	\end{align*}
	
	
	\section{Rings and modules}
	\begin{defn}
		Let $A$ be a ring, the \textbf{Jacobson radical} $\frak{R}$ is the intersection of all maximal ideals of $A$.
	\end{defn}
	\begin{lemma}
		Let $A$ be a ring. $x \in \frak{R}$ if and only if $1 - xy$ is a unit for all $y \in A$.
	\end{lemma}
	\begin{proof}
		Say $1 - xy$ is not a unit. Then it is contained inside some maximal ideal $\frak{m}$, but so is $xy$ as $x \in \frak{m}$, thus $1 \in \frak{m}$ which is a contradiction.
		
		Conversely, if $x$ is not contained in some maximal ideal $\frak{m}$ then $\frak{m}$ and $x$ generated $(1)$ (by maximality). Thus $1 = yx + u$ for some $u \in \frak{m}$, that is, $1 - xy \in \frak{m}$, and is therefore not a unit.
	\end{proof}
	\begin{defn}
		\label{def:nilradical_one}
		Let $A$ be a ring, the \textbf{nilradical} is the ideal of nilpotents.
	\end{defn}
	\begin{lemma}
		\label{lem:intersection_primes}
		The nilradical is equal to the intersection of all prime ideals (in a commutative ring).
	\end{lemma}
	\begin{proof}
		Clearly all nilradicals are contained in all prime ideals.
		
		Conversely, if $a$ is not nilpotent then $A_{a}$ is not the zero ring and thus contains a prime.
	\end{proof}
	
	\subsection{Localisation (Nakayama's Lemma)}
	\begin{lemma}
		\label{lem:localistation_zero}
		Let $M$ be an $R$-module such that for all maximal ideals $\frak{m}$ of $R$ we have $M_\frak{m} = 0$. Then $M = 0$.
	\end{lemma}
	\begin{proof}
		Let $x \in M$ be such that $x/1 = 0$ in $M_\frak{m}$. Then there exists $a \not\in \frak{m}$ such that $ax = 0$, which is to say $\operatorname{ann}(x) \not\subseteq \frak{m}$. Since this is true for all maximal ideals $\frak{m}$ we have that $\operatorname{ann}(x) = A$ which implies $x = 0$.
	\end{proof}
	We use the above to give a slick proof that a ring map being an isomorphism is a local property:
	\begin{cor}
		\label{cor:isomorphism_is_local}
		A ring homomorphism $\psi: A \lto B$ is an isomorphism if and only if its localisation at all maximal ideals is.
	\end{cor}
	\begin{proof}
		It's easy to show $\ker \psi_{\frak{m}} \cong (\ker \psi)_{\frak{m}}$ and $\operatorname{coker} \psi_{\frak{m}} \cong (\operatorname{coker} \psi)_{\frak{m}}$. Thus the Lemma follows from Lemma \ref{lem:localistation_zero}
	\end{proof}
	\begin{lemma}[Nakayama's Lemma]
		Let $R$ be a ring and $M$ a finitely generated $R$-module. If $I \subseteq R$ is an ideal contained in the jacobson radical such that $IM = M$ then $M = 0$.
	\end{lemma}
	We reduce to the local case and then make a simple observation.
	\begin{proof}
		We use Lemma \ref{lem:localistation_zero}.
		
		Let $\frak{m}$ be a maximal ideal such that $I \subseteq \frak{m}$ which necessarily exists as $I$ is contained in the jacobson radical. Then $\frak{m}M = M$ and $(\frak{m}A_{\frak{m}})M_{\frak{m}} = M_{\frak{m}}$, so it suffices to assume $A$ is local and $I$ is maximal. Let $\frak{m}$ denote the unique maximal ideal.
		
		Let $m_1,..., m_n$ be a set of generators for $M$. Then $m_1 = i_1m_1 + \hdots + i_nm_n$ for some elements $i_j$ contained in the maximal ideal of $R$. Thus $(1-i_1)m_1 = i_2m_2 + \hdots + i_nm_n$. In fact $1 - i_1$ is a unit because $i_1 \in \frak{m}$ and $1 \not\in \frak{m}$, thus $m_2,...,m_n$ form a generating set. Applying this logic finitely many times we see that $M$ is generated by $m_n$, but then $m_n = im_n$ for some $i \in \frak{m}$ so $(1 - i)m_n = 0$ which by the same logic as above implies $m_n = 0$.
	\end{proof}
	\subsection{Chain conditions}
	\begin{lemma}
		\label{lem:neotherian_ses} Given a short exact sequence of $A$-modules
		\[0 \lto M' \stackrel{\varphi}{\lto} M \stackrel{\psi}{\lto} M'' \lto 0\]
		$M$ is Noetherian if and only if $M',M''$ are.
	\end{lemma}
	\begin{proof}
		Every sub and quotient module of a neotherian module is Noetherian which establishes one direction.
		
		Conversely, let $N \subseteq M$ be a submodule and let $x \in N$. Then consider $[x] \in M/\ker \psi$ where we can write $[x] = \sum_{i = 0}^n \alpha_i [x_i]$ where $\lbrace [x_i]\rbrace_{i = 0}^n$ is a finitely generating set of $\psi(N)$. Choosing representatives we have $x - \sum_{i = 0}^n\alpha_i x_i \in \ker \psi$. We have a short exact sequence so $\ker \psi = M'$ which is finitely generated so there exists $y_1,...,y_m$ such that $x - \sum_{i = 0}^n \alpha_i x_i = \sum_{j = 0}^m\beta_j y_j$. Thus $N$ is finitely generated.
	\end{proof}
	\begin{remark}
		There is a better proof which can be used here. The obvious idea working directly with the ascending chain condition works and gives a proof idea which also works for Artinian. The above proof does not work for Artinian rings because there is no analogue in the setting of Artinian rings to the statement that a ring a module is Noetherian if and only if all its submodules are finitely generated.
	\end{remark}
	\begin{cor}
		If $R$ is a Noetherian ring then so is $R^n$.
	\end{cor}
	\begin{proof}
		Obvious inductive argument.
	\end{proof}
	\begin{cor}
		If $R$ is Noetherian then every finitely generated $R$-module $M$ is Noetherian.
	\end{cor}
	\begin{proof}
		Write $M = R^n/I$.
	\end{proof}
	\subsubsection{Artinian rings/modules}
	\begin{defn}
		A ring $A$ is \textbf{Artinian} if every \emph{descending} chain of ideals
		\[I_1 \supseteq I_2 \supseteq \hdots\]
		terminates. That is, there exists $N > 0$ where for all $n > N$ we have $I_n = I_{n+1}$.
	\end{defn}
	\begin{lemma}
		Every Artinian ring $A$ has finitely many maximal ideals.
	\end{lemma}
	\begin{proof}
		\label{lem:artin_fin_max_ideals}
		Say $A$ has infinitely many maximal ideals $\lbrace \frak{m}_1,\frak{m}_2,\hdots\rbrace$. Then consider the chain
		\[\frak{m}_1 \supseteq \frak{m}_1\frak{m}_2 \supseteq \hdots\]
		we claim this is an infinite descending chain. Consider the link $\frak{m}_1\hdots \frak{m}_n \supseteq \frak{m}_1\hdots \frak{m}_n\frak{m}_{n+1}$ for any $n$. If this was equality then we would have
		\[\frak{m_1}\hdots \frak{m}_n \subseteq \frak{m}_1\hdots\frak{m}_n\frak{m}_{n+1} \subseteq \frak{m}_{n+1}\]
		so by primality, $\frak{m}_i \subseteq \frak{m}_{n+1}$ for some $i \leq n$. By maximality it follows that $\frak{m}_i = \frak{m}_{n+1}$ contradicting that these are distinct maximal ideals.
	\end{proof}
	\begin{lemma}
		Let $A$ be Artinian, by Lemma \ref{lem:artin_fin_max_ideals} there is a finite set of maximal ideals $\lbrace \frak{m}_1,\hdots,\frak{m}_n\rbrace$, denote by $I$ the product $\frak{m}_1\hdots\frak{m}_n$. Then there exists $n > 0$ such that $I^n = (0)$.
	\end{lemma}
	\begin{proof}
		Suppose for a contradiction that $I^n \neq (0)$ for any $n$. Let $n$ be such that $I^n = I^{m}$ for all $m > n$, which exists as $A$ is Artinian. Let $\call{S}$ be the set of ideals of $A$ which do not annihilate $I^n$, then $A \in \call{S}$ and so $\call{S}$ is non-empty. Let $J$ be a minimal element of $\call{S}$, which exists as $A$ is Artinian. We have that $JI^n \subseteq J$ and $(JI^n)I^n = JI^2n = JI^n \neq (0)$. so by minimality we have $JI^n = J$. There exists $j \in J$ such that $jI^n \neq 0$ and so again by minimality we have $(j) = J$. Thus there exists $i \in I^n$ such that $ji = j$, that is, $j(i - 1) = 0$. We then have that $i \in \frak{m}_k$ for all $k$ and so $i - 1$ must not be in any $\frak{m}_k$, which implies $i - 1$ is a unit and that $j = 0$, contradicting that $J$ does not annihilate $I^n$.
	\end{proof}
	\begin{proposition}
		All Artinian rings are Noetherian.
	\end{proposition}
	\begin{proof}
		Let $A$ be Artinian and $\lbrace \frak{m}_1,...,\frak{m}_m\rbrace$ be the set of maximal ideals of $A$ and let $n$ be such that $(\frak{m}_1\hdots\frak{m}_m)^n = (0)$. Consider the chain
		\[A \supseteq \frak{m}_1 \supseteq \hdots \supseteq \frak{m}_1^n \supseteq \frak{m}_1^n\frak{m}_2\supseteq\hdots\supseteq \frak{m}_1^n\frak{m}_2^n \supseteq \hdots \supseteq \frak{m}_1^n\hdots\frak{m}_m^n = 0\]
		each subquotient is an $A/\frak{m}_i$-vector space for some $i$, and in fact is finite dimensional as these subquotients are Artinian modules.  We thus have a decomposition series with Noetherian quotients and thus $A$ is Noetherian.
	\end{proof}
	\begin{cor}
		All finitely generated modules over Artinian rings are both Artinian and Noetherian.
	\end{cor}
	\begin{proof}
		Let $M$ be finitely generated over Artinian $A$. Then $M \cong A^n/I$ for some integer $n$ and ideal $I \subseteq A$, and hence is Artinian. Since $A$ is Artinian, it is thus Noetherian, and so $M$ is Noetherian.
	\end{proof}
	\begin{defn}
		A \textbf{composition series} of a module $M$ is a finite sequence of submodules
		\[0 = M_0 \subseteq M_1 \subseteq \hdots \subseteq M_n = M\]
		such that $M_{i+1}/M_{i}$ is simple (admits no non-trivial submodules) for all $i \geq 0$. Such a series is denoted $(M_i)$ and the length is denoted $l(M_i)$ (we will see shortly that this integer is independent of choice of decomposition series where the notation $l(M)$ will be adopted).
	\end{defn}
	If $N$ is a proper submodule of $M$ and $(M_i)$ is a decomposition series for $M$ then we have a chain of submodules of $N$ given by $(N_i := N \cap M_i)$. These are such that $N_{i+1}/N_{i} \rightarrowtail M_{i+1}/M_i$ so since the latter is simple we either have $N_{i+1}/N_i = M_{i+1}/M_i$ or $N_{i+1} = N_i$. We can remove equal terms so that the latter case is ruled out, and then we have a decomposition series for $N$ satisfying $l(N_i) \leq l(M_i)$. If we had equality we would then have $N_{i+1}/N_i = M_{i+1}/M_i$ for all $i$ form which we deduce that $N_1 = M_{1}$ which implies $N_2 = M_2$ and so on until $M = N$. Thus $l(N_i) < l(M_i)$.
	
	\begin{remark}
		\label{remark:saving_grace}
		An application of this is the following: let $(N_i)$ be any ascending chain, say of length $k$. Then $N_0 \subseteq \hdots \subseteq N_k = M$ implies $l(N_0) < \hdots < l(N_k)$ and so $k \leq l(M)$. Thus all ascending chains have length less than or equal to that of the minimal decomposition series, in particular, all decomposition series have the same length. We denote this integer $l(M)$:
	\end{remark}
	\begin{proposition}
		\label{prop:decomposition_series_basics}
		For any module $M$:
		\begin{enumerate}
			\item all decomposition series of $M$ have the same length,
			\item if $N \subsetneq M$ is a proper submodule, then $l(N)<l(M)$,
			\item\label{prop:sus} if $M$ admits a decomposition series then any ascending chain can be extended to a decomposition series.
		\end{enumerate}
		\begin{proof}
			The first two dotpoints have already been proved. For the last, if an ascending chain is not a decomposition series, then there exist intermediate modules which can be added to the chain. Do so finitely many times until a decomposition series is obtained.
		\end{proof}
		\begin{remark}
			One might suspect that since there is no finite chain condition imposed on $M$ in Proposition \ref{prop:decomposition_series_basics} that there may be an issue with part \ref{prop:sus}, for instance, maybe $M$ admits infinite length chains as well as finite decomposition series. However this is impossible, as the length of any chain in $M$ is bounded by the length of the decomposition series assumed to exist as per Remark \ref{remark:saving_grace}.
		\end{remark}
	\end{proposition}
	The proof of the next Corollary shows that any finitely generated module over an Artinian ring admits a decomposition series:
	\begin{cor}
		Any finitely generated module over an Artinian ring has finite length.
	\end{cor}
	\begin{proof}
		Let $M$ be such a module. Then $M$ is also Noetherian and so admits a maximal proper submodule $M_1$. $M_1$ itself is Noetherian and so also admits a maximal proper submodule. Continuing in this way we obtain a descending chain which terminates by the Artinian property. Thus we have a composition series and so all composition series are of this length, moreover any chain must have length less than this. (\cite[\S 6]{atiyah_macdonald})
	\end{proof}
	\begin{thm}
		\label{prop:artinian_Noeth_dimzero} A ring $A$ is Artinian if and only if it is Noetherian and has dimension 0.
	\end{thm}
	
	\subsection{Associated primes/primary decomposition}
	\begin{defn}
		\label{def:p-primary}
		An ideal $I \subseteq R$ of a ring $R$ is \textbf{primary} if it satisfies the following property:
		
		if $ab \in I$ then either $a \in I$ or $b \in \sqrt{I}$.
		
		If $I$ is primary and $\sqrt{I}$ is a known prime $\frak{p}$ then $I$ is \textbf{$\frak{p}$-primary}.
	\end{defn}
	\begin{defn}
		We refer to $\sqrt{(0)}$ as the \textbf{nilradical}. (Notice this agrees with Definition \ref{def:nilradical_one}).
	\end{defn}
	\begin{lemma}
		The nilradical is equal to the intersection of all prime ideals, in symbols:
		\[\sqrt{(0)} = \bigcap_{\frak{p}\text{ prime}}\frak{p}\]
	\end{lemma}
	\begin{proof}
		See Lemma \ref{lem:intersection_primes}.
	\end{proof}
	\begin{cor}
		\label{cor:intersection_primes}
		For an ideal $I$ the radical $\sqrt{I}$ is equal to the intersection of all prime ideals containing $I$, $\displaystyle\bigcap_{\frak{p} \supseteq I\text{, }\frak{p}\text{ prime}}\frak{p}$
	\end{cor}
	\begin{proof}
		By the correspondence Theorem the only check to make is that the image of $\sqrt{I}$ under the projection $A \lto A/I$ is equal to $\sqrt{(0)}$ but this is clear.
	\end{proof}
	\begin{defn}
		\label{def:vanishing} Let $I \subseteq R$ be an ideal of a ring $R$. The \textbf{vanishing set} $V(I)$ is the set of prime ideals containing $I$,
		\[V(I) := \lbrace \frak{p} \in \operatorname{Spec}R \mid \frak{p} \supseteq I\rbrace\]
	\end{defn}
	\begin{cor}
		\label{cor:vanishing_subset}
		If $I,J$ are ideals and $V(I) \subseteq V(J)$, then $\sqrt{J} \subseteq \sqrt{I}$.
	\end{cor}
	\begin{proof}
		Follows from Corollary \ref{cor:intersection_primes}.
	\end{proof}
	\begin{remark}
		\label{rem:radical_prime}
		Clearly, if $I$ is primary then $\sqrt{I}$ is prime however the converse does not hold: let $R = k[x,y,z]/(xy - z^2)$ and let $P = (x,z)$ which is prime, then $P^2$ is not primary. This is because $xy = z^2 \in P^2$ but $x \not\in P^2$ and $y^n \not\in P^2$ for any $n \geq 0$. This also shows that a power of a prime need not be primary.
	\end{remark}
	\begin{lemma}
		If $\sqrt{I}$ is maximal, then $I$ is primary. In particular, for a maximal ideal $\frak{m}$ we have that $\frak{m}^n$ for any $n > 0$ is $\frak{m}$-primary.
	\end{lemma}
	\begin{proof}
		Let $\sqrt{I} = \frak{m}$. Then the image of $\frak{m}$ in $A/I$ is the nilradical of $A/I$. Since the nilradical is the intersection of all primes, it follows that the $A/I$ has only one prime. Thus every element of $A/I$ is either a nilpotent or a unit, which means every zero divisor of $A/I$ is nilpotent.
	\end{proof}
	\begin{lemma}
		\label{lem:radical_power} Let $R$ be a Noetherian ring and $I \subseteq R$ and ideal. There exists $n > 0$ such that $(\sqrt{I})^n \subseteq I$.
	\end{lemma}
	\begin{proof}
		Let $\sqrt{I}$ be generated by $a_1,...,a_m$. For any $n$, the ideal $(\sqrt{I})^n$ is generated by elements of the form $a_1^{k_1}...a_m^{k_m}$ where $k_1 + \hdots + k_m = n$. Now let $r_i>0$ be such that $a_i^{r_i} \in I$ and fix $n = r_1 + \hdots + r_m$. For each generating element $a_1^{k_1}...a_m^{k_m}$ of $(\sqrt{I})^n$ we must have for some $j$ that $k_j \geq r_j$, and so $a_1^{k_1}...a_m^{k_m} \in I$ which completes the proof. Notice this proof works for any finitely generated ideal, be $R$ Noetherian or not.
	\end{proof}
	\begin{cor}
		\label{cor:primary_ideals_nested}
		Let $A$ be a Noetherian local ring with maximal ideal $\frak{m}$. Then for some $n \geq 0$, an ideal $I$ is $\frak{m}$-primary if and only if $\frak{m}^n \subseteq I \subseteq \frak{m}$.
	\end{cor}
	\begin{proof}
		If $I$ is $\frak{m}$-primary then $\sqrt{I} = \frak{m}$ and so by Lemma \ref{lem:radical_power} we have $\frak{m}^n = \sqrt{I}^n \subseteq I$. Also since $A$ is local we have $I \subseteq \frak{m}$.
		
		Conversely, if $\frak{m}^n \subseteq I \subseteq \frak{m}$ for some $n$, $ab \in I$ implies $ab \in \frak{m}$, say $a \not\in \frak{m}$. Then $b \in \frak{m}$ and so $b^n \in \frak{m}^n \subseteq I^n$, thus $I$ is primary. Moreover,
		\[\frak{m} \subseteq \sqrt{\frak{m}^n} \subseteq \sqrt{I} \subseteq \sqrt{\frak{m}} \subseteq \frak{m}\]
		and so $I$ is $\frak{m}$-primary.
	\end{proof}
	We know that the prime ideals of $\bb{Z}$ are given by $(p)$ where $p$ is prime. The \emph{primary ideals} of $\bb{Z}$ are given by $(p^n)$. This follows from the fact that $\bb{Z}$ is a PID and that an ideal $I$ is primary implies $\sqrt{I}$ is prime. Consider for example $(p^n)$ for $n > 0$, then there exist zero divisors (as a $\bb{Z}$-module) $p,p^2,...,p^{n-1} \in \bb{Z}$ of $\bb{Z}/p^n$ and all of these are such that $p^j \in \sqrt{\operatorname{ann}_\bb{Z}(\bb{Z}/p^n)} = (p)$.
	\begin{defn}
		A submodule $N \subseteq M$ of an $R$-module $M$ is \textbf{primary} if it satisfies the following condition:
		
		if $a \in R$ is a zero-divisor of $M/N$ then $a \in \sqrt{\operatorname{ann}_R(M/N)}$.
	\end{defn}
	\begin{fact}
		If $M$ is a primary submodule then $\operatorname{ann}_R(M/N)$ is primary.
	\end{fact}
	\begin{proof}
		Let $ab \in \operatorname{ann}_R(M/N)$ and say $a \not\in \operatorname{ann}_R(M/N)$. Then $ab(M/N) = 0$ but $a(M/N) \neq 0$. This implies $b(ax) = 0$ for some $x \in M/N$ which is to say that $b$ is a zero-divisor of $M/N$. Since $N$ is primary, this implies $b \in \sqrt{\operatorname{ann}_R(M/N)}$.
	\end{proof}
	\begin{defn}
		Let $M$ be an $R$-module, then the set of \textbf{associated primes} is \[\operatorname{Ass}_RM := \lbrace \frak{p} \mid \exists x \in M, \operatorname{ann}_R(x) = \frak{p}\rbrace\]
		We say that $\frak{p} = \operatorname{ann}_RM$ is \textbf{associated}.
	\end{defn}
	How do we think about associated primes? They have surprisingly useful properties which we go through now.
	\begin{lemma}
		\label{lem:ass_nonempty_zero_div}
		If $R$ is Noetherian and $M$ a non-zero $R$-module, then
		\begin{enumerate}
			\item\label{lem:ass_non-empty} $\operatorname{Ass}_RM \neq \varnothing$,
			\item\label{lem:zero-div_ass-primes} the set of zero divisors of $M$ is the union of all associated primes of $M$.
		\end{enumerate}
	\end{lemma}
	\begin{proof}
		\eqref{lem:ass_non-empty}: as $R$ is Noetherian the set $\lbrace \operatorname{ann}_R(x) \mid x \in M\rbrace$ contains a maximal element.\\
		%
		\eqref{lem:zero-div_ass-primes} Any zero divisor $a$ is contained in $\operatorname{ann}_R(x)$ for some $x$ and so by the first part is contained in some associated prime.
	\end{proof}
	The next Theorem shows how associated primes interact with localisation:
	\begin{thm}
		\label{thm:primes_loc_interact}
		Let $S$ be a multiplicative subset of $R$ and consider $\operatorname{Spec}A_S$ as a subset of $\operatorname{Spec}A$,
		\begin{enumerate}
			\item\label{thm:AS-mod} Let $M$ an $R_S$-module (and hence also an $A$-module). Then $\operatorname{Ass}_RM = \operatorname{Ass}_{R_S}M$.
			\item\label{thm:A-mod} Let $M$ be an $R$-module, if $R$ is Noetherian then $\operatorname{Ass}_RM \cap \operatorname{Spec}R_S = \operatorname{Ass}_{R_S}M_S$.
		\end{enumerate}
	\end{thm}
	\begin{proof}
		\eqref{thm:AS-mod} We already know there is a bijection between primes of $R_S$ and primes of $R$ disjoint from $S$ given by $\frak{p} \mapsto \frak{p} \cap R$. In fact, any associated prime $\frak{p}$ of $R$ must be disjoint from $S$ as elements of $S$ act invertably on $M$, thus it remains to show that associated primes are mapped to associated primes under this bijection. We have
		\begin{align*}
			a(x/1) = 0 &\Leftrightarrow \exists s \in S,\text{}sax = 0\\
			&\Leftrightarrow ax = 0
		\end{align*}
		because $M$ is an $R_S$ module and thus elements of $S$ act invertibly on $M$. Thus $\operatorname{ann}_{R_S}(x) \cap R = \operatorname{ann}_{R}(x)$, for any $x \in M$.
		
		\eqref{thm:A-mod} Let $\frak{p} = \operatorname{ann}_R(x) \in \operatorname{Ass}_RM \cap \operatorname{Spec}R_S$ and consider the prime ideal $\frak{p}R_S$, we claim this is equal to $\operatorname{ann}_{R_S}(x/1)$. Say $(a/s)(x/1) = 0$, then there exists $t \in S$ such that $tax = 0$, but $t \not\in \operatorname{ann}_R(x)$ (as $\frak{p} \cap S = \varnothing$) and so $ax = 0$, which is to say $a \in \frak{p}$ and so $a/1$ and thus $a/s \in \frak{p}R_S$. Also, if $a/1 \in \frak{p}R_S$ then $ax = 0$ and thus $(a/1)(x/1) = 0$. Notice that we did not use the assumption that $R$ is Noetherian here.
		
		Conversely, let $\frak{p} = \operatorname{ann}_{R_S}(x/s) \in \operatorname{Ass}_{R_S}M_S$ and consider the prime $P := \frak{p} \cap R$. Let $a_1,...,a_n$ generate $P$. The image of these under the localisation map $P \lto \frak{p}$ are such that $(a_i/1)(x/s) = 0$, so there exists $t_i \in S$ such that $t_i a_i x = 0$. We claim $P = \operatorname{ann}_R(t_1...t_nx)$. If $y \in P$ then $y = \sum_{i = 1}^n \alpha_i a_i$ which annihilates $t_1...t_nx$, and if $y \in \operatorname{ann}_R(t_1,...,t_nx)$ then $yx/1 = 0$ in $R_S$ so $y/1 \in \frak{p}$ which implies $y \in P$.
	\end{proof}
	\begin{cor}
		\label{cor:ass_R,ass_Rp}
		For a Noetherian ring $R$ and $R$-module $M$ we have
		\[\frak{p} \in \operatorname{Ass}_RM \Longleftrightarrow \frak{p}R_\frak{p} \in \operatorname{Ass}_{R_\frak{p}}M_{\frak{p}}\]
	\end{cor}
	\begin{proof}
		This follows from \eqref{thm:A-mod} and the simple observation $\frak{p} \in \operatorname{Spec}R_{\frak{p}}$.
	\end{proof}
	\begin{thm}
		\label{thm:shortexactsequence_ass}
		Let $R$ be a ring and let the following be a short exact sequence of $M$ modules:
		\[0 \lto M' \stackrel{\varphi}{\lto} M \stackrel{\psi}{\lto} M'' \lto 0\]
		then $\operatorname{Ass}_RM \subseteq \operatorname{Ass}_RM' \cup \operatorname{Ass}_RM''$.
	\end{thm}
	\begin{proof}
		Let $\frak{p} = \operatorname{ann}_Rx \in \operatorname{Ass}_RM$, the map $R \lto M$ given by $a \mapsto ax$ gives rise to a submodule $N$ of $M$ which is isomorphic to $R/\frak{p}$. If $y \neq 0 \in N$ then $\operatorname{ann}_A(y) = \frak{p}$ as $\frak{p}$ is prime. Thus if $N \cap M' \neq \varnothing$ we have that $\frak{p} \in \operatorname{Ass}_RM'$. On the other hand, if $N \cap M' = \varnothing$ then the image of $N$ under $\psi$ is also isomorphic to $A/\frak{p}$ and so $\psi(N) = \operatorname{ann}_A(y)$ for any $y \in \psi(N)$.
	\end{proof}
	\begin{thm}
		\label{thm:sequence}
		Let $R$ be Noetherian and $M$ a finitely generated $R$-module. Then there exists a sequence of submodules
		\[0 = M_0 \subseteq ... \subseteq M_n = M\]
		along with a sequence of prime ideals $\frak{p}_1,...,\frak{p}_n$ of $R$ such that for $i > 0$, $M_i/M_{i-1} \cong A/\frak{p}_i$.
	\end{thm}
	\begin{proof}
		Choose any $\frak{p_1} \in \operatorname{Ass}_RM$ which gives rise to a submodule $M_1$ of $M$ which is isomorphic to $A/\frak{p}_1$. Then either $M_1 = M$ or not. If not, then consider $M/M_1$ and perform the same process to obtain a submodule $M_2' \subseteq M/M_1$, then set $M_2$ to be the preimage of $M_2'$ under $M \lto M/M_1$. $M$ is Noetherian by Lemma \ref{lem:neotherian_ses} so this process eventually terminates.
	\end{proof}
	\begin{remark}
		The statement of Theorem \ref{thm:sequence} provides a statement of some structure of finitely generated modules over a Noetherian ring and is \emph{completely free of any mention of associated primes}. However, the existence of a submodule isomorphic to an integral domain is crucially used in the proof presented here, so this gives a good justification for the existence of associated primes.
	\end{remark}
	\begin{defn}
		The \textbf{support} of an $R$-module $M$ is $\operatorname{Supp}M := \lbrace \frak{p} \subseteq R \mid M_\frak{p} \neq 0\rbrace$.
	\end{defn}
	\begin{thm}
		\label{thm:fin_sub_min}
		Let $R$ be Noetherian and $M$ a finitely generated $R$-module. Then
		\begin{enumerate}
			\item\label{thm:finite} $\operatorname{Ass}_RM$ is a finite set,
			\item\label{thm:ass_supp} $\operatorname{Ass}_RM \subseteq \operatorname{Supp}M$,
			\item\label{thm:min_elts} The minimal elements of $\operatorname{Ass}_RM$ and $\operatorname{Supp}M$ coincide.
		\end{enumerate}
	\end{thm}
	\begin{proof}
		\eqref{thm:finite} Follows from Theorems \ref{thm:shortexactsequence_ass} and \ref{thm:sequence}.\\
		%
		\eqref{thm:ass_supp} By Corollary \ref{cor:ass_R,ass_Rp} we have $\frak{p} \in \operatorname{Ass}_RM \Rightarrow \frak{p}R_{\frak{p}} \in \operatorname{Ass}_{R_\frak{p}}M_{\frak{p}}$ which in particular means $\frak{p}R_{\frak{p}}$ is prime and thus not equal to $R_\frak{p}$ so $M_\frak{p} \neq 0$.\\
		%
		\eqref{thm:min_elts} By \eqref{thm:ass_supp} it suffices to show that minimal elements of $\operatorname{Supp}M$ are associated. Let $\frak{p}$ be such. Then $M_\frak{p} \neq 0$ which means there exists an associated prime in $\operatorname{Ass}_{R_\frak{p}}M_\frak{p}$. Thus there is an element of $\operatorname{Ass}_RM \cap \operatorname{Spec}R_\frak{p}$ by Corollary \ref{cor:ass_R,ass_Rp}. We use that $M_\frak{p}$ is non-zero, \eqref{thm:ass_supp}, and \eqref{thm:A-mod} of Theorem \ref{thm:primes_loc_interact} to obtain:
		\[\varnothing \neq \operatorname{Ass}_{R_\frak{p}}M_\frak{p} = \operatorname{Ass}_RM \cap \operatorname{Spec}R_{\frak{p}} \subseteq \operatorname{Supp}M \cap \operatorname{Spec}R_{\frak{p}} = \lbrace \frak{p} \rbrace\]
		which shows $\frak{p} \subseteq \operatorname{Ass}_{R_\frak{p}}M_\frak{p}$.
	\end{proof}
	We will make use of the following:
	\begin{lemma}
		\label{lem:product}
		Let $I,J$ be ideals of a ring $R$ and $\frak{p}$ a prime ideal. Then $IJ \subseteq \frak{p}$ implies $I \subseteq \frak{p}$ or $J \subseteq \frak{p}$.
	\end{lemma}
	\begin{proof}
		The proof reduces to showing if $R$ is an integral domain and $IJ = 0$ then either $I = 0$ or $J = 0$. If neither $I$ nor $J$ were zero then there exists $i\neq 0 \in I$ and $j\neq 0 \in J$ such that $ij \neq 0 \in IJ$.
	\end{proof}
	The following provides another way of thinking about support of a finitely generated module over a neotherian ring:
	\begin{lemma}
		\label{lem:support_vanishing}
		If $M$ is a finitely generated $R$-module then \[\operatorname{Supp}M = V(\operatorname{ann}_R(M))\]
		where $V(\operatorname{ann}_R(M))$ is the vanishing set (Definition \ref{def:vanishing}).
	\end{lemma}
	\begin{proof}
		Let $x_1,...,x_n$ be a set of generators for $M$. We have
		\begin{align*}
			M_\frak{p} \neq 0 &\Longleftrightarrow \exists i\text{, }x_i/1 \neq 0\\
			&\Longleftrightarrow \exists i\text{, }\operatorname{ann}_R(x_i) \subseteq \frak{p}\\
			&\Longleftrightarrow \operatorname{ann}_R(M) = \bigcap_{i = 1}^n \operatorname{ann}_R(x_i) \subseteq \frak{p}
		\end{align*}
		The $(\Longleftarrow)$ direction of the final implication uses that $M$ is finitely generated. Indeed, \[\prod_{i = 1}^n \operatorname{ann}_R(x_i) \subseteq \bigcap_{i = 1}^n\operatorname{ann}_R(x_i) \subseteq \frak{p} \Longrightarrow \exists i\text{, }\operatorname{ann}_R(x_i) \subseteq \frak{p}\]
		using Lemma \ref{lem:product}.
	\end{proof}
	\begin{thm}
		\label{thm:primary_single_elt}
		Let $R$ be Noetherian and $M$ a finitely generated $R$-module. A submodule $N \subseteq M$ is primary if and only if $\operatorname{Ass}_R(M/N)$ consists of a single element. In this case, $\sqrt{\operatorname{ann}_R(M/N)}$ is associated, and thus is the single element of $\operatorname{Ass}_R(M/N)$.
	\end{thm}
	\begin{proof}
		First say $\operatorname{Ass}_R(M/N) = \lbrace \frak{p} \rbrace$. Denote $\operatorname{ann}_R(M/N)$ by $I$. By \eqref{thm:min_elts} of Theorem \ref{thm:fin_sub_min} we have that $\operatorname{Supp}(M/N) = V(\frak{p})$, which by Lemma \ref{lem:support_vanishing} implies $V(\frak{p}) = V(I)$, and thus by Corollary \ref{cor:vanishing_subset}, $\frak{p} = \sqrt{I}$. Using this, if $a \in R$ is a zero divisor, and so by \ref{lem:zero-div_ass-primes} of Lemma \ref{lem:ass_nonempty_zero_div}, $a \in \frak{p}$, then $a \in \sqrt{I}$. That is, $N$ is primary.
		
		Conversely, say $N$ is primary and let $\frak{p}$ be associated. For any $a \in \frak{p}$ we have that $a$ is a zero divisor, and thus $a \in \sqrt{I}$. This shows that $\frak{p} \subseteq \sqrt{I}$, and by the definition of associated prime we clearly have the reverse inclusion.
	\end{proof}
	\begin{remark}
		\emph{Recall that an ideal $I$ with the property that $\sqrt{I}$ is prime need not be such that $I$ is primary (Remark \ref{rem:radical_prime}). So we do not obtain from Theorem \ref{thm:primary_single_elt} for free that in the context given there, $\operatorname{ann}_R(M/N)$ is primary. This however is true (we continue to denote $\operatorname{ann}_R(M/N)$ by $I$): say $ab \in I$ and $a \not\in I$, then $ab(M/N) = 0$ and $a(M/N) \neq 0$, which means $b$ is a zero-divisor of $M/N$ and so $b \in \sqrt{I}$ as $N$ is primary.}
	\end{remark}
	\begin{defn}
		If $M$ is a finitely generated $R$ module with $R$ Noetherian, $N \subseteq M$ is primary, and $\operatorname{Ass}_R(M) = \lbrace \frak{p}\rbrace$ then $M$ is \textbf{$\frak{p}$-primary}.
	\end{defn}
	\begin{defn}
		Let $M$ be an $R$-module, $M$ finitely generated and $R$ Noetherian. 
		\begin{itemize}
			\item The module $M$ is \textbf{reducible} if there exists submodules $N_1,N_2 \subseteq M$ such that $N_1 \cap N_2 = M$ with $N_1 \neq M$ and $N_2 \neq M$. If $M$ is not reducible it is \textbf{irreducible},
			\item An \textbf{irreducible decomposition} of $M$ is a finite set of modules $N_1,...,N_n \subseteq M$ such that $N_1 \cap \hdots \cap N_n = M$,
			\item A \textbf{primary decomposition} of $M$ is a set of primary modules $N_1,...,N_n$ such that $N_1 \cap \hdots \cap N_n = M$,
			\item If $N_1 \cap ... \cap N_n$ is either type of decomposition and moreover for all $j$ satisfies: $N_1 \cap ... \cap \hat{N}_j \cap ... \cap N_n \neq M$ (where $\hat{N}_j$ means to omit $N_j$) then the decomposition is \textbf{irredundant}.
		\end{itemize}
	\end{defn}
	\begin{lemma}
		\label{lem:irred_primary_ass} Let $M$ be a finitely generated $R$ module with $R$ Noetherian. If $N = N_1 \cap ... \cap N_n$ is an irredundant, primary decomposition of a proper submodule $N \subseteq M$ where $N_i$ is $\frak{p}_i$-primary then $\operatorname{Ass}_R(M/N) = \lbrace \frak{p}_1,...,\frak{p}_n\rbrace$.
	\end{lemma}
	\begin{proof}[Proof of Lemma \ref{lem:irred_primary_ass}]
		By replacing $M$ with $M/N$ we can assume that $N = 0$. The module $M$ is isomorphic to a submodule of $\bigoplus_{i = 1}^nM/N_i$ and so
		\[\operatorname{Ass}_R(M) = \operatorname{Ass}_R\Big(\bigoplus_{i = 1}^nM/N_i\Big) \subseteq \bigcup_{i = 1}^n \operatorname{Ass}_RM/N_i = \lbrace \frak{p}_1,...,\frak{p}_n\rbrace\]
		where the inclusion is by Theorem \ref{thm:shortexactsequence_ass}.
		
		For the reverse inclusion, pick an arbitrary $\frak{p}_i$, we will construct explicitly an element $y\in M$ such that $\operatorname{ann}_R(y) = \frak{p}_i$. By irredundancy, $N_1 \cap ... \cap \hat{N}_i \cap ... \cap N_n \neq 0$, so choose some element $x \neq 0$ of this module. We claim there exists $\nu > 0$ such that $\frak{p}_i^\nu x = 0$. We know that $N_i$ is $\frak{p}_i$ primary which means $\frak{p}_i = \sqrt{\operatorname{ann}_R(M/N_i)}$. By Lemma \ref{lem:radical_power} there exists $\nu > 0$ such that $\frak{p}_i^\nu M \subseteq N_i$, and so $\frak{p}_i^{\nu}x = 0$, establishing the claim.
		
		Assume that $\nu$ is such that $\frak{p}_i^{\nu - 1}x \neq 0$ and pick any non-zero element of this module, we take this to be $y$. We have that $\frak{p}_iy = 0$ and so $\frak{p}_i \subseteq \operatorname{ann}_R(y)$, it remains to show this is an equality.
		
		Since $N_i$ is primary and $\frak{p}_i = \sqrt{\operatorname{ann}_R(M/N_i)}$ it suffices to show that every element of $\frak{p}_i$ is a zero-divisor of $M/N_i$. We know that $\frak{p}_i y = 0$ so this reduces to showing $y \not\in N_i$. Say $y \in N_i$. As $y$ is a scalar multiple of $x$, and $x \in N_j$ for all $i \neq j$, we have $y \in N_1$ iff $y = 0$, thus $y \neq 0$.
	\end{proof}
	\begin{fact}
		\label{fact:irreducible_decomp} Every finitely generated module over a Noetherian ring admits an irreducible decomposition.
	\end{fact}
	\begin{proof}
		If the module is reducible, reduce it. This terminates as the module is Noetherian.
	\end{proof}
	\begin{lemma}
		\label{lem:irred_primary} All irreducible modules are primary.
	\end{lemma}
	\begin{proof}
		Let $N \subseteq M$ be a submodule which is not primary. By replacing $M$ by $M/N$ we can assume that $N = 0$. Moreover, assume $N_1 \cap N_2 = 0$. By Theorem \ref{thm:primary_single_elt} we have that $\operatorname{Ass}_R(M)$ has at least two elements $\frak{p}_1,\frak{p}_2$. Thus there are two submodules of $K_1,K_2 \subseteq M$ such that $K_i \cong A/\frak{p}_i$. For any non-zero element $x \in K_i$ we have $\operatorname{ann}_R(x) = \frak{p}_i$ and so $K_1 \cap K_2 = 0$, that is, $0$ is reducible.
	\end{proof}
	Fact \ref{fact:irreducible_decomp} and Lemma \ref{lem:irred_primary} together show that every module admits a primary decomposition, in fact, more can be said, see \cite[\S 2.6 Thm 6.8]{matsumura}
	%
	\section{Polynomial rings}
	\subsection{The quotient of a polynomial ring by a maximal ideal}
	\label{subsec:quotient_polynomial}
	Given a field $F$ and maximal ideal $\frak{m}$ of the polynomial ring $F[x_1,...,x_n]$ we obtain a field extension $F[x_1,...,x_n]/\frak{m}$ of $F$. The following shows that this is always an \emph{algebraic} extension:
	\begin{lemma}
		\label{lem:algebraic}
		Let $\frak{m}$ be a maximal ideal of $F[x_1,...,x_n]$, then $F[x_1,...,x_n]/\frak{m}$ is an algebraic extension of $F$.
	\end{lemma}
	This Lemma is a special case of the following more general result:
	\begin{lemma}
		\label{converse}
		Let $K/F$ be some field extension, and say $k_1,...,k_n \in K$ are such that $F[k_1,...,k_n]$ is an integral domain. If $F[k_1,...,k_n]$ is a field, then it is an algebraic extension of $F$.
	\end{lemma}
	Notice that once this is established, Lemma \ref{lem:algebraic} follows by simply making the observation that $$F[x_1,...,x_n]/\frak{m} = F[[x_1]_{\frak{m}},...,[x_n]_{\frak{m}}]$$
	We will need the following lemmas:
	\begin{lemma}
		\label{lem:extension}
		Let $k$ be a field and $l$ an algebraic extension. Then for any finite sequence of elements in $l$, $(l_1,...,l_n)$:
		\begin{itemize}
			\item $k[l_1,...,l_n] = k(l_1,...,l_n)$, and
			\item there exists polynomials $f_i \in k[x_1,...,x_i]$ for $i= 1,...,n$ such that $\operatorname{ker}\varphi_n = (f_1,...,f_n)$ where $\varphi_n: k[x_1,...,x_n] \to k(l_1,...,l_n)$ is the map defined by $x_i \mapsto l_i$.
		\end{itemize}
	\end{lemma}
	\begin{proof}
		The first claim is proved by induction on $n$. First notice that the ideal generated by the minimal polynomial $f_1$ of $(l_1)$ is contained within the kernel of the surjective map $\varphi_1: k[x_1] \to k[l_1]$ defined by $\varphi_1(x_1) = l_1$. Moreoever, if $p \in k[x_1]$ is such that $\varphi_1(p) = 0$, ie, $p(l_1) = 0$, then we can divide by $f_1$ to obtain $p = f_1 q + r$. Notice that $r(l_1) = 0$. To avoid contradicting minimality of $f_1$, it must be that $r = 0$, that is, $p \in (f_1)$. Thus $(f_1) = \operatorname{ker}\varphi_1$. As $f_1$ is minimal, $(f_1)$ is maximal, thus $k[x_1]/\operatorname{ker}\varphi_1 = k[x_1]/(f_1) \cong k[l_1]$ is a field, that is, $k[l_1] = k(l_1)$.\\\\
		%
		The inductive step is similar; first notice that $k[l_1,...,l_r] = (k[l_1,...,l_{r-1}])[l_r]$ which by the inductive hypothesis is equal to $k(l_1,...,l_{r-1})[l_r]$. As proven in the base case, the map
		\[k(l_1,...,l_{r-1})[x_r] \twoheadrightarrow k(l_1,...,l_{r-1})[l_r]\]
		has kernel given by the ideal generated by the minimal polynomial $g_r \in k(l_1,...,l_{r-1})[x_r]$ of $l_r$. Again, since $g_r$ is minimal, $(g_r)$ is maximal, thus $k(l_1,...,l_{r-1})[l_r] = k(l_1,...,l_{r-1})(l_r) = k(l_1,...,l_r)$.\\\\
		%
		For the second claim, for all $r = 1,...,n$, since $k(l_1,...,l_{r-1}) = k[l_1,...,l_{r-1}]$ there exists a polynomial $f_r \in k[x_1,...,x_{r-1}]$ such that $f(l_1,...,l_{r-1},x_r) = g_r$. So if $p \in \operatorname{ker}\varphi_n$, ie, if $p$ is such that $p(l_1,...,l_n) = 0$, we can divide $p$ as a polynomial in $x_n$ by $f_n$ to obtain $p = f_n q_n + r_n$ for some $q_n$ and $r_n(l_1,...,l_{n-1},x_n)$ either equal to $0$ or such that $\operatorname{deg}(r_n(l_1,...,l_{n-1},x_n)) < \operatorname{deg}(f_n)$. By minimality of $g_n$, it follows that $r_n(l_1,...,l_{n-1},x_n) = 0$. We can thus divide $r_n$ by $f_{n-1}$ to obtain $r_n = f_{n-1}q_{n-1} + r_{n-2}$. Repeating this process finitely many times yields
		\[p = \sum_{i =1}^n \big(f_nq_n + r_{n-1}\big)\]
		where $r_0 = 0$. Thus $p \in (f_1,...,f_n)$.
	\end{proof}
	The first dotpoint of Lemma \ref{lem:extension} can be extended to the case where infinitely many elements of $l$ are taken, this is a useful result and so we include it here, but only the finite version will be used to prove the Nullstellensatz.
	\begin{lemma}
		\label{lem:ext_infinite}
		Let $F/k$ be an algebraic extension and $L \subseteq F$ a subfield. Then $k[L] = k(L)$.
	\end{lemma}
	\begin{proof}
		We prove that every non-zero element $x$ of $k[L]$ is a unit. Write $x = \alpha_1 x_1 + ... + \alpha_n x_n$ for elements $\alpha_i \in k, x_i \in L$. By the finite case we have $k(x_1,...,x_n) = k[x_1,...,x_n] \subseteq k[L]$.
	\end{proof}
	\begin{proof}[Proof of Lemma \ref{converse}]
		We will prove the contrapositive. It can be assumed that $k_1,...,k_n$ are ordered such that $k_1,...,k_r$ form a transcendence basis of $F(k_1,...,k_n)$ so that $F(k_1,...,k_n)$ is an algebraic extension of $F(k_1,...,k_r)$. By Lemma \ref{lem:extension} there exists $f_{r+i} \in F(k_1,...,k_r)[x_{r+1},...,x_i]$ such that the kernel of the map $F(k_1,...,k_r)[x_{r+1},...,x_n] \to F(k_1,...,k_n)$ which maps $x_{r +i}$ to $k_{r+ i}$ is given by $(f_{r+1},...,f_n)$. Since the coefficients of each $f_{r+i}$ are in $F(k_1,...,k_r)$, by clearing denominators, there exists $g \in F[k_1,...,k_r]$ such that for all $i$, $gf_{r+i} \in F[k_1,...,k_r,x_{r+1},...,x_n]$. In other words, for all $i$, $$f_{r+i} \in (F[k_1,...,k_r,x_{r+1},...,x_n])_g$$
		Now, $(F[k_1,...,k_r])_g$ is not a field, as $F[k_1,...,k_r]$ is isomorphic to a polynomial ring with infinitely many irreducible elements, so we can pick an irreducible element which is not in the unique factorisation of $g$, this element will not be a unit in $(F[k_1,...,k_r])_g$. Thus there exists a non-trivial ideal $I$ of $(F[k_1,...,k_r])_g$. The module $(F[k_1,...,k_n])_g$ is free over $(F[k_1,...,k_r])_g$, a fact we leave as an exercise, and so $I(F[k_1,...,k_n])_g$ is a non-trivial ideal of $(F[k_1,...,k_n])_g$. Lastly, notice that if $F[k_1,...,k_n]$ were a field, then so would be $(F[k_1,...,k_n])_g$, thus $F[k_1,..,k_n]$ is not a field.
	\end{proof}
	\subsection{Hilbert's Nullstellensatz}
	The goal of this section is to prove Hilbert's Nullstellensatz, for part \ref{power} of Theorem \ref{thm:hilbertsnullstellensatz} we will need the content of Section \ref{subsec:quotient_polynomial}. Throughout, $F$ is a field:
	\begin{defn}
		\label{def:algzero}
		An \textbf{algebraic zero} of a subset $\Phi \subseteq F[x_1,...,x_n]$ is a sequence $(\alpha_1,...,\alpha_n)$ of elements in an algebraic closure $\bar{F}$ such that $f(\alpha_1,...,\alpha_n) = 0$ for all $f \in \Phi$.
	\end{defn}
	Notice that if a root exists in any algebraic closure it exists in them all, so it makes sense to talk about an algebraic zero in absence of a particular algebraic closure.
	\begin{thm}
		\label{thm:hilbertsnullstellensatz}
		Let $\Phi \subseteq F[x_1,...,x_n]$, and write $(\Phi)$ for the ideal generated by $\Phi$,
		\begin{enumerate}
			\item\label{generates} if $\Phi$ admits no algebraic zeros, then $(\Phi) = F[x_1,...,x_n]$.
			\item\label{power} let $f \in F[x_1,...,x_n]$ be such that $f(\alpha_1,...,\alpha_n) = 0$ for all algebraic zeros $(\alpha_1,...,\alpha_n)$ of $\Phi$, then there exists $r > 0$ such that $f^r \in (\Phi)$.
		\end{enumerate}
	\end{thm}
	First we show how \ref{generates} proves \ref{power}.
	\begin{proof}[Proof of part \ref{generates} of Theorem \ref{thm:hilbertsnullstellensatz}]
		Consider the set $\Phi \cup \lbrace 1 - fy\rbrace \subseteq F[x_1,...,x_n,y]$. Then by the assumption of $f$, this set has no algebraic zeros. Thus by \ref{generates} $(\Phi \cup \lbrace 1 - fy\rbrace) = f[x_1,...,x_n,y]$, so there exists sets of polynomials $\lbrace h_i\rbrace_{i \in I} \subseteq \Phi$, $\lbrace p_i\rbrace_{i \in I} \subseteq F[x_1,...,x_n,y]$ and polynomial $q \in F[x_1,...,x_n,y]$ such that
		\[1 = \sum_{i \in I}p_i(x,y)h_i(x) + q(1 - f(x)y)\]
		Thus the image of both sides of the equation are equal under the map $F[x_1,...,x_n,y] \to (F[x_1,...,x_n])_f$ given by substituting $1/f$ for $y$ are equal, ie,
		\[1 = \sum_{i \in I}p_i(x,1/f(x))h_i(x) \in (F[x_1,...,x_n])_f\]
		clearing denominators then gives the result.
	\end{proof}
	\begin{proof}[Proof of part \ref{power} of Theorem \ref{thm:hilbertsnullstellensatz}]
		Assume that $(\Phi) \neq F[x_1,...,x_n]$ and let $\frak{m}$ be a maximal ideal containing $(\Phi)$. $F[x_1,...,x_n]/\frak{m}$ over $F$ being algebraic (\ref{lem:algebraic}) admits an embedding $\theta$ into $\bar{F}$. For any $f \in \Phi$, $f(\alpha_1,...,\alpha_m) = f(\theta([x_1]),...,\theta([x_n])) \in \operatorname{ker}(\theta)$, and so $(\theta([x_1]),...,\theta([x_n]))$ is an algebraic zero of $\Phi$.
	\end{proof}
	%
	\subsection{Hilbert's Basis Theorem}
	\begin{thm}[Hilbert's Basis Theorem]
		If $R$ is Noetherian then so is $R[x]$.
	\end{thm}
	\begin{proof}
		Say $I \subseteq R[x]$ is an ideal which is not finitely generated. Let $f_0 \in I$ be of minimal degree, and $f_r \in I\setminus(f_0,...,f_{r-1})$ be of minimal degree (note $\setminus$ here is set exclusion, not modulus). Denote by $a_i$ the coefficient of the leading term of $f_i$. The sequence $(a_0) \subseteq (a_0,a_1) \subseteq (a_0,a_1,a_2) \subseteq \hdots$ eventually stabilises and so that $(a_0,...,a_{N-1}) = (a_0,...,a_n)$ for any $n \geq N$. Thus we can write
		\[a_N = \sum_{i = 0}^{N-1}u_i a_i\]
		for some $u_i \in R$. Consider the following polynomial:
		\[g = \sum_{i = 0}^{N-1}u_ix^{\operatorname{deg}f_N - \operatorname{deg}f_i}f_i\]
		which has the same leading term as $f_N$ and is in $(f_0,...,f_{N-1})$. $f_N$ itself is not in $(f_0,...,f_{N-1})$ and so neither is $g - f_{N}$, which has smaller degree than $f_{N}$, contradicting minimality.
	\end{proof}
	\begin{cor}
		Every finitely generated algebra over a Noetherian ring is Noetherian.
	\end{cor}
	\begin{proof}
		Using that quotients of Noetherian rings are Noetherian.
	\end{proof}
	%
	\subsection{Noether normalisation}
	There is a great note by Hochster \url{http://www.math.lsa.umich.edu/~hochster/615W10/supNoeth.pdf}.
	%
	We extend the notion of \emph{algebraic independence} (Definition \ref{def:algebraic_independence}) to make sense over any $k$-algebra (not just over a field):
	\begin{defn}
		Let $A$ be a $k$-algebra. A set of elements $\lbrace \alpha_1,...,\alpha_n \rbrace \subseteq A$ are \textbf{algebraically independent} if the ring morphism $k[x_1,...,x_n] \lto A$ which maps $x_i \mapsto \alpha_i$ is injective.
	\end{defn}
	%
	\begin{lemma}
		\label{lem:noether_normalisation}
		Let $k$ be a field and $A\cong k[\alpha_1,...,\alpha_n]$ a finitely generated $k$-algebra. Then there exists algebraically independent elements $\lbrace \beta_1,...,\beta_r\rbrace \subseteq A$ such that $A$ is a finite $k[\beta_1,...,\beta_r]$-module. In other words, every finitely generated $k$-algebra is a finite module over a polynomial ring.
	\end{lemma}
	\begin{proof}
		We proceed by induction on $n$. $k$ is a finite $k$-module so the case when $n =0$ is trivial. Say $n > 0$ and the result holds for $k$-algebras finitely generated by $n-1$ elements. If $n = r$ then we can take $\beta_i = \alpha_i$ and then $A$ is finitely generated by $1$ over $A$. So, assume there exists a non-zero polynomial $f \in k[x_1,...,x_n]$ such that $f(\alpha_1,...,\alpha_n) = 0$. Take $N$ to be any integer which is greater than every exponent of every $x_i$ in $f$. Consider the following set of generators of $A$:
		\[\lbrace \alpha_i' := \alpha_i - \alpha_n^{N^i}\text{,  for }i < n\text{ and }\alpha_n\rbrace\]
		These satisfy the polynomial $g(x_1,...,x_n) := f(x_1 + x_1^{N},x_2 + x_2^{N^2},...,x_{n-1} + x_{n-1}^{N^{n-1}}, x_n)$, moreover, for every monomial $x_1^{d_1}...x_{n-1}^{d_{n-1}}x_{n}^{d_n}$ in $f$ we have $(x_1 + x_n^N)^{d_1}...(x_{n-1} + x_{n}^{N^{n-1}})^{d_{n-1}}x_n^{d_n}$ whose highest degree monomial is given by $x_n^{d_n + d_1N + ... + d_{n-1}N^{n-1}}$ whose exponent, by the uniqueness of representations of integers base $N$, is uniquely determined by $d_1,...,d_n$. This means that in $g$ none of these terms cancel out, and so there exists a highest degree power of $x_n$ in $g$ and it is of the form $cx_n^m$ for some $c \in k$ and integer $m$.
		
		We can divide through by $c$ to replace $g$ with a monic polynomial $h$ in $x_n$ with coefficients in $k[x_1,...,x_{n-1}]$ such that $h(\alpha_1',...,\alpha_{n-1}',\alpha_n) = 0$. This shows that $\alpha_n$ is integral over $k[\alpha_1',...,\alpha_{n-1}']$ and thus $k[\alpha_1',...,\alpha_{n-1}',\alpha_n]$ is a finite $k[\alpha_1',...,\alpha_{n-1}']$-module (Lemma \ref{lem:finite_integral}). Since $k[\alpha_1',...,\alpha_{n-1}']$ is generated by $n-1$ elements, the inductive hypothesis implies there is algebraically independent elements $\beta_1,...,\beta_{l}$ of the ring $k[\alpha_1',...,\alpha_{n-1}']$ such that $k[\alpha_1',...,\alpha_{n-1}']$ is a finite $k[\beta_1,...,\beta_l]$ module. The result follows by transitivity of finiteness of modules.
	\end{proof}
	If $A$ is a $k$-integral domain, then $l = \operatorname{tr.deg}_kA$. This is because $k[\beta_1,...,\beta_l] \lto A$ is finite and thus integral, which in turn implies $k(\beta_1,...,\beta_l) \lto \operatorname{Frac}A$ is algebraic (Lemma \ref{lem:int_implies_alg}). Thus $\operatorname{tr.deg}_kA = \operatorname{tr.deg}_kk(\beta_1,...,\beta_l) = l$.
	\begin{example}
		Let $A$ be the finitely generated $k$-algebra $k[x_1,x_2,x_3,x_4]/(x_1x_2 - x_3x_4)$ which we write as $k[\alpha_1,...,\alpha_4]$. Then $f(X_1,X_2,X_3,X_4) := X_1X_2 - X_3X_4$ is such that $f(\alpha_1,...,\alpha_4) = 0$, so consider the polynomial
		\[f(X_1 + X_4^2,X_1 + X_4^{4}, X_1 + X_4^8, X_4) = (X_1 + X_3^2)(X_2 + X_4^4) - (X_3 + X_4^8)X_4 = \hdots + X_4^9\]
		which is a monic polynomial such that $f(\alpha_1 - \alpha_4^2, \alpha_2 - \alpha_4^4, \alpha_3 - \alpha_4^8, \alpha_4) = 0$. This shows that $\alpha_4$ is integral over $k[\alpha_1,\alpha_2,\alpha_3]$ and thus $k[\alpha_1,\alpha_2,\alpha_3,\alpha_4]$ is a finite $k[\alpha_1,\alpha_2,\alpha_3]$-module with generating set $\lbrace 1, \alpha_4,...,\alpha_4^l\rbrace$ for some $l$. Moreover, $\lbrace \alpha_1,\alpha_2,\alpha_3\rbrace$ is an algebraically independent set, and so $A$ is a finitely generated $k[\alpha_1,\alpha_2,\alpha_3]$-module.
	\end{example}
	%
	\section{Fields}
	\subsection{Algebraic closure}
	\label{sec:fieldextensions}%
	Every integral domain $R$ can canonically be embedded within a field in the following way:
	\begin{defn}
		Let $\operatorname{Frac}(R)$ be the \textbf{field of fractions} of $R$, the construction of which mimics that of the rational numbers from the integers: The underlying set of $\operatorname{Frac}(R)$ consists of equivalence classes of pairs $(x,y) \in R\times R\setminus\lbrace 0 \rbrace$ where two pairs $(x,y),(x',y')$ are equivalent if $xy' - x'y = 0$. Addition is defined by $(x,y) + (x',y') = (xy' + x'y,yy')$ and multiplication $(x,y)\cdot(x',y') = (x\cdot x',y\cdot y')$. The canonical injection $\varphi_R$ is given by $x \mapsto (x,1)$.
	\end{defn}
	%
	This field is minimal as made precise by the following Lemma:
	\begin{lemma}
		\label{uniquenessoffff}
		Say $R$ is an ID and let $\psi: R \to F$ is a ring homomorphism where $F$ is a field. Then there exists a unique morphism $\gamma: \operatorname{Frac}(R) \to F$ such that the following diagram commutes
		\[
		\begin{tikzcd}
			R\arrow[r,"{\varphi_R}"]\arrow[dr,swap,"{\psi}"] & \operatorname{Frac}(R)\arrow[d,"{\gamma}"]\\
			& F
		\end{tikzcd}
		\]
	\end{lemma}
	\begin{proof}
		The map $\gamma(x,y) = \psi(x)\cdot\psi(y)^{-1}$ is the unique map.
	\end{proof}
	If a field $F$ is such that for every polynomial $p \in F[x]$ there exists $f \in F$ which is a root of $p$, then $F$ is said to be \textbf{algebraically closed}.
	\begin{lemma}
		\label{algebraicclosure}
		Every field $F$ can be embedded into an algebraically closed field $\bar{F}$.
	\end{lemma}
	\begin{proof}
		Let $\Lambda$ be the collection of monic, irreducible polynomials with coefficients in $F$. For each $f \in F$, let $u_{f,0},...,u_{f,d}$ be formal indeterminants, where $d$ is the degree of $f$. Let $F[\lbrace U\rbrace]$ be the polynomial ring over $F$ where $U$ is the collection of all $u_{f,i}$. Write
		\[f - \prod_{i = 0}^d(x - u_{f,i}) = \sum_{i = 0}^{d-1}\alpha_{f,i}x^i \in F[\lbrace U \rbrace][x]\]
		Let $I$ be the ideal generated by $\alpha_{f,i}$. $I$ is not all of $F[\lbrace U \rbrace]$ so there exists a maximal ideal $M$ containing $I$. Let $F_1 = F[\lbrace U \rbrace]/M$. Repeat this process to define $f_i$ for all $i > 0$. Then $\cup_{i = 1}^\infty F_i$ is algebraically closed which $F$ embeds into, and moreover is an algebraic extension of $F$.
	\end{proof}
	This constructed field will be denoted $\bar{F}$ and it along with the embedding $F \rightarrowtail \bar{F}$ is called the \textbf{algebraic closure} of $F$ and is denoted $\bar{F}$. It is essentially unique in a way made precise by the following Lemma:
	\begin{lemma}
		Let $F$ be a field and $\varphi: F \to L$ a ring homomorphism such that $L$ is algebraic over  $F$. Then if $L$ is algebraically closed, $L \cong \bar{F}$.
	\end{lemma}
	\begin{proof}
		The collection of pairs $(K,\sigma)$ where $K$ is an algebraic extension of $F$ and $\sigma: K \to L$ is a ring homomorphism, with partial order $(K,\sigma) < (K',\sigma')$ defined by $K \rightarrowtail K'$ and $\sigma'\restriction_{K} = \sigma$ defines a non-empty poset closed under ascending chains. By zorn's Lemma, there thus exists a maximal element which can be shown to be $\bar{F}$. Since $L$ is algebraic over $F$ it then follows that $\sigma: K \to L$ is surjective, thus this is an isomorphism.
	\end{proof}
	\begin{notation}
		Given a field extension $K/F$, and elements $k_1,...,k_n \in K$ we denote
		\begin{itemize}
			\item the smallest subring of $K$ containing $F$ and $k_1,...,k_n$ by $F[k_1,...,k_n]$,
			\item the smallest subfield of $K$ containing $F$ and $k_1,...,k_n$ by $F(k_1,...,k_n)$.
		\end{itemize}
	\end{notation}
	Notice that $F(k_1,...,k_n) \cong \operatorname{Frac}(F[k_1,...,k_n])$. So we can define these notions without the presence of a field extension:
	\begin{notation}
		\label{not:smstring_smstfield_noext}
		Given a field $k$ we denote
		\begin{itemize}
			\item $k[x_1,...,x_n]/I$ by $k[\alpha_1,...,\alpha_n]$,
			\item $\operatorname{Frac}\big(k[x_1,...,x_n]/I\big) = \operatorname{Frac}k[\alpha_1,...,\alpha_n]$ by $k(\alpha_1,...,\alpha_n)$.
		\end{itemize}
	\end{notation}
	
	\subsection{Transcendence degree}
	\label{transcendence}
	Throughout, let $K/F$ be a field extension.
	\begin{defn}
		An element $f$ of a field $F$ is \textbf{transcendental} if whenever $p \in F[x]$ admits $f$ as a root, $p$ is the zero polynomial.
	\end{defn}
	Similarly, there are \emph{algebraically independent sets}:
	\begin{defn}\label{def:algebraic_independence}
		A subset $S \subseteq F$ is \textbf{algebraically independent} if the map
		\begin{align*}
			K[x_s\mid s\in S] &\to F
		\end{align*}
		which maps $x_s \mapsto s$ is injective.
	\end{defn}
	\begin{defn}
		An algebraically independent subset $S$ of $F$ is a \textbf{transcendence basis} of $K/F$ if $F$ is an algebraic extension of $K(S)$.
	\end{defn}
	\begin{lemma}
		A transcendence basis always exists, and the cardinality of any two such bases are always equal.
	\end{lemma}
	\begin{proof}
		That a transcendence basis always exists can be shown using a similar method to how a basis for a vector space always exists; apply Zorn's Lemma to the poset of algebraically independent sets $S$ of $K$ to yield a maximal element $B$ (note: if this poset is empty, then the empty set can be taken as a basis for $K/F$). It can then be shown that $K$ is an algebraic extension of $F(B)$ \cite[\S 9.26]{stacksproject}.\\\\
		%
		Next we prove the following statement by induction on $n$: if $E/J$ is any field extension, and $B = \lbrace b_1,...,b_n\rbrace,B' = \lbrace b_1',...,b_m'\rbrace$ for some $m \leq n$ are bases for $E/J$ then $m=n$. This establishes the case of the claim when the cardinality of the two bases are finite.\\\\
		%
		If $n$ = 0 then $E/J$ is an algebraic extension, which means $n = m = 0$.\\\\
		%
		Now say $n > 0$. Since $B'$ is a basis, there exists a polynomial $f \in J[x,y_1,...,y_m]$ such that $f(b_1,b_1',...,b_m') = 0$. This polynomial $f$ must involve $x$ and some $y_i$, lest either $B'$ not be a basis, or $b_1$ be algebraic over $J$. Without loss of generality, assume $i = 1$.\\\\
		%
		Let $B^* = \lbrace b_1,b_2',...,b_m'\rbrace$. Our next claim is that $B^*$ is algebraically independent over $J$. Indeed, if $g \in J[x_1,...,x_m]$ were such that $g(b_1,b_2',...,b_m') = 0$, where $g$ necessarily involves $x_1$, then $b_1$ is algebraic over $J(b_2',...,b_m')$. This in turn implies $b_1'$ is algebraic over $J(b_2',...,b_m')$, due to the existence of $f$.\\\\
		%
		Thus $\lbrace b_2,...,b_n\rbrace$ and $\lbrace b_2',...,b_m'\rbrace$ are bases for $E/J(b_1)$, which by the inductive hypothesis implies $n = m$.\\\\
		%
		Now say $B,B'$ are such that $|B'| \leq |B|$ and $|B|$ is infinite, it will be shown throughout the course of this part of the argument that it is necessarily the case that $|B'|$ is also infinite, so this is the last case to consider.\\\\
		%
		For each $b \in B'$ choose a polynomial $p[x_1,...,x_n]$ and elements $b_2,...,b_n$ of $B$ such that $p(b,b_2,...,b_n) = 0$. Let $B^\ast$ be the set containing all such $b_i$ for all such $p$. Then $B^\ast \subseteq B$ and we claim moreover that $B^\ast = B$. To see this, say $\beta \in B\setminus B^\ast$. Then $\beta$ is algebraic over $F(B')$ and so is algebraic over $F(B^\ast)$, a contradiction. Thus $|B| = |B^\ast|$ which since $|B|$ is infinite implies that $|B'|$ is infinite. It now follows from $|B'|$ being infinite that $|B^*| = |B'|$.
	\end{proof}
	\begin{lemma}
		Any generating set contains a transcendence basis.
	\end{lemma}
	\begin{proof}
		Similar to the corresponding statement about bases of vector spaces (we are working with fields here).
	\end{proof}
	%
	\subsection{Perfect fields and separable elements}
	Throughout, $k$ is a completely arbitrary field, possibly not algebraically closed, possibly of positive characteristic.
	Say $k$ has characteristic $p$, denote by $k^p$ the image of the \textbf{Frobenius Endomorphism} on $k$ which maps $x \mapsto x^p$. This is indeed a homomorphism, with additivity following from the important relation $(x + y)^p = x^p + y^p$. Since $k$ is a field we have that $x^p = 0$ implies $x = 0$ so indeed this map is injective. Of particular interest is the case when this endomorphism is also surjective:
	\begin{defn}
		A field is \textbf{perfect} if the characteristic is $0$, or it is not $0$ and the Frobenius Endomorphism is an isomorphism.
	\end{defn}
	\begin{example}
		Examples and a non-example of perfection:
		\begin{itemize}
			\item If $k$ is finite then the Frobenius Endomorphism is an injective map between two sets with equal cardinality, and thus is an isomorphism. So every finite field is perfect.
			\item If $k$ is algebraically closed then it is perfect.
			\item Let $\bb{F}_p$ be the finite field of characteristic $p > 0$. The field $\bb{F}_p(t)$ is not perfect, see Example \ref{ex:imperfect}.
		\end{itemize}
	\end{example}
	\begin{lemma}
		\label{lem:irred_UFD}
		Let $A$ be a UFD. If $f \in A$ is an irreducible polynomial of positive degree then its image in $\operatorname{Frac}A$ is also irreducible.
	\end{lemma}
	\begin{proof}
		Write $f = f_1f_2$ for polynomials $f_1,f_2 \in \operatorname{Frac}A$, we can write $f_i = \frac{f_i'}{a_i}$ with $a_i \in A$. We now have that $f$ divides $f_1'f_2'$ and since $f$ is irreducible and $A$ is a UFD we thus have $f$ is prime and so $f$ divides either $f_1'$ or $f_2'$, say $f$ divides $f_1'$. This implies $\operatorname{deg}f \leq \operatorname{deg}f_1'$.  As $A$ is an integral domain we also have $\operatorname{deg}f_1' + \operatorname{deg}f_2' = \operatorname{deg}f$. It follows that $\operatorname{deg}f_2' = 0$ and so $f_2$ is a unit. Since $\operatorname{deg}f > 0$ it follows that $f_1$ is not a unit, thus $f$ is irreducible in $\operatorname{Frac}A$.
	\end{proof}
	%
	\begin{defn}
		\label{def:pthroot}
		An element $a$ of a field $k$ \textbf{admits an $l^{\operatorname{th}}$ root} if there exists $b \in k$ such that $b^l = a$.
	\end{defn}
	%
	An alternative condition for a field being perfect will involve its \emph{formal derivative}:
	\begin{defn}
		The \textbf{formal derivative} (often abbreviated to \textbf{derivative}) of a polynomial $f = \sum_{i = 0}^n a_i x^i \in k[x]$ is $f' := \sum_{i = 1}^{n-1} ia_ix^{i-1}$.
	\end{defn}
	\begin{lemma}
		\label{lem:irred_example}
		Let $k$ be a field of characteristic $p$ and let $a \in k$ be an element which does not admit a $p^\text{th}$ root. For any $e \geq 0$, the polynomial $x^{p^e} - a$ is irreducible in $k[x]$.
	\end{lemma}
	\begin{proof}
		We proceed by induction on $e$, the result holds trivially if $e = 0$. Assume $e > 0$ and the result holds for $e-1$. Let $f \in k[x]$ be a monic, irreducible polynomial which divides $x^{p^e} - a$. Let $d \geq 0$ be the greatest integer such that $f^d$ divides $x^{p^e} - a$ and let $g \in k[x]$ be such that 
		\begin{equation}
			\label{eq:quotient}
			f^h g = x^{p^e} - a
		\end{equation}
		Taking derivatives of both sides and dividing by $f^{d-1}$ we obtain:
		\begin{equation}
			\label{eq:prod_rule}
			0 = df'g + fg'
		\end{equation}
		This equation implies $g$ divides $fg'$. Since $\operatorname{gcd}(f,g) = 1$ it follows that $g$ divides $g'$, which means $g' = 0$. Thus $g \in k[x^p]$. Moreover, \eqref{eq:prod_rule} now reads $0 = df'g$ which implies $df' = 0$, that is, $f^d \in k[x^p]$. Equation \eqref{eq:quotient} now can be written as $f_1(x)g_1(x) = x^{p^{e-1}} - a$ where $f_1(x^p) = f(x)^d$ and $g_1(x^p) = g(x)$. By the inductive hypothesis, this is irreducible, and so $g_1$ is a uni. In fact, $g_1 = 1$ as both $x^{p^{e-1}} - a$ and $f_1$ are monic. We now have \[f_1(x) = x^{p^{e-1}} - a,\qquad f(x)^d = x^{p^e} - a\]
		We finish the proof by proving $d = 1$, first we show $p$ does not divide $d$. Say it did, then $f(x)^d$ would be a power of $f(x)^p$ which would imply all the coefficients of $f(x)^d$ have a $d^{\operatorname{th}}$ root, which would mean all the coefficients of $x^{p^e} - a$ would have a $d^{\operatorname{th}}$ root (as such elements form a subring), but this contradicts the assumption that $a$ does not have a $p^{\operatorname{th}}$ root. Since $p$ does not divide $d$, the equation $df' = 0$ implies $f' = 0$ which means $f \in k[x^p]$, so we can write $f(x^p) - f_2(x)$. Thus the equation $f_1(x^p) = f(x)^d$ implies $f_1(x) = f_2^d$ and so to avoid contradicting irreducibility of $x^{p^{e-1}}-a$ we must have that $d = 1$.
	\end{proof}
	\begin{defn}\label{def:separable_irred_poly}
		An irreducible polynomial $f \in k[x]$ is \textbf{separable} if $f' \neq 0$ and \textbf{inseperable} if $f' = 0$. An arbitrary polynomial $f \in k[x]$ of positive degree is is \textbf{separable} if its irreducible components are. Otherwise it is \textbf{inseparable}. 
	\end{defn}
	We now give an alternate characterisation of a field being \emph{perfect}:
	\begin{lemma}
		\label{lem:perf_alt_defn}
		A field $k$ is perfect if and only if every irreducible polynomial $f \in k[x]$ is separable.
	\end{lemma}
	\begin{proof}
		Assume $k$ is perfect. Let $f$ be an arbitrary polynomial with zero derivative. Then $f \in k[x^p]$ so we can write $f = \sum_{i = 0}^n \alpha_i x^i$ where $\alpha_i \in k$. Since $k$ is perfect there exists $\alpha'_i$ such that $(\alpha_i')^n = \alpha_i$. Thus we have $\sum_{i = 0}^n \alpha_i x^i = \big(\sum_{i = 0}^n \alpha_i' x\big)^n$. That is, $f$ is reducible.
		
		Conversely, say $k$ is imperfect and let $a \in k$ admit no $n^\text{th}$ root for some $n > 1$. Consider the polynomial $x^p - a$, this has zero-derivative so it remains to show that this is irreducible. This follows from Lemma \ref{lem:irred_example}.
	\end{proof}
	\begin{example}
		\label{ex:imperfect} The field $\bb{F}_p(t)$ is imperfect. It admits at least one irreducible, separable polynomial.
	\end{example}
	%
	\begin{defn}\label{def:separable_element}
		Given a field extension $K/k$, an element $a \in K$ which is algebraic over $k$ is \textbf{separable over $k$} if its minimal polynomial is. Otherwise it is \textbf{inseparable}.
	\end{defn}
	The following gives a reduction to the problem of separability of an element.
	\begin{lemma}
		\label{lem:sep_elt_alt_def}
		An element $a \in F$ of a field extension $F/k$ is separable if and only if $f'(a) \neq 0$ where $f$ is the minimal polynomial of $a$.
	\end{lemma}
	\begin{proof}
		We should that $a$ is \emph{inseparable} if and only if $f'(a) = 0$. If $f'(a) = 0$ then by minimality of $f$ we have that $f' = 0$. Conversely $f' = 0$ implies $f'(a) = 0$.
	\end{proof}
	%
	The following lemma show that the derivative of a polynomial which vanishes at a separable element also vanishes at that separable element, thus extending Lemma \ref{lem:sep_elt_alt_def}:
	\begin{lemma}
		\label{lem:seemingly_useless}
		Let $a \in k$ be an inseparable element of a field extension $F/k$ and let $g \in k[x]$ be a polynomial such that $g(a) = 0$, then $g'(a) = 0$.
	\end{lemma}
	\begin{proof}
		Let $f \in k[x]$ be the minimal polynomial of $a$. That $g(a) = 0$ implies $f$ divides $g$ and so $fh = g$ for some $h$, taking derivatives gives the result.
	\end{proof}
	\section{Field extensions}
	Extensions of algebraic objects can be studied at various levels of generality, we may have an extension of \emph{groups}, or an extension of \emph{rings}, etc. A bottom up approach would be to consider extensions of decreasingly ``bare" algebraic objects, perhaps starting at group extensions. However, the nature of the theory changes. For example, when considering an extension of fields $K/k$ in the special situation where $K$ is a finite dimensional vector space over $k$, one may ask ``what does the dimension of this vector space mean for the extension"? This is a question which in the more general setting of an extension of rings $A/B$ where $A$ is a finitely generated $B$-module cannot be asked.
	
	We thus consider the theory of field extensions in this Section, and the theory of extensions of rings (\emph{integral extensions}) separately in Section \ref{sec:integral_extensions_jacobson_rings}.
	\begin{defn}
		Given a field extension $K/k$, an element $\alpha \in K$ is:
		\begin{itemize}
			\item \textbf{algebraic} if there exists a polynomial $f \in k[x]$ such that $f(\alpha) = 0$. Since $k[x]$ is a UFD for any algebraic $\alpha$ there exists a unique, monic, irreducible polynomial $\hat{f} \in k[x]$ such that $\hat{f}(\alpha) = 0$ which we call the \textbf{minimal polynomial of $\alpha$},
			\item \textbf{purely inseparable over $k$} in the case where $k$ has characteristic $p \neq 0$ and there exists $e \geq 0$ such that $\alpha^{p^e} \in k$,
		\end{itemize}
	\end{defn}
	Recall also Definition \ref{def:separable_element} that given a field extension $K/k$ and an element $\alpha \in K$ is \emph{separable} if its minimal polynomial admits a nonzero formal derivative.
	\begin{defn}
		A field extension $K/k$ is:
		\begin{itemize}
			\item \textbf{algebraic} if every element of $K$ is,
			\item \textbf{finitely generated} if there exists $\alpha_1,...,\alpha_n \in K$ such that $K = k(\alpha_1,...,\alpha_n)$,
			\item \textbf{finite} if the dimension of $K$ as a $k$-vector space is finite,
			\item \textbf{separable} if every element of $K$ is, otherwise the extension is \textbf{inseparable},
			\item \textbf{purely inseparable} if every element of $K$ is,
			\item \textbf{separably generated} if $K/k$ is finitely generated, and there exists a transcendence basis $\lbrace \alpha_1,...,\alpha_m\rbrace \subseteq K$ such that $K/k(\alpha_1,...,\alpha_m)$ is a separable. Such a set of elements $\lbrace \alpha_1,...,\alpha_m\rbrace$ is a \textbf{separating transcendence basis}.
		\end{itemize}
	\end{defn}
	\begin{proposition}\label{prop:extension_relations}
		Let $K/k$ be a field extension, then:
		\begin{itemize}
			\item if $K/k$ is finite then it is algebraic,
			\item if $K/k$ is finitely generated and algebraic, then it is finite,
		\end{itemize}
	\end{proposition}
	\begin{proof} The respective arguments are:
		\begin{itemize}
			\item if $K/k$ is a finite field extension and say the dimension of $K$ as a $k$-vector space is $d$, then for any $\alpha \in K$, the set $\lbrace 1, \alpha, \hdots, \alpha^d\rbrace$ is linearly dependent, and so $\alpha$ is algebraic.
			\item if $\alpha \in K$ is such that $K = k(\alpha)$ and moreover, $\alpha$ is algebraic over $k$, then the extension $K/k$ is finite, this is because there exists a polynomial $p(x) \in k[x]$ such that $p(\alpha) = 0$, which implies $\alpha^r$ for some $r$ can be written as a linear combination of $1,...,\alpha^{r-1}$. Continuing inductively, the result follows.
		\end{itemize}
	\end{proof}
	\subsection{Separable extensions}
	%
	We want to introduce the terminology of a root's \emph{multiplicity} but we need to show this is well defined:
	%
	\begin{lemma}
		Let $F_1/k$ and $F_2/k$ be two field extensions and $a \in k$ a root of a polynomial $g \in k[x]$. Write $g(x) = (x - a)^{r_1}f_1(x)$ and $g(x) = (x-a)^{r_2}f_2(x)$ where $f_i \in F_i[x]$ and $f_i(a) \neq 0$. Then $r_1 = r_2$. This integer is the \textbf{multiplicity} of the root $a$.
	\end{lemma}
	\begin{proof}
		Assume without loss of generality that $r_1 \geq r_2$. Let $\bar{k}$ be an algebraically closed field and consider $F_1$ and $F_2$ as subfields of $\bar{k}$. Then inside $\bar{k}[x]$ we have $f_2(x) = (x-a)^{r_1 - r_2}f_1(x)$, but $f_2(a) \neq 0$ and so $r_1 = r_2$.
	\end{proof}
	%
	\begin{lemma}
		\label{lem:sep_ext_trans_sort_of}
		If $K/F$ is a separable extension and $L$ is any field such that $F \subseteq L \subseteq K$ then $K/L$ is a separable extension.
	\end{lemma}
	\begin{proof}
		Let $a \in K$ be separable over $F$. The minimal polynomial $f \in F[x]$ of $a$ admits $a$ as a \emph{simple root} (a root of multiplicity 1). The image of $f$ in $L[x]$ must also have $a$ as a simple root otherwise $f$ in $K[x]$ would have a multiple root, which by Lemma \ref{lem:sep_elt_alt_def} would contradict $a$ being separable (over $F$). Thus by Lemma \ref{lem:seemingly_useless} we have the result.
	\end{proof}
	From here on, assume $k$ has characteristic $p \neq 0$.
	\begin{lemma}
		\label{lem:insep_purely_sep}
		If an element $\alpha \in F$ is separable over $k$ and is purely inseparable, then $\alpha \in k$.
	\end{lemma}
	\begin{proof}
		Let $e$ be the least integer such that $\alpha^{p^e} \in k$. Suppose for a contradiction that $e \neq 0$, then $\alpha^{p^e}$ does not have a root in $k$ and so the polynomial $p(x) := x^{p^e} - \alpha^{p^e}$ is irreducible (Lemma \ref{lem:irred_example}). This is also monic, and thus is the irreducible polynomial of $\alpha$. By separability, $p' \neq 0$, but this is a contradiction, so $e = 0$.
	\end{proof}
	\begin{defn}
		Given a polynomial $f \in k[x]$, the greatest integer $e$ such that $f \in k[x^{p^e}]$ is the \textbf{reduced degree} of $f$.
	\end{defn}
	Recall the notation $k^p$ for the subfield of $k$ given by $p^{\operatorname{th}}$ powers of elements of $k$.
	\begin{lemma}
		\label{lem:sep_ext_condition}
		Say $F/k$ is a separable extension, then $k = k[F^p]$. Conversely, if $k = k[F^p]$ and $F/k$ is finite, then $F/k$ is separable.
	\end{lemma}
	\begin{proof}
		By Lemma \ref{lem:ext_infinite} we have that $k[F^p]$ is a field. We have $k \subseteq k[F^p] \subseteq F$ so since $F/k$ is separable, by Lemma \ref{lem:sep_ext_trans_sort_of} we have $F/k[F^p]$ is separable. Moreover, since $F \subseteq k[F^p]$ we have that every element of $F$ is purely inseparable over $k[F^p]$. By Lemma \ref{lem:insep_purely_sep} we have $F = k[F^p]$.
		
		For the converse, we first prove the following claim: let $\alpha_1,...,\alpha_n$ be a linearly independent set of $F$ as a $k$-vector space, then $\alpha_1^p,...,\alpha_n^p$ is also linearly independent. We know the Frobenius endomorphism is an isomorphism onto its image, so $\alpha_1^p,...,\alpha_n^p$ form a linearly independent set in $F^p$ as a $k$-vector space. By assumption though, $k[F^p] = F$ and so this set is linearly independent in $F$.
		
		Let $a \in F$ be an element not in $k$ and let $f \in k[x]$ be the minimal polynomial of $a$, say $\operatorname{deg}f = n$. Say $a$ is inseparable and let $e < n$ be the reduced degree of $f$. To avoid contradicting minimality of $n$ we must have $1, a, a^2,..., a^e$ is linearly independent, but $1,a^{p^e},a^{2p^e},...,a^{ep^e}$ we claim is linearly independent. Since $f \in k[x^{p^e}]$ we can write $f(x) = f_1(x^{p^e})$ where $f_1 \in k[x^{p^e}]$. We have $0 = f(a) = f_1(a^{p^e})$.
	\end{proof}
	\begin{cor}
		\label{cor:base_induct}
		If $x$ is separable over $k$ then $k(x) = k(x^p)$. Conversely if $k(x) = k(x^p)$ then $x$ is separable over $k$.
	\end{cor}
	\begin{lemma}
		\label{lem:generated}
		If $\alpha_1,...,\alpha_n \in F$ are separable over $k$ then $F/k(\alpha_1,...,\alpha_n)$ is separable.
	\end{lemma}
	\begin{proof}
		By induction, apply Corollary \ref{cor:base_induct}.
	\end{proof}
	%
	\begin{lemma}
		\label{lem:separable_ext_transitive} If $k \subseteq L \subseteq K$ are fields with $L/K$ separable and $K/L$ separable then $K/k$ is separable.
	\end{lemma}
	\begin{proof}
		See \cite[II \S 5, 9]{Zariski}.
	\end{proof}
	\subsection{Theorem of a Primitive element}
	In this Section, $k$ is an arbitrary field of infinite cardinality (not necessarily algebraically closed).
	\begin{lemma}
		\label{lem:irred_in_ext}
		If $p \in k[x]$ is irreducible, then $p \in (k[x])(\lbrace x_i\rbrace_{i \in I})$ is irreducible for any collection of indeterminants $\lbrace x_i \rbrace_{i \in I}$.
	\end{lemma}
	\begin{proof}
		Write
		\begin{equation}\label{eq:factor}
			p(x) = p_1(x,x_{i_1},...,x_{i_{n_1}})p_2(x,x_{j_1},...,x_{j_{{n_2}}}) \in (k[x])[\lbrace x_i\rbrace_{i \in I}]    
		\end{equation}
		for some elements $i_{1},...,i_{n_1},j_1,...,j_{n_2} \in I$. Then \eqref{eq:factor} still holds if we set $x_{i_k} = x_{j_l} = 0$ for all $k= i_1,...,i_{n_1}, l = j_1,...,j_{n_2}$. We obtain $p(x) = p_1(x,0,...,0)p_2(x,0,...,0)$ which we consider as an equation in the ring $k[x]$, by irreducibility of $p$ we have $\operatorname{deg}p(x) = \operatorname{deg}p_1(x)$, say. Thus $\operatorname{deg}p_1 \geq \operatorname{deg}p$ and so $p_2$ has degree $0$ in $x$. Hence, $p_1$ considered as an element of $(k[x])(\lbrace x_i\rbrace_{i \in I})$ is a unit.
	\end{proof}
	\begin{notation}
		Given a polynomial $f \in k[x_1,...,x_n]$ we denote $(\partial/\partial x_i)f$ by $f_{x}$.
	\end{notation}
	\begin{thm}[Theorem of a primitive element]
		\label{thm:primitive_element}
		Let $F/k$ be a finite, separable extension. Then there exists $\alpha \in F$ such that $F = k(\alpha)$.
		
		If $\alpha_1,...,\alpha_n \in F$ are such that $F = k(\alpha_1,...,\alpha_n)$ (which exists as $F/k$ is finite, hence finitely generated) then $\alpha$ can be taken to be a linear combination of $\alpha_1,...,\alpha_n$ which coefficients in $k$.
	\end{thm}
	\begin{proof}
		Since $F/k$ is finite, there exists $\alpha_1,...,\alpha_n \in F$ such that $F = k(\alpha_1,...,\alpha_n)$. We let $k^\ast := k(x_1,...,x_n)$ and $F^\ast := F(x_1,...,x_n)$. Notice that $F^\ast = k^\ast(\alpha_1,...,\alpha_n)$ and since $\alpha_i$ is separable over $k$ we have that $\alpha_i$, when considered in $F^\ast$, is separable over $k^\ast$ for all $i$, by Lemma \ref{lem:irred_in_ext}. It then follows from Lemma \ref{lem:generated} that $F^\ast$ is a finite, separable extension of $k^\ast$. Consider the element $\beta(x_1,...,x_n) := \alpha_1x_1 + ... + \alpha_nx_n$ of $F^\ast$ and let $f$ be the minimal polynomial of $\beta(x_1,...,x_n)$ in $k^\ast[x]$. By clearing denominators, there exists $h \in k[x_1,...,x_n], g\in k[x,x_1,...,x_n]$ such that 
		\begin{equation}
			\label{eq:clear_denoms}
			h(x_1,...,x_n)f(x,x_1,...,x_n) = g(x,x_1,...,x_n) \in k[x,x_1,...,x_n]
		\end{equation}
		subject to 
		\begin{equation}
			\label{eq:g}
			g(\beta(x_1,...,x_n),x_1,...,x_n) = 0
		\end{equation}
		By \eqref{eq:clear_denoms} we have 
		\begin{equation}
			g_x(x,x_1,...,x_n) = h(x_1,...,x_n)f_x(x,x_1,...,x_n)    
		\end{equation}
		and since $\beta(x_1,...,x_n)$ is separable over $k^\ast$ we have
		\begin{equation}
			g_x(\beta(x_1,...,x_n),x_1,...,x_n) = h(x_1,...,x_n)f_x(\beta(x_1,...,x_n),x_1,...,x_n) \neq 0
		\end{equation}
		Since $k$ is infinite we can find elements $c_1,...,c_n \in k$ such that $g_x(\beta(c_1,...,c_n),c_1,...,c_n) \neq 0$.
		
		On the other hand, by \eqref{eq:g} and the chain rule we have
		\begin{equation}
			\label{eq:partials}
			g_{x_i} = \alpha_i g_x(\beta(x_1,...,x_n),x_1,...,x_n) + g_{x_i}(\beta(x_1,...,x_n),x_1,...,x_n) = 0
		\end{equation}
		So, setting $\alpha = \alpha_1 c_1 + ... + \alpha_n c_n$ we have:
		\begin{equation}
			\label{eq:result}
			0 = \alpha_ig_x(\alpha,c_1,...,c_n) + g_{x_i}(\alpha,c_1,...,c_n)
		\end{equation}
		which implies $\alpha_i \in k(\alpha)$, thus $k(\alpha) = F$.
	\end{proof}
	\begin{remark}
		In the proof of Theorem \ref{thm:primitive_element} we only used the fact that $F/k$ is finitely generated and separable, however, if $F/k$ is separable then it is in particular algebraic. Hence also being finitely generated, we have by Proposition \ref{prop:extension_relations} that $F/k$ is finite. So this hypothesis is equivalent to what was taken here.
	\end{remark}
	%
	\subsection{Separating transcendence bases}
	\begin{remark}
		Considering the definition of \emph{separably generated} one might think that a field extension $K/k$ is finitely generated if there exists $\alpha_1,...,\alpha_n \in K$ such that $K/k(\alpha_1,...,\alpha_n)$ is a finite extension. Notice however, that $K/k(\alpha_1,...,\alpha_n)$ being finite implies it is algebraic, and so every element $x \in K$ is a root of a monic polynomial $p(x)$ with coefficients in $k(\alpha_1,...,\alpha_n)$, that is, there exists $\beta_1,...,\beta_n \in k(\alpha_1,...,\alpha_n)$ such that
		\[x^n + \beta_1 x^{n-1} + \hdots + \beta_{n-1}x + \beta_n = 0\]
		so since $K$ is a field, we have
		\[x = \beta_1 + (x^{-1})\beta_2 + \hdots + (x^{-1})^{n-2}\beta_{n-1} + (x^{-1})^{n-1}\]
		which is to say $K = k(\alpha_1,...,\alpha_n)$, so these definitions are equivalent.
	\end{remark}
	\begin{thm}
		\label{thm:sep_transc_subset}
		Let $K/k$ be an extension which is finitely generated and separably generated. Then any transcendence basis is a separating transcendence basis.
	\end{thm}
	\begin{proof}
		We proceed by induction on $\operatorname{tr.deg}_kK := r$. Say $r = 1$ and let $\lbrace \alpha \rbrace$ be a separating transcendence basis, in other words, $\alpha \in K$ is transcendental over $k$ and $K/k(\alpha)$ is separable. Let $\beta \in K$ be any element transcendental over $k$. We need to show that $K/k(\beta)$ is separable. First, extend $\beta$ to a generating set $\lbrace \beta, \beta_1,...,\beta_n\rbrace$ (which is necessarily finite by hypothesis of $K/k$), and notice that each $\beta_i$ and $\beta$ are separable over $k(\alpha)$ (as $K = k(\beta,\beta_1,...,\beta_n)$ and $K/k(\alpha)$ is separable). We have $k(\alpha) \subseteq k(\alpha,\beta) \subseteq K$ and thus (Lemma \ref{lem:sep_ext_trans_sort_of}) $K/k(\alpha,\beta)$ is separable. By Lemma \ref{lem:separable_ext_transitive} it thus remains to show that $k(\alpha,\beta)/k(\beta)$ is separable, that is, $\alpha$ is separable over $k(\beta)$.
		
		Since $\operatorname{tr.deg}_kK = 1$ and both $\alpha, \beta$ are transcendental, there exists a polynomial $f(x,y) \in k[x,y]$ such that $f(\alpha,\beta) = 0$. Moreover, as $k[x,y]$ is a UFD we may assume that $f$ is irreducible. Assume for a contradiction that $\alpha$ is inseparable over $k(\beta)$. Then by Lemma \ref{lem:seemingly_useless} we have $f'(x,\beta) = 0$ and thus $f(x,\beta) \in k(\beta)[x^p]$, write $f(x,\beta) = g(x^p,\beta)$. We know that $\beta$ is separable over $k(\alpha)$ (as $K/k(\alpha)$ is separable) and so for any irreducible polynomial $j(y) \in k(\alpha)[y]$ we have $j'(\beta) \neq 0$. The polynomial $g(\alpha^p,y)$ is irreducible, to see this, notice $\alpha$ and hence $\alpha^p$ is transcendental over $k$, so $g(\alpha^p, y)$ reducible implies $g(x^p,y)$ and hence $f(x,y)$ reducible. Thus, $\frac{\partial}{\partial\beta}g(\beta,\alpha^p) \neq 0$. This implies by Lemma \ref{lem:seemingly_useless} that $\beta$ is separable over $k(\alpha^p)$.
		
		On the other hand, $\alpha$ is transcendental and so $\alpha \not\in k(\alpha^p)$ which means $x^p - \alpha^p \in k(\alpha^p)[x]$ is the minimal polynomial of $\alpha$ over $k(\alpha^p)$ which shows $\alpha$ is inseparable over $k(\alpha^p)$. Thus $K/k(\alpha^p)$ cannot possibly be separable.  and noting that $K = k(\beta,\beta_1,...,\beta_n)$, we have that $\beta$ is inseparable over $k(\alpha^p)$ (Lemma \ref{lem:generated}). Thus we have a contradiction.
		
		For the inductive step, let $\lbrace \alpha_1,...,\alpha_r\rbrace$ be a separating transcendence basis for $K/k$ and let $\lbrace \beta_1,...,\beta_r\rbrace$ be a transcendence basis. We extend $\lbrace \beta_1,...,\beta_r\rbrace$ to a set of generators $\lbrace \beta_1,...,\beta_r,\gamma_1,...,\gamma_l\rbrace$ of $K$. Now, $\lbrace \alpha_2,...,\alpha_r\rbrace$ form a separating transcendence basis for $k(\beta_1)(\beta_2,...,\beta_r,\gamma_1,...,\gamma_l)$ and so by the inductive hypothesis there is a subset of $\lbrace \beta_1,...,\beta_r\rbrace$ consisting of $r-1$ elements which is a separating transcendence basis, say this set is $\lbrace \beta_1,...,\beta_{r-1}\rbrace$. Extend $\lbrace \alpha_1,...,\alpha_r\rbrace$ to a generating set $\lbrace \alpha_1,...,\alpha_r,\delta_{r+1},...,\delta_n\rbrace$, then $K = k(\alpha_1,...,\alpha_{r-1})(\alpha_r,\delta_{r+1},...,\delta_{n})$ and via this decomposition we have that $K$ is a finitely generated and separably generated by the single variable $\alpha_r$. This must be separating by the inductive hypothesis again and so the result follows.
	\end{proof}
	\begin{lemma}
		\label{lem:not_sep_ext} Let $K$ be a finitely generated $k$-field with $\operatorname{tr.deg}_kK = r$, and $\lbrace \alpha_1,...,\alpha_n\rbrace$ a set of generators. If $K/k$ is not separably generated then there exists $i_1,...,i_{r+1}$ such that $k(\alpha_{i_1},...,\alpha_{r+1})/k$ is not separably generated.
	\end{lemma}
	\begin{proof}
		We proceed by induction on $n$, if $n = r+1$ then there is nothing to show. Assume $n > r+1$ and assume the result holds for the $n-1$ case. Assume wlog that $\alpha_1$ is algebraically dependent on $\alpha_2,...,\alpha_n$. If $k(\alpha_2,...,\alpha_n)/k$ is not separably generated then the result holds by the inductive step. Assume $k(\alpha_2,...,\alpha_n)/k$ is separably generated. Then by Theorem \ref{thm:sep_transc_subset} there exists $i_2,...,i_{r+1}$ such that $\alpha_2,...,\alpha_{i_{r+1}}$ is a separating transcendence basis of $k(\alpha_2,...,\alpha_n)$. Since $\alpha_1$ is algebraically dependent on $\alpha_2,...,\alpha_n$ we have that $k(\alpha_2,...,\alpha_n) = k(\alpha_1,...,\alpha_n) = K$, and thus $K/k(\alpha_1,\alpha_{i_2},...,\alpha_{i_{r+1}})$ is separable, so since $K/k$ is not, it follows from Lemma \ref{lem:sep_ext_trans_sort_of} that $k(\alpha_1,\alpha_{i_2},...,\alpha_{i_{r+1}})/k$ is not.
	\end{proof}
	\begin{thm}
		\label{thm:perf_fin_sep} Let $k$ be perfect. Every algebraic extension of $k$ is separable.
	\end{thm}
	\begin{proof}
		Let $\alpha \in K$ and let $f(x) \in k[x]$ be the minimal polynomial of $\alpha$. Write $f(x) = g(x^{p^{d}})$ for some $d \geq 0$ with $g$ separable. Then $g(x^{p^{d}}) = h(x)^{p^{d}}$ for some $h$ (as $k$ is perfect). Since $f$ is irreducible, we have that $d = 0$ so that $f(x) = g(x)$ and thus $f$ is separable.
	\end{proof}
	\begin{cor}
		If $k$ is a perfect field then any finitely generated extension of $k$ is separably generated.
	\end{cor}
	%
	\section{Integral extensions and jacobson rings}
	\label{sec:integral_extensions_jacobson_rings}
	\subsection{Cayley-Hamilton Theorem, finite modules, and integrality}
	\begin{defn}
		A morphism $f:A \to B$ is a \textbf{finitely generated $A$-algebra} or \textbf{is of finite type} if $B$ is an $A$-algebra and there exists a surjective algebra homomorphism $A[x_1,...,x_n] \to B$. In such a setting we often denote the subring $f(A)$ by $A$, even though $f$ need not be injective.
	\end{defn}
	\begin{defn}
		Let $B$ be an $A$ algebra. An element $b\in B$ of $B$ is \textbf{integral over $A$} if there exists a monic polyonomial $f(x) \in A[x]$ such that $f(b) = 0$. The ring $B$ is \textbf{integral over $A$} if every element $b \in B$ is integral over $A$. A homomorphism of finite type $f: A \to B$ is \textbf{integral} if $B$ is an integral extension of $A$.
	\end{defn}
	\begin{thm}[Cayley-Hamilton Theorem]\label{thm:cayley_hamilton}
		Let $M$ be a finitely generated $A$-module (note: module, not algebra) and $\varphi: M \to M$ an endomorphism. Then $\varphi$ satisfies its own characteristic equation.
	\end{thm}
	Throught, we denote the $n\times n$ identity matrix by $\bold{1}_n$. Recall that for an $n \times n$ matrix $X$, the \textbf{adjugate} of $X$, $\operatorname{Adj}X$ is given by the trasnpose of the matrix of cofactors. The adjugate matrix has the important property that $\operatorname{Adj}X\cdot X = \operatorname{det}X\cdot \bold{1}_n$.
	\begin{proof}[Proof of Theorem \ref{thm:cayley_hamilton}]
		Let $\lbrace m_1,...,m_n\rbrace$ be a set of generators of $M$ and let $\underline{m}$ denote the column vector $(m_1,...,m_n)^T$. For each $i$, write $\varphi(m_i) = \sum_{j = 1}^n a_{ij}m_j$ and let $A$ denote the matrix with $ij^\text{th}$ entry $a_{ij}$. Notice that
		\begin{equation}
			\varphi \bold{1}_n\cdot \underline{m} = A\cdot \underline{m}
		\end{equation}
		and so
		\begin{equation}\label{eq:cayley-hamilton-mainequation}
			(\varphi \bold{1}_n - A) \underline{m} = 0
		\end{equation}
		Left multiplying both sides of \eqref{eq:cayley-hamilton-mainequation} by the adjugate of $\varphi\bold{1} - A$ gives:
		\begin{align*}
			0 &= \operatorname{Adj}(\varphi \bold{1}_n - A)\cdot \big(\varphi \bold{1}_n - A)\underline{m}\big)\\
			&= \big(\operatorname{Adj}(\varphi \bold{1}_n - A)\cdot \big(\varphi \bold{1}_n - A)\big)\underline{m}\\
			&= \operatorname{det}(\varphi \bold{1}_n - A)\bold{1}_n \cdot \underline{m}
		\end{align*}
		and since $\underline{m}$ is a set of generators, this implies that $\operatorname{det}(\varphi \bold{1}_n - A) = 0$.
	\end{proof}
	\begin{remark}
		\label{remark:rest_of_CHT}
		An important further observation is that $\operatorname{det}(\varphi\bold{1}_n - A)$ is a monic polynomial $x^n + c_1x^{n-1} + ... + c_{n-1}x + c_n$, and if there is an ideal $I$ such that $\varphi(M) \subseteq IM$ we have that $c_i \in I^i$.
	\end{remark}
	\begin{cor}
		\label{cor:cayley_helpful_cor} If $M$ is a finitely generated $R$-module and there is an ideal $I \subseteq R$ such that $IM = M$ then there exists $c_1,...,c_n \in I$ with $c_i \in I^i$ such that $(1 + c_1 + ... + c_n)M = 0$. In particular, there exists $c \in I$ such that $(1 + c)M = 0$.
	\end{cor}
	\begin{proof}
		Apply Theorem \ref{thm:cayley_hamilton} to the identity function and take note of Remark \ref{remark:rest_of_CHT}.
	\end{proof}
	So Theorem \ref{thm:cayley_hamilton} gives a powerful way of creating integral elements.
	\begin{defn}
		If $f: A \to B$ is a finitely generated $A$-module, then $f$ is \textbf{finite}. Also, for an element $b \in B$ we denote by $f(A)[b]$ the $A$-algebra $\lbrace f(b) \in B \mid f(x) \in A[x]\rbrace$ (in other words, $f(A)[b]$ is the $A$-subalgebra of $B$ generated by $b$).
	\end{defn}
	\begin{lemma}\label{lem:int_f(A)fin}
		An element $b \in B$ is integral if and only if $f(A)[b]$ is finite.
	\end{lemma}
	\begin{proof}
		If $b$ is integral then there exists monic $g(x) = \alpha_0 + \alpha_1x + \hdots + \alpha_{n-1}x^{n-1} + x^n \in A[x]$ such that $g(b) = 0$. Thus $b^n \in A + Ab + \hdots + Ab^{n-1}$. That $\lbrace 1, b, \hdots, b^{n-1}\rbrace$ generate $f(A)[b]$ follows from an obvious inductive argument.
		
		If $f(A)[b]$ is a finitely generated $A$-module, then multiplication by $b$ gives an endomorphism. The result then follows by Cayley-Hamilton.
	\end{proof}
	\begin{lemma}
		\label{lem:int_elts_subalg}
		The integral elements of $B$ over $A$ form a subalgebra.
	\end{lemma}
	\begin{proof}
		$A\cdot 1$ is integral, thus it suffices to show the integral elements are closed under multiplication and subtraction. Let $b_1,b_2$ be integral. Let $i_1: A \to A[b_1]$ and $i_2: A[b_1] \to (A[b_1])[b_2]$ be inclusion maps. By \ref{lem:int_f(A)fin} we have that $i_1(A)[b_1]$ and $i_2(A[b_1])[b_2]$ are finitely generated modules, and by the previous exercise, this implies $A[b_1,b_2]$ is a finitely generated $A$-module. Multiplication by $b_1 - b_2$ and multiplication by $b_1b_2$ give endomorphisms so the result follows from Cayley-Hamilton.
	\end{proof}
	In light of Lemma \ref{lem:int_elts_subalg} we define the \textbf{integral closure} of $A$ in $B$, denoted $\bar{A}$, to be the subalgebra of $B$ given by the integral elements.\\\\
	%
	The following establishes a strong relationship between integrality and finitality:
	\begin{lemma}\label{lem:finite_integral}
		A morphism $f: A \to B$ is finite if and only if $B = f(A)[b_1,...,b_n]$ with $b_i$ integral. In other words, $f: A \to B$ is finite if and only if $B$ is a finitely generated and integral over $A$.
	\end{lemma}
	\begin{proof}[Proof of $(\Rightarrow)$ direction]
		If $f$ is finite, then $f(A)[b]$ for all $b \in B$ is a finitely generated $A$-module, and thus $b$ is integral by Lemma \ref{lem:int_f(A)fin}.
		
		The converse is proved by induction on $n$ and using the fact that the composition of finite morphisms is finite.
	\end{proof}
	The following results show that integrality is preserved by quotients, and is a local property:
	\begin{lemma}
		\label{lem:quotient_integrality}
		Let $f: A \lto B$ be a ring homomorphism and let $I \subseteq B$ be an ideal. Then $A/(A \cap I) \lto B/I$ is integral.
	\end{lemma}
	\begin{proof}
		This really just comes down to realising what the induced $A/(A \cap I)$-algebra structure on $B/I$ is: let $\bar{b} \in B/I$ and consider a representative $b \in B$. Then since $f:A \lto B$ is integral there exists a monic polynomial $p(x) \in A[x]$ with coefficients in $A$ such that $p(b) = 0$. This polynomial with coefficients reduced modulo $A/(A \cap I)$ evaluates $\bar{b}$ to 0.
	\end{proof}
	\begin{lemma}
		\label{lem:int_implies_alg} Let $A \lto B$ be integral where $A,B$ are $k$-algebras. Then $\operatorname{Frac}A \lto \operatorname{Frac}B$ is algebraic.
	\end{lemma}
	\begin{proof}
		Let $a/b \in \operatorname{Frac}A$ and $f = x^n + \sum_{j = 0}^{n-1}\alpha_j x^j \in k[x]$ such that $f(a) = 0$. Then
		\[0 = (1/b^n)(a^n/1) + (1/b^n)\sum_{j = 0}^{n-1}\alpha_j(a^j/1) = (a/b)^n + \sum_{j = 0}^{n-1}\alpha_j/b^{n-j} (a/b)^j\]
	\end{proof}
	\begin{remark}
		The above proof uses nothing special about the fact that we localised at $(0)$. In fact if $A \lto B$ is integral and $S$ is any multiplicative subset of $A$ then $A_S \lto B_S$ is also integral.
	\end{remark}
	%
	\subsection{Jacobson rings}
	We need the following fact about jacobson rings:
	\begin{lemma}\label{lem:jacob_def}
		$A$ is jacobson if and only if it satisfies the following property: for any prime $\frak{p} \in A$ and element $a \in A/\frak{p}$, if $(A/\frak{p})_a$ is a field, then $A/\frak{p}$ is a field.
	\end{lemma}
	We prove the following special case of Lemma \ref{lem:jacob_tech_hard} as a warm up:
	\begin{lemma}\label{lem:jacob_tech_easy}
		Let $A$ be a jacobson domain and $B = A[s]$ an $A$-algebra generated by a single element. Then if $B$ is a field, so is $A$.
	\end{lemma}
	\begin{proof}
		In light of Lemma \ref{lem:jacob_def} we see that it is sufficient to find an element $a \in A$ such that $A_a$ is a field. Let $K = \operatorname{Frac}A$ be the field of fractions of $A$, we argue first that $B$ is a finite field extension of $K$.
		
		Write $B \cong A[x]/\frak{q}$ for some ideal $\frak{q}$. There is an obvious map $\varphi: A[x] \to K[x]/\frak{q}K[x]$ with kernel equal to $\frak{q}$. The induced map $\psi: A[x]/\frak{q} \to K[x]/\frak{q}K[x]$ is injective, we show this is an isomorphism.Let $\sum_{i = 0}^n\big[\frac{a_i}{a_i'}x^i\big]$ be an arbitrary element of $K[x]/\frak{q}K[x]$. In general, if $\gamma: Y \to X$ is a ring homomorphism with both $y \in Y$ and $\gamma(y) \in Y$ units, then $\gamma(y)^{-1} = \gamma(y^{-1}).$ Thus 
		\[\sum_{i = 0}^n\big[\frac{a_0}{a_0'}x^i\big] = \psi\Big(\sum_{i = 0}^n[a_i][a_i']^{-1}x^i\Big)\]
		proving surjectivity.
		
		Since $B$ is a finite field extension of $K$, it is algebraic. Thus there exists a (not necessarily monic) polynomial $p(x) = \sum_{i = 0}^n\frac{a_i}{a_i'}x^i$ with coefficients in $K$ such that $p(s) = 0$. By clearing denominators we obtain an expression $\sum_{i = 0}^na_i s^i = 0$, that is, $s$ is integral over $A$. By inverting the leading coefficient $a_n$ and dividing through, we see that $s$ is integral over $A_{a_n}$. Since $A_{a_n}$ is an integral extension of the field $B$, it follows from Corollary \ref{cor:lying_over_cor} that $A_{a_n}$ is a field, which finishes the proof.
	\end{proof}
	With that warm up out of the way, we show the result we really want:
	\begin{lemma}\label{lem:jacob_tech_hard}
		Let $A$ be a jacobson domain and $B = A[s]$ an $A$-algebra generated by a single element. If $B$ is a domain and there exists $b \in B$ such that $B_b$ is a field, then both $A$ and $B$ are fields.
	\end{lemma}
	\begin{proof}
		In light of Lemma \ref{lem:jacob_def} we see that to show that $A$ is a field, it is sufficient to find an element $a \in A$ such that $A_a$ is a field, which similarly to the proof of Lemma \ref{lem:jacob_tech_easy}, we do by finding an element $a \in A$ so that $B_b$ is an integral extension of $A_a$. Once this is done, we will use the same lemma to show that $B$ is a field by proving that $B_b$ and hence $B$ is an integral extension of $A$.
		
		Let $K = \operatorname{Frac}A$ be the field of fractions of $A$, we argue that $B_b$ is a finite field extension of $K$.
		
		Write $B \cong A[x]/\frak{q}$. There is an obvious map $\varphi: A[x] \to K[x]/\frak{q}K[x]$ with kernel equal to $\frak{q}$. The induced map $\big(A[x]/\frak{q}\big)_b \to K[x]/\frak{q}K$ is an isomorphism. Thus $B_b$ is a finite field extension of $K$ and so is algebraic; so there exists a polynomial $p(x) \in A[x]$ with coefficients in $A$ such that $p(s) = 0$. Inverting the leading term $p_{n}$ shows that $s$ is integral over $A_{a_n}$, however, this is not the $A_{a_n}$ that we will end up taking, as we still need $b^{-1}$ to be integral over $A_{a}$.
		
		Since $b \in B \subseteq K[x]/\frak{q}K[x]$, there exists a polynomial $q(x) \in A[x]$ with coefficients in $A$ such that $q(b) = q_mb^m + \hdots + q_0 = 0$. Since $B$ is an domain we can cancel powers of $b$ to assume that the Let $q_{m}$ be the leading term of this polynomial. We can invert $q_{0}b^m$ and divide through show that $b^{-1}$ is integral in $A_{q_0}$. We now have that $s$ and $b^{-1}$ are integral in $A_{q_0p_n}$ and so $B_b$ is an integral extension of the field $A_{q_0p_n}$.
		
		Thus, $B_b$ is an integral extension of the field $A_{q_0p_n}$ and thus $A_{q_0p_n}$ is a field. Since $A$ is jacobson, it follows that $A$ is a field, in fact, $A = A_{q_0p_n}$. This shows that $B_b$ and hence $B$ is an integral extension of $A$, and so $B$ is also a field. 
	\end{proof}
	We use Lemma \ref{lem:jacob_tech_hard} to prove two important properties of jacobson rings.
	\begin{thm}\label{thm:fin_gen_jacob}
		If $A$ is a jacobson ring and $B$ a finitely generated $A$-algebra, then $B$ is jacobson.
	\end{thm}
	\begin{proof}
		Consider the case where $B$ is generated by a single element $s \in B$. Let $\frak{p} \subseteq B$ be a prime and $b \in B/\frak{p}$ be such that $(B/\frak{p})_b$ is a field. In light of Lemma \ref{lem:jacob_tech_hard} it suffices to show that $B/\frak{p}$ is generated by a single element as an algebra over some jacobson ring. $B/\frak{p}$ is generated by $[s]$ over $A/(A \cap \frak{p})$, and indeed this is a jacobson ring as the quotient of any jacobson ring by any ideal is jacobson by the correspondence Theorem. The general case then follows by an obvious induction argument.
	\end{proof}
	\begin{thm}\label{thm:jacob_preimage}
		If $f: A \to B$ is a ring homomorphism with $A$ jacobson and $B$ a finitely generated $A$-algebra, then $A \cap \frak{m}$ is maximal for any maximal ideal $\frak{m} \in B$.
	\end{thm}
	\begin{proof}
		First consider the case where $B$ is generated by a single element $s$. Let $\frak{m} \in B$ be maximal. By the Correspondence Theorem, $A/f^{-1}(\frak{m})$ is jacobson. Moreover, $B/\frak{m}$ is generated by a single element as an algebra over $A/f^{-1}(\frak{m})$. Thus by Lemma \ref{lem:jacob_tech_easy} we have that $A/f^{-1}(\frak{m})$ is a field, that is, $f^{-1}(\frak{m})$ is maximal. For the general case we proceed by induction. Say $B = A[b_1,...,b_n]$. Let $\frak{m}' \subseteq A[b_1,...,b_{n-1}]$ be the preimage of $\frak{m}$ in $A[b_1,...,b_{n-1}]$. Since $A[b_1,...,b_{n}] = \big(A[b_1,...,b_{n-1}]\big)[b_n]$, and $A[b_1,...,b_{n-1}]$ being a finitely generated $A$-algebra is jacobson (Theorem \ref{thm:fin_gen_jacob}), it follows that $\frak{m}'$ is maximal from the base case. The final observation to make is $\frak{m}' = \frak{m}$.
	\end{proof}
	\begin{lemma}\label{lem:jacob_easy}
		For a jacobson ring $A$, the nilradical is equal to the jacobson radical.
	\end{lemma}
	\begin{proof}
		The nilradical is equal to the intersection of all primes, since all primes are the intersection of a family of maximals, the result follows.
	\end{proof}
	\subsection{Going Up and Lying over Theorems}
	Loosely speaking, an integral extension $A \subseteq B$ occurs when every element of $B$ is algebraically related to $0$ usingonly using elements scalars from $A$. If $A,B$ are integral domains then any polynomial relating an element of $B$ to $0$ can be ``divided through" by the powers of $x$ so that the constant term is non-zero. Another way of stating this is that if $A,B$ are integral domains then an integral extension $A \subseteq B$ occurs when every element of $B$ is algebraically related to an element of $A$ only using elements of $A$:
	%
	\begin{lemma}
		\label{lem:integral_ext_fields}
		Let $A \subseteq B$ be an integral extension with $A,B$ integral domains. Then $A$ is a field if and only if $B$ is.
	\end{lemma}
	\begin{proof}
		First assume that $A$ is a field. Let $b \neq 0 \in B$ and consider an expression
		\[b^n + a_1b^{n-1} + \hdots a_{n-1}b + a_0 = 0\]
		where we may assume $a_0 \neq 0$ as $B$ is an integral domain. We then ahve
		\[a_0^{-1}(-b^{n-1} - a_1b^{n-2} - \hdots - a_{n-1})b = 1\]
		and so $b$ is a unit.
		
		Conversely, let $a \neq 0\in A$ and consider $a$ as an element of $B$. Since $B$ is a field we have that $a^{-1}$ exists in $B$ and so there is
		\[(a^{-1})^n + a_{1}(a^{-1})^{n-1} + \hdots + a_{n-1}(a^{-1}) + a_n = 0\]
		which yields:
		\[a^{-1} = a_1a + \hdots + a_{n-1}a^{n-2} + a_na^{n-1}\]
		where the expression on the right is an element of $A$.
	\end{proof}
	A corollary of this is a sufficient condition for maximal ideals to be pulled back to maximal ideals:
	\begin{cor}
		\label{cor:maximality_preserved_integrality}
		Let $A \subseteq B$ an integral extension and $\frak{p} \subseteq B$ an ideal of $B$. Then $\frak{p}$ is maximal if and only if $A \cap \frak{p}$ is.
	\end{cor}
	\begin{proof}
		Integrality is preserved by taking quotients (Lemma \ref{lem:quotient_integrality}) so $A/(A \cap \frak{p}) \lto B/\frak{p}$ is integral. We now have an integral extension of integral domains so we can apply Lemma \ref{lem:integral_ext_fields}.
	\end{proof}
	In turn, an application of this is for integral extensions $A \subseteq B$ the chains of primes of $B$ lying over a prime in $A$ are of length $0$:
	\begin{cor}
		Let $A \subseteq B$ be integral and $\frak{q}\subseteq \frak{q}' \subseteq B$ primes in $B$ such that $\frak{q} \cap A = \frak{q}' \cap A$. Then $\frak{q} = \frak{q}'$.
	\end{cor}
	\begin{proof}
		Denote $\frak{q} \cap A$ by $\frak{p}$. Integrality is preserved by localisation (Lemma \ref{lem:int_implies_alg}) so $A_\frak{p} \lto B_{A\setminus\frak{p}}$ is integral (warning: $\frak{p} \subseteq B$ need not even be an ideal, let alone prime. However, $A\setminus \frak{p}$ is multiplicative, so this localisation still makes sense). Consider $\frak{q}B_{A\setminus\frak{p}}$ and $\frak{q}'B_{A\setminus\frak{p}}$ which both intersect with $A_\frak{p}$ to give $\frak{p}A_\frak{p}$. The result then follows from Corollary \ref{cor:maximality_preserved_integrality} and that primes in $B_{A\setminus\frak{p}}$ are in bijection with primes in $B$ disjoint from ${A\setminus\frak{p}}$ (notice that $A\setminus \frak{p} = A\setminus (\frak{q} \cap A)$ so and ideal $I \subseteq B$ such that $I \cap (A\setminus(\frak{q} \cap A)) = \varnothing$ is just an ideal $I$ such that $I \cap A = \frak{p}$).
	\end{proof}
	We have the lying over Theorem:
	\begin{thm}
		\label{thm:lying_over}
		Let $A \subseteq B$ be integral. Then $\operatorname{Spec}B \lto \operatorname{Spec}A$ is surjective.
	\end{thm}
	\begin{proof}
		Let $\frak{p} \subseteq A$ be a prime. The localisation of integral extensions in integral, and so $A_\frak{p} \lto B_{A\setminus\frak{p}}$ is integral. We have the following commutative diagram:
		\[
		\begin{tikzcd}
			A\arrow[r]\arrow[d] & B\arrow[d,"{\beta}"]\\
			A_\frak{p}\arrow[r] & B_{A\setminus\frak{p}}
		\end{tikzcd}
		\]
		Since $A_\frak{p} \lto B_{A\setminus\frak{p}}$ is integral, any maximal ideal $\frak{m}$ of $B_{A\setminus\frak{p}}$ is such that $\frak{m} \cap A_\frak{p} = \frak{p}A_\frak{p}$. So by commutativity we have $\beta^{-1}(\frak{m})$ is a prime such that $\beta^{-1}(\frak{m}) \cap A = \frak{p}$.
	\end{proof}
	an easy Corollary of which is the going up Theorem:
	\begin{thm}
		Let $A \subseteq B$ be integral and consider say $\frak{p}_1 \subseteq \frak{p}_2 \subseteq A$ are prime ideals, and $\frak{q}_1 \subseteq B$ is prime such that $\frak{q}_1 \cap A = \frak{p}_1$. Then there exists prime $\frak{q}_2 \subseteq B$ containing $\frak{q}_1$ such that $\frak{q}_2 \cap A = \frak{p}_1$.
	\end{thm}
	\begin{proof}
		Apply Theorem \ref{thm:lying_over} to the integral extension $A/\frak{p}_1 \subseteq B/\frak{q}_1$.
	\end{proof}
	\section{Dimension Theory}
	\subsection{Transcendence degree of finitely generated $k$-domains}
	We prove:
	\begin{thm}
		\label{thm:onepointeight}
		Let $A$ be a finitely generated $k$-integral domain, with $k$ a field. Then
		\begin{enumerate}
			\item\label{thm:onepointeighta} $\operatorname{tr.deg}_kA = \operatorname{dim}A$,
			\item\label{thm:onepointeightb} if $\frak{p}$ is any prime of $A$ then $\operatorname{ht.}\frak{p} + \operatorname{dim}A/\frak{p} = \operatorname{dim}A$
		\end{enumerate}
	\end{thm}
	We first establish some lemmas, we denote $k[x_1,...,x_n]$ by $k[\underline{x}]$
	\begin{lemma}
		\label{lem:notationdance}
		Let $\frak{p} \subseteq k[\underline{x}]$ be prime ideal, and consider $S = k[x_1,...,x_m]\setminus\lbrace 0 \rbrace$ as a multiplicative subset of $k[\underline{x}]$. Then writing $k[\underline{x}]/\frak{p} = k[\alpha_1,...,\alpha_n]$ we have
		\[k[\underline{x}]_S/\frak{p}k[\underline{x}]_S \cong k(\alpha_1,...,\alpha_r)[\alpha_{r+1},...,\alpha_n]\]
	\end{lemma}
	\begin{proof}
		Writing $\overline{S}$ for the image of $S$ under $k[\underline{x}] \to k[\underline{x}]/\frak{p}$ we have,
		\[k[\underline{x}]_S/\frak{p}k[\underline{x}]_S \cong (k[\underline{x}]/\frak{p})_{\overline{S}} \cong k(\alpha_1,...,\alpha_r)[\alpha_{r+1},...,\alpha_n]\]
	\end{proof}
	The following Lemma was established in \cite{com_alg_notes} as a required Lemma for proving Hilbert's Nullstellensatz, but we also use it here to prove Theorem \ref{thm:onepointeight}:
	\begin{lemma}
		\label{lem:algebraic}
		Let $\frak{m}$ be a maximal ideal of $F[x_1,...,x_n]$, then $F[x_1,...,x_n]/\frak{m}$ is an algebraic extension of $F$.
	\end{lemma}
	\begin{proof}
		See \cite[\S 2.1]{com_alg_notes}
	\end{proof}
	\begin{proof}[Proof of Theorem \ref{thm:onepointeighta}]
		First we show $\operatorname{tr.deg}_kA \geq \operatorname{dim}A$, we claim it suffices to show for any pair of prime ideals $\frak{q} \subsetneqq \frak{r}$ of $k[\underline{x}]$ that
		\begin{equation}
			\label{eq:clever}
			\operatorname{tr.deg}_kk[\underline{x}]/\frak{r} < \operatorname{tr.deg}_kk[\underline{x}]/\frak{q}
		\end{equation}
		Write $A \cong k[\underline{x}]/\frak{p}$, any chain of primes in $A$ corresponds to a chain $\frak{x_0} \subsetneqq \hdots \subsetneqq \frak{x_m}$ of primes in $k[\underline{x}]$ containing $\frak{p}$. So given Equation \ref{eq:clever} holds, we find
		\[n - 1 < \operatorname{tr.deg}_kA\]
		establishing the claim.
		
		There is a surjective map $k[\underline{x}]/\frak{q} \to k[\underline{x}]/\frak{r}$ so $\operatorname{tr.deg}_kk[\underline{x}]/\frak{r} \leq \operatorname{tr.deg}_kk[\underline{x}]/\frak{q}$ is clear. Say equality held. Let $\beta_1,...,\beta_n$ denote the image of $x_1,...,x_n$ under $k[\underline{x}] \to \operatorname{Frac}k[\underline{x}]/\frak{r}$ and by rearranging the order of $\underline{x}$ if necessary, assume that $\beta_1,...,\beta_r$ be algebraically independent where $r := \operatorname{tr.deg}_kk[\underline{x}]/\frak{r}$. We denote by $\alpha_1,...,\alpha_n$ elements of $k[\underline{x}]/\frak{q}$ such that under $k[\underline{x}]/\frak{q} \to k[\underline{x}]/\frak{p}$ $\alpha_i$ maps to $\beta_i$. Notice that $\alpha_1,...,\alpha_r$ are algebraically independent.
		
		Consider $S := k[x_1,...,x_r]\setminus\lbrace 0 \rbrace$ as a subset of $k[\underline{x}]$. $\alpha_1,...,\alpha_r$ are algebraically independent, so $k[x_1,...,x_r] \to k[\underline{x}]/\frak{q}$ is injective and so $\frak{q} \cap S = \varnothing$. Similarly, $\frak{r} \cap S = \varnothing$. Writing $k[\underline{x}] = R$, it follows from Lemma \ref{lem:notationdance} that
		\[R_S/\frak{q}R_S \cong k(\alpha_1,...,\alpha_r)[\alpha_{r+1},...,\alpha_n]\]
		where we think of the right hand side as a subring of $k(\alpha_1,...,\alpha_n)$. By Lemma \ref{lem:extension} this is a field, and so $\frak{q}R_S$ is maximal. That is $\frak{q}R_S = \frak{r}R_S$ and so $\frak{q} = \frak{r}$, a contradiction.
		
		Now we show $\operatorname{tr.deg}_kA \leq \operatorname{dim}A$, we proceed by induction on $r:= \operatorname{tr.deg}_kA$. Write $A \cong k[\underline{x}]/\frak{p}$, if $r = 0$ then $A$ is a field and so $\operatorname{dim}A = 0$. Say $r > 0$, write $k[\underline{x}]/\frak{p} = k[\alpha_1,...,\alpha_n]$ and assume that $\alpha_1$ is transcendental. Write $R := k[\underline{x}]$ and consider $S := k[x_1]\setminus\lbrace 0 \rbrace$ as a subset of $R$. Then $R_S \cong k(x_1)[x_2,...,x_n]$ and $R_S/\frak{p}R_S = k(\alpha_1)[\alpha_2,...,\alpha_n]$, by Lemma \ref{lem:notationdance}. Now, $\operatorname{tr.deg}_kR_S/\frak{p}R_S < r$ so by the inductive hypothesis there exists a chain of primes $\frak{x}_0 \subsetneqq ... \subsetneqq \frak{x}_{r-1}$ of $R_S$ all containing $\frak{p}R_S$. We set $\frak{r}_i := \frak{x}_i \cap \frak{p} \subset R$ and notice that in particular $x_1 \not\in \frak{r}_{r-1}$, and hence the residue class $[x_1] \in R/\frak{r}_{r-1}$ is transcendental over $k$, which is to say that $\frak{r}_{r-1}$ is not maximal (Lemma \ref{lem:algebraic}). Thus it is contained in a maximal ideal so we obtain a chain $\frak{r}_0 \subsetneq \hdots \subsetneqq \frak{r}_n$ in $R$ all containing $\frak{p}$. Thus $\operatorname{dim}A\geq \operatorname{tr.deg}_kA$.
	\end{proof}
	We move onto the proof of Theorem \ref{thm:onepointeightb}, we start with the following special case:
	\begin{lemma}
		\label{lemma:onepointeightb}
		Denote $k[\underline{x}]$ by $R$. Let $\frak{p} \subseteq R$ be prime, then $\operatorname{ht.}\frak{p} + \operatorname{dim}R/\frak{p} = n$.
	\end{lemma}
	\begin{proof}
		We proceed by induction on $n$. If $n = 0$ then $\frak{p} = (0)$ and $\operatorname{ht.}(0) = \operatorname{dim}R/(0) = 0$. Say $n > 0$. Let $r := \operatorname{tr.deg}_kR/\frak{p}$ and write $R/\frak{p} = k[\alpha_1,...,\alpha_n]$ where $\alpha_1,...,\alpha_r$ are algebraically independent. Consider $S := k[x_1,...,x_r]\setminus\lbrace 0 \rbrace$ as a subset of $R$. Then $R_S \cong k(x_1,...,x_r)[x_{r+1},...,x_n]$. By the inductive hypothesis we have
		\[\operatorname{ht.}\frak{p}R_S + \operatorname{dim}_kR_S/\frak{p}R_S = n - r\]
		By Lemma \ref{lem:extension} we have $\operatorname{dim}_kR_S/\frak{p}R_S = 0$. Furthermore, $\frak{p} \cap S = \varnothing$ so $\operatorname{ht.}\frak{p}R_S = \operatorname{ht.}\frak{p}$. We thus have $\operatorname{ht.}\frak{p} + r = n$, the result then follows from Theorem \ref{thm:onepointeighta} as $r = \operatorname{tr.deg}_kR/\frak{p} = \operatorname{dim}R/\frak{p}$.
	\end{proof}
	We now generalise this:
	\begin{proof}[Proof of Theorem \ref{thm:onepointeightb}]
		Write $A \cong k[\underline{x}]/\frak{p}$ and let $\frak{q} \subseteq A$ be prime. Then there is prime $\frak{q}'$ in $k[\underline{x}]$ containing $\frak{p}$ such that $A/\frak{q} \cong k[\underline{x}]/\frak{q}'$. From Lemma \ref{lemma:onepointeightb} we thus have
		\[\operatorname{ht.}\frak{q}' + \operatorname{dim}k[\underline{x}]/\frak{q}' = n = \operatorname{ht.}\frak{p} + \operatorname{dim}k[\underline{x}]/\frak{p}\]
		We thus have $\operatorname{ht.}\frak{q}' - \operatorname{ht.}\frak{p} + \operatorname{dim}A/\frak{q}' = \operatorname{dim}A/\frak{p}$. Clearly, $\operatorname{ht.}\frak{q}' - \operatorname{ht.}\frak{p} = \operatorname{ht.}\frak{q}$, and $\operatorname{dim}k[\underline{x}]/\frak{q}' = \operatorname{dim}A/\frak{q}$, thus
		\[\operatorname{ht.}\frak{q} + \operatorname{dim}A/\frak{q} = \operatorname{dim}A\]
		as required.
	\end{proof}
	Next we prove:
	\begin{thm}
		\label{thm:onepointonetwoa}
		A Noetherian integral domain $A$ is a UFD if and only if every prime ideal of height 1 is principal.
	\end{thm}
	\begin{proof}
		Let $\frak{p}$ be of height 1 and $f \in \frak{p}\setminus\lbrace 0\rbrace$. $\frak{p}$ by assumption is principal so write $\frak{p} = (g_1)$. Let $h_1 \in A$ be such that $f = h_1g_1$. Say $h_1$ is not a unit, then similarly there exists a prime ideal $\frak{p}_1$ of height 1 containing $h_1$. Let $h_2 \in A$ be such that $(h_2) = \frak{p}_2$ and $r_2 \in A$ be such that $h_1 = r_2h_2$. Repeating this process we obtain a sequence $g_1,g_2,...$ such that $(g_1) \subsetneqq (g_2) \subsetneqq \hdots$ which by the Noetherian assumption is finite, of length $n$ say. We have $f = r_n g_1\hdots g_n$.
		
		Conversely, let $f \in A$ be a non-unit and not zero. Let $\frak{p}$ be a minimal primes lying over $f$ and write $f =rf_1...f_n$ for irreducibles $f_i$ and unit $r$. Fix some $i \leq n$, we claim $\frak{p} = (f_i)$. It suffices to show $f_i$ is prime, but $A$ is a UFD and so all irreducibles are prime.
	\end{proof}
	
	\subsection{The Poincare Series and the length of a module}
	\subsubsection{The length polynomial}
	Sometimes the notation of a geometric series is used for convenience sake, for instance, we have that in $\widehat{k[x]}$:
	\[(1-x,1-x,1-x,\hdots)(1,1+x,1+x+x^2,\hdots) = (1,1,1,\hdots) - (x,x^2,x^3,\hdots)\]
	and that $(x,x^2,x^3,...)$ is equivalent to zero, this is often written as:
	\[\frac{1}{1-x} = 1 + x + x^2 + \hdots\]
	where both sides of the equality are thought of as elements of $\widehat{k[x]}$. This notation will be used in the statement involving the \emph{Poincare series} (Definition \ref{def:poincare}) of a module with respect to an \emph{additive function}:
	\begin{defn}
		\label{def:length}
		Let $A$ be a ring and $\call{M}_A$ the class of all $A$-modules. A funcion:
		\[\lambda: \call{M}_A \lto \bb{Z}\]
		is \textbf{additive} if for every short exact sequence
		\[0 \lto M' \lto M \lto M'' \lto 0\]
		we have that $\lambda(M') - \lambda(M) + \lambda(M'') = 0$.
	\end{defn}
	Eventually we will specialise to the case where $\lambda$ is the \emph{length} of a module:
	\begin{defn}
		Let $M$ be an $A$-module. The \textbf{length} of $M$ is the supremum of the lengths of all ascending chains of submodules
		\[M_0 \subsetneq \hdots \subsetneq M_n\]
		A chain consisting of $n+1$ modules has length $n$.
	\end{defn}
	\begin{defn}
		\label{def:poincare} Let $A = \bigoplus_{i = 0}^\infty A_i$ be a Noetherian graded ring and $M = \bigoplus_{i = 0}^\infty M_i$ a graded $A$-module. The \textbf{Poincare series} is the element of $\bb{Z}\llbracket t \rrbracket$ given by
		\[P(M,t) = \sum_{i = 0}^\infty \lambda(M_i)t^i\]
	\end{defn}
	What happens in the case where $M$ is finitely generated? Being Noetherian, $A$ is finitely generated as an $A_0$-module (see \cite{com_alg_notes}). In the case where where $M$ is finitely generated as an $A_0$-module, we have $M_n = 0$ for large $n$, and so $P(M,t)$ is just a polynomial in $t$. Now consider the case where $A$ admits elements $a_1,...,a_m$ with respective degrees $k_1,...,k_m$ such that $M$ is a finitely generated $A_0[a_1,...,a_m]$-module. Multiplication by $a_m$ yields an exact sequence for any $n$:
	\begin{equation}
		\label{eq:mult_by_an}
		0 \lto \ker^n a_m \lto M_n \stackrel{a_m}{\lto} M_{n + k_m} \lto \operatorname{Coker}^n a_m \lto 0
	\end{equation}
	where the $n$ in $\ker^n a_m$ is a label signifying this is the kernel which is a submodule of $M_n$, similarly for $\operatorname{Coker}^n a_m$. Since $\lambda$ is additive, by multiplying by $t^{n+k_m}$ we obtain:
	\[\lambda (\ker^n a_m)t^{n+k_m} - \lambda (M_n)t^{n+k_m} + \lambda (M_{n+k_m})t^{n+k_m} - \lambda (\operatorname{Coker}^{n + k_m} a_{m})t^{n + k_m} = 0\]
	summing over all $n$ yields:
	\begin{equation}
		\label{eq:additive_function_reduction}
		(1 - t^{k_m})P(M,t) - \sum_{n = 0}^{k_m}\lambda(M_n)t^n = \sum_{n = 0}^\infty\lambda (\operatorname{Coker}^{n + k_m} a_{m})t^{n + k_m} - t^{k_m}P(\ker a_m, t)
	\end{equation}
	where $\ker a_m = \bigoplus_{n = 0}^\infty \ker^n a_m$. Now, by defining
	\[\operatorname{Coker }a_m := M_0 \oplus \hdots \oplus M_n \oplus \bigoplus_{n = 0}^\infty \operatorname{Coker}^{n+k_m}a_m\]
	we have
	\[
	\label{eq:clever_graded_module} \sum_{n = 0}^\infty\lambda (\operatorname{Coker}^{n + k_m} a_{m})t^{n + k_m} = P(\operatorname{Coker}a_m,t) - \sum_{n = 0}^{k_m}\lambda(M_n)t^n
	\]
	and so \eqref{eq:additive_function_reduction} becomes:
	\begin{equation}
		\label{eq:generation_reduction} (1 - t^{k_m})P(M,t) = P(\operatorname{Coker}a_m,t) - t^{k_m}P(\ker a_m, t)
	\end{equation}
	Noticing now that $\operatorname{Coker}a_m$ and $\ker a_m$ are both finitely generated $A_0[a_1,...,a_m]$-modules and are annihilated by $a_m$ so in fact are finitely generated $A_0[a_1,...,a_{m-1}]$-modules, we have proved:
	\begin{thm}
		\label{thm:Hilbert_Serre}
		Let $A = \bigoplus_{i = 0}^\infty A_i$ be a Noetherian graded ring and $M = \bigoplus_{i = 0}^\infty M_i$ a finitely generated graded $A$-module. Let $a_1,...,a_m$ be generators of $A$ as an $A_0$-module with degrees $k_1,...,k_m$ respectively. The Poincare series can be written as:
		\begin{equation}
			\label{eq:Poincare_quotient}
			P(M,t) = \frac{f(t)}{\prod_{i = 1}^m(1 - t^{k_m})}
		\end{equation}
		where $f(t)$ is a polynomial.
	\end{thm}
	We obtain different representations \eqref{eq:Poincare_quotient} by taking different sets of generators of $A$, however the \emph{pole at $t = 1$} is invariant:
	\begin{defn}
		The \textbf{pole} of $\frac{f(t)}{\prod_{i = 1}^m(1 - t^{k_m})}$ at $t = 1$, denoted $d(M)$, is the pole in the ordinary sense when considered as a meromorphic function $\bb{C} \lto \bb{C}$.
	\end{defn}
	That this pole is an invariant follows from the fact that each representation \eqref{eq:Poincare_quotient} is equal to $P(M,t)$ which does not depend on a choice of generators.
	
	A further special case of Theorem \ref{thm:Hilbert_Serre} is when all the generators $a_1,...,a_m$ have degree 1, in such a situation we can make a statement about the restriction of $\lambda$ to the modules $M_n$ for large $n$:
	\begin{cor}
		\label{cor:polynomial}
		Let $A$ be a Noetherian graded ring and $M$ a finitely generated $A$-module. We know $A$ is finitely generated as an $A_0$-module, assume further that generators of $A$ all of degree $1$ can be chosen. Then the function $n \mapsto \lambda(M_n)$ is given by a polynomial (in $\bb{Q}[t]$) for sufficiently large $n$. The degree of this polynomial is independent of the choice of generators and is equal to $d(M)-1$.
	\end{cor}
	\begin{proof}
		By definition of the Poincare series, the coefficient next to $t^n$ is equal to $\lambda(M_n)$ (for all $n$). First, we calculate the coefficient next to $t^n$ in $\prod_{i = 1}^m(1 - t)^{-m}$. Recall that $(1 - t)^{-1} = 1 + t + t^2 + \hdots$ and so we wish to calculate the coefficient in front of $t^n$ of
		\[(1 + t + t^2 + \hdots)(1 + t + t^2 + \hdots)\hdots (1 + t + t^2 + \hdots)\]
		where there are $m$ factors. This has a combinatorial answer; this coefficient counts the number of multisubsets of the set $\lbrace t_1,...,t_m\rbrace$ of size $n$, where $t_i$ represents $t$ chosen from the $i^{\text{th}}$ factor. This coefficient is thus ${n + m - 1 \choose m - 1}$.
		
		Consider the representation of the Poincare series given by Theorem \ref{thm:Hilbert_Serre}, we have that $f(t)$ is a polynomial so write $f(t) = \sum_{i = 0}^N \alpha_i t^i$, by cancelling factors of $(1 - t)$ we may assume $m = d(M)$ and $f(1) \neq 0$. We then have that the coefficient in front of $t^n$ in $P(M,t)$ is the coefficient in front of $t^n$ of
		\[(\alpha_0 + \alpha_1t + \alpha_2t^2 + \hdots)(1 + t + t^2 + \hdots)(1 + t + t^2 + \hdots)\hdots (1 + t + t^2 + \hdots)\]
		which by the previous calculation is
		\begin{equation}
			\label{eq:combinatorial_poincare}
			\sum_{k = 0}^N\alpha_k{n  + d - k - 1 \choose d - 1}
		\end{equation}
		notice that:
		\begin{equation}
			\label{eq:primary_school}
			{n  + d - k - 1 \choose d - 1} = \frac{(n + d + k - 1)!}{(d-1)!(n+k)!} = \frac{(n+d+k-1)\hdots(n+d-k-(d-1))}{(d-1)!}
		\end{equation}
		which is a polynomial in $n$, and hence so is \eqref{eq:combinatorial_poincare}. Equation \eqref{eq:combinatorial_poincare} holds true for all $n$, and is always a polynomial, but for $n < N$ this polynomial changes as $n$ increases. On the other hand, for all $n \geq N$ this polynomial remains exactly the same. Thus for all $n \geq N$ we have that $\lambda(M_n)$ is equal as a function to a fixed polynomial. Lastly, notice that the numberator of \eqref{eq:primary_school} has $d - 1$ factors, and so the leading term of \eqref{eq:combinatorial_poincare} is \[\frac{\big(\sum_{k = 0}^N\alpha_k\big)n^{d-1}}{(d-1)!} = \frac{f(1)n^{d-1}}{(d-1)!}\] which is non-zero.
	\end{proof}
	From now on, $\lambda: \call{M}_A \lto \bb{Z}$ is taken to be the \emph{length} function (Definition \ref{def:length}).
	
	Assume $M$ is an $A$-module (with no assumptions on either $M$ nor $A$) and there is a filtration
	\[\hdots \subseteq M_1 \subseteq M_0 = M\] of $M$. If $n \geq 0$ is such that $M/M_n$ admits a decomposition series (see the section on Artin Rings/modules of \cite{com_alg_notes}) then since any chain of submodules can be extended to a decomposition series we have:
	\begin{equation}
		\label{eq:sum_lengths}
		\lambda(M/M_n) = \sum_{i = 0}^n\lambda (M_i/M_{i+1})
	\end{equation}
	\begin{lemma}
		\label{lem:polynomial_sum}
		Let $f: \bb{Z} \lto \bb{R}$ be a polynomial function of degree $d$. Then the function
		\begin{align*}
			h_f: \bb{Z} &\lto \bb{R}\\
			n &\mapsto \sum_{i = 0}^nf(i)
		\end{align*}
		is a polynomial of degree $d+1$.
	\end{lemma}
	\begin{proof}
		Write $f(n) = \sum_{j = 0}^d \alpha_j n^j$ so that
		\begin{align*}
			h_f(n) &= \sum_{i = 0}^n \sum_{j = 0}^d \alpha_j i^j\\
			&= \sum_{j = 0}^d \alpha_j\sum_{i = 0}^n i^j
		\end{align*}
		so it remains to show for all $j \geq 0$ that $\sum_{i = 0}^n i^j$ is a polynomial in $n$ and that $\sum_{i = 0}^ni^d$ has degree $d+1$. This can be done in many different ways, one of which is by using \emph{Bernoulli numbers} and \emph{Faulhaber's formula}, we omit the details.
	\end{proof}
	We have:
	\begin{proposition}
		\label{prop:Noeth_primary_fin_gen_filtration}
		Let $A$ be a Noetherian local ring, $\frak{m}$ its maximal ideal, $\frak{q}$ an $\frak{m}$-primary ideal, $M$ a finitely-generated $A$-module, $(M_n)$ a stable $\frak{q}$-filtration of $M$. Then,
		\begin{enumerate}
			\item\label{prop:finite_length} $M/M_n$ is of finite length for each $n \geq 0$,
			\item\label{prop:polynomial} for sufficiently large $n$, this length is a polynomial $g(n)$ of degree less than or equal to $s$ where $s$ is the least number of generators of $\frak{q}$,
			\item\label{prop:leading_coefficient} the degree and leading coefficient of $g(n)$ is independent of the choice of stable $\frak{q}$-filtration.
		\end{enumerate}
	\end{proposition}
	\begin{proof}
		\ref{prop:finite_length}: If $A$ is a Noetherian local ring with maximal ideal $\frak{m}$, let $\frak{q} \subseteq A$ be a $\frak{m}$-primary ideal, and assume $M$ is finitely generated. The only prime ideal containing $\frak{q}$ is $\frak{m}$ and so $A/\frak{q}$ is a Noetherian ring of dimension $0$, and thus is Artinian. So, each $M_i/M_{i+1}$ is a finitely generated module over an Artinian ring thus has finite length. It follows from \eqref{eq:sum_lengths} that $\lambda(M/M_n)$ is finite.
		
		\ref{prop:polynomial}: If $a_1,...,a_s$ is a minimal set of generators of $\frak{q}$ then the images $\bar{a}_1,...,\bar{a}_m$ in $\frak{q}/\frak{q}^2$ generate $G(A) := \bigoplus_{i = 0}^\infty \frak{q}^i/\frak{q}^{i+1}$. All of these have degree $1$ and so by Corollary \ref{cor:polynomial} there exists $N>0$ such that the function $n \mapsto \lambda(M_{N+n}/M_{N+n+1})$ is given by a polynomial $p \in \bb{Q}[n]$ such that $\operatorname{deg}p \leq d(G(M))$. By the shape of \eqref{eq:Poincare_quotient} we have that $d(M) \leq s$. Equality holds when $f(t)$ does not admit $1$ as a root.
		
		\ref{prop:leading_coefficient}: Say $(M_n)$ and $(M_n')$ are two stable $\frak{q}$ filtrations of $M$. Let $N > 0$ be such that for all $n > N$ we have $\frak{q}M_n = M_{n+1}$, make a similar definition for $N'$. We have
		\[M_{n + N} = \frak{q}^n M_N \subseteq \frak{q}^n M = \frak{q}^nM_0' \subseteq M_n'\]
		and
		\[M_{n + N}' = \frak{q}^n M_{N'}' \subseteq \frak{q}^n M = \frak{q}^nM_0 \subseteq M_n\]
		and so if $g(n)$ is the polynomial corresponding to $(M_n)$ and $g'(n)$ is the polynomial corresponding to $(M_n')$ then
		\[g(n + N') \leq g'(n)\]
		and
		\[g(n) \leq g'(n + N)\]
		since these are both polynomials we get $\lim_{n \lto \infty}g(n)/g'(n) \lto 1$ and so these have the same degree and leading coefficient.
	\end{proof}
	In the context of Proposition \ref{prop:Noeth_primary_fin_gen_filtration} where the stable $\frak{q}$-filtration given by $(\frak{q}^nM)$ is taken, we denote the polynomial $g(n)$ by $\chi_{\frak{q}}^M(n)$. In the case where $M = A$ we denote this polynomial by $\chi_\frak{q}(n)$. In fact, in this case, the degree of this polynomial is invariant under choice of $\frak{m}$-primary ideal $\frak{q}$, astonishingly, we will see later that this invariant degree is equal to the dimension of $A$.
	\begin{lemma}
		\label{lem:invariant_under_primary}
		The degree of $\chi_\frak{q}(n)$ is invariant under choice of $\frak{m}$-primary ideal $\frak{q}$.
	\end{lemma}
	\begin{proof}
		Since $A$ is Noetherian and $\frak{q}$ is $\frak{m}$-primary, there exists $r > 0$ such that $\frak{m}^r \subseteq \frak{q} \subseteq \frak{m}$, so for all $n$ we have $\frak{m}^{nr} \subseteq \frak{q}^n \subseteq \frak{m}^n$ and so for all $n$:
		\[\lambda(A/\frak{m}^{rn}) \leq \lambda(A/\frak{q}^n) \leq \lambda(A/\frak{m}^n)\]
		and so
		\[1 = \frac{\chi_{\frak{m}}(rn)}{\chi_{\frak{m}}(rn)} \leq \frac{\chi_\frak{q}(n)}{\chi_\frak{m}(rn)} \leq \frac{\chi_{\frak{m}}(n)}{\chi_\frak{m}(rn)}\stackrel{n \to \infty}{\lto} < \infty\]
		the result follows.
	\end{proof}
	\begin{defn}
		\label{def:polynomial_degree} In light of Lemma \ref{lem:invariant_under_primary}, we denote the degree of $\chi_\frak{q}(n)$ by $d(A)$.
	\end{defn}
	\begin{remark}
		Lemma \ref{lem:invariant_under_primary} shows that the degree of $\chi_\frak{q}(n)$ is independent of the choice of $\frak{q}$, and Proposition \ref{prop:Noeth_primary_fin_gen_filtration} shows that this degree is equal to the size of the least number of generators of $\frak{q}$, a corollary of this is that the size of the least number of generators of all $\frak{m}$-primary ideals are equal.
	\end{remark}
	
	\subsection{The Dimension Theorem}
	Given a Noetherian, local ring $A$ with maximal ideal $\frak{m}$ we denote the least number of elements required to generate $\frak{m}$ by $\delta(A)$. The amazing fact that we prove in this Section is that this integer and $d(A)$ (Definition \ref{def:polynomial_degree}) are both equal to $\operatorname{dim}A$. We do this by proving the following sequence of inequalities:
	\[\delta(A) \geq d(A) \geq \operatorname{dim}A \geq \delta(A)\]
	The first inequality is already proved by part \ref{prop:polynomial} of Proposition \ref{prop:Noeth_primary_fin_gen_filtration}. To prove the second inequality, we need the following general Lemmas:
	\begin{lemma}
		\label{lem:degree_modulo}
		Let $A$ be Noetherian, local, and $M$ a finitely generated $A$-module. Given any non-zero-divisor $x \in A$ of $M$ we have
		\begin{equation}
			\label{eq:dimension_inductive_reduction} d(M/xM) \leq d(M) - 1
		\end{equation}
	\end{lemma}
	\begin{proof}
		Since $x$ is a non-zero-divisor, the map $M \longmapsto xM$ is injective and thus an isomorphism. It can be shown using the Nine Lemma that in general, if
		\[0 \lto N' \stackrel{\alpha}{\lto} N \stackrel{\beta}{\lto} N'' \lto 0\]
		is a short exact sequence of modules and $J \subseteq N$ is a submodule, then the sequence
		\[0 \lto N'/\alpha^{-1}J \lto N/J \lto N''/\beta(J)N''\]
		is also a short exact sequence. Applying this to the submodule $\frak{m}^nM \subseteq M$ we have for all $n \geq 0$ a short exact sequence:
		\[0 \lto xM/(xM \cap \frak{m}^nM) \lto M/\frak{m}^nM \lto M'/\frak{m}^nM' \lto 0\]
		where $M' := M/xM$. If we let $g(n)$ denote the polynomial $xM/(xM \cap \frak{m}^nM)$ (taking $n$ sufficiently large) we have:
		\[g(n) - \chi_\frak{m}^M(n) + \chi_\frak{m}^{M'}(n) = 0\]
		Now, by the Artin-Rees Lemma (see \cite{completion}) we have that $xM \cap \frak{m}^nM$ is a stable $\frak{m}$-filtration of $xM$, and so by part \ref{prop:leading_coefficient} of Proposition \ref{prop:Noeth_primary_fin_gen_filtration} the leading term of $g(n)$ and $\chi_\frak{m}^M(n)$ cancel out. The result follows.
	\end{proof}
	Applying Lemma \ref{lem:degree_modulo} to the special case where $M = A$ we get:
	\begin{cor}
		\label{cor:degree_ring_inductive_reduction}
		If $x$ is a non-zero-divisor of a Noetherian, local ring $A$, then
		\[d(A/(x)) \leq d(A) - 1\]
	\end{cor}
	We can now prove:
	\begin{lemma}
		\[d(A) \geq \operatorname{dim}A\]
	\end{lemma}
	\begin{proof}
		We proceed by induction on $d(A)$. If $d(A) = 0$ then for sufficiently large $n$ we have $\lambda(A/\frak{m}^n) = \lambda(A/\frak{m}^{n+1})$ which means $\frak{m}^n = \frak{m}^{n+1}$ which by Nakayama's Lemma implies $\frak{m}^n = 0$. Thus, if $\frak{p}$ is a prime ideal of $A$ we have $\frak{m}^n = 0 \subseteq \frak{p}$ which implies $\frak{m} \subseteq \frak{p}$, in other words, $\operatorname{dim}A = 0$.
		
		Now say that $d(M) > 0$. Consider a chain of ascending prime ideals:
		\[\frak{p}_0 \subsetneq \hdots \subsetneq \frak{p}_r\]
		in $A$. Let $x \in \frak{p}_1\setminus\frak{p}_0$, denote $A/\frak{p}_0$ by $A'$ and consider the image $x'$ of $x$ in $A'$. Then $A'$ is an integral domain and $x' \neq 0$, so by Corollary \ref{cor:degree_ring_inductive_reduction} we have $d(A'/(x')) \leq d(A') - 1$. Our next claim is that $d(A') \leq d(A)$. There are many ways of showing this so we leave it as an exercise.
		
		Thus $d(A'/(x')) \leq d(A) - 1$ and so the inductive hypothesis applies. However, the image of $\frak{p}_1 \subsetneq \hdots \frak{p}_r$ is an ascending chain in $A'/(x')$ and so $r - 1 \leq d(A) - 1$ which implies $r \leq d(A)$, proving the result.
	\end{proof}
	The remaining inequality, that $\operatorname{dim}A \leq \delta(A)$ follows from a \emph{Krull's Principal Ideal Theorem}:
	\begin{thm}[Krull's Principal Ideal Theorem]
		Let $A$ be a Noetherian ring, $a_1,...,a_r \in A$ elements of $A$ and $\frak{p}$ a  prime, minimal among those containing $(a_1,...,a_r)$, then $\operatorname{ht}.\frak{p} \leq r$.
	\end{thm}
	\begin{proof}
		We proceed by induction on $r$. Say $\frak{p}$ is minimal over $(a)$ and let $\frak{q} \subseteq \frak{p}$ be a prime not equal to $\frak{p}$, we show $\operatorname{ht}.\frak{q} = 0$. Let $l: A \lto A_\frak{q}$ denote the localisation map and let $\scr{X}^n:= l^{-1}\big((\frak{q}A_\frak{q})^nA_\frak{q}\big)$. We claim $\scr{X}^n = (a)\scr{X}^n + \scr{X}^{n+1}$.
		
		Since $A$ is Noetherian the chain
		\[(a) \subseteq (a) + \scr{X} \subseteq (a) + \scr{X}^2 \subseteq \hdots\]
		eventually stablises, say $(a) + \scr{X}^n = (a) + \scr{X}^{n+1}$. In particular this means $\scr{X}^n \subseteq (a) + \scr{X}^{n+1}$ and so for any $f \in \scr{X}^n$ we have $f = ba + g$ for some $b \in A$ and $g \in \scr{X}^{n+1}$. Thus $f - ba \in \scr{X}^{n+1} \subseteq \scr{X}^n$, so since $f \in \scr{X}^n$ it follows that $ba \in \scr{X}^n$. Now, $\frak{p}$ is minimal over $(a)$ and $\frak{q} \subseteq \frak{p}$ so $a \not\in \frak{q}$, this means $b \in \scr{X}^n$ by definition of $\scr{X}$, establishing the claim.
		
		By Nakayama's Lemma, we thus have $\scr{X}^n = \scr{X}^{n+1}$. Localising at $\frak{q}$ we have $\scr{X}^nA_\frak{q} = \scr{X}^{n+1}A_\frak{q}$, ie, $(\scr{X}A_\frak{q})^n = (\scr{X}A_\frak{q})^{n+1}$. Applying Nakayama's Lemma again we have $(\scr{X} A_\frak{q})^n = 0$. Thus if $\frak{r} \subseteq A_\frak{q}$ was any prime then $\frak{r} \supseteq (0) = (\scr{X}A_\frak{q})^n$ and thus $\frak{r} \supseteq \scr{X}A_\frak{q} = \frak{q}A_\frak{q}$ so by maximality $\frak{r} = \frak{q}A_\frak{q}$. Thus $\operatorname{dim}A_\frak{q} = 0$.
		
		We now prove the inductive step. Let $\frak{p} \subseteq A$ be minimal over $x_1,...,x_n$ and by replacing $A$ by $A_\frak{p}$ if necessary, assume that $A$ is local and $\frak{p}$ maximal. Let $\frak{q} \subseteq \frak{p}$ be a prime with no other primes strictly sitting between. We will show that $\operatorname{ht}.\frak{q} \leq n-1$ by finding elements $y_1,...,y_{n-1}$ such that $\frak{q}$ is minimal over $(y_1,...,y_{n-1})$.
		
		Since $\frak{p}$ is minimal over $(x_1,...,x_n)$ and $\frak{q} \subsetneqq \frak{p}$ we have $\lbrace x_1,...,x_n \rbrace \not\subseteq \frak{q}$, say $x_1 \not\in \frak{q}$. $\frak{p}$ is minimal over $(\frak{q},x_1)$ and so $\sqrt{(\frak{q},x_1)} = \frak{p}$, thus for $i = 2,...,n$ there exists $r_i > 0$, $a_i \in A$, and $y_i \in \frak{q}$ such that $x_i^{r_i} = a_ix_1 + y_i$. We claim $\frak{q}$ is minimal over $y_2,...,y_{n}$.
		
		Denote the image of $\frak{p}$ in the quotient ring $A/(y_2,...,y_n)$ by $\bar{\frak{p}}$, similarly for $\frak{q}$. Then $\bar{\frak{p}}$ is minimal over $\bar{x_1}$ and so $\bar{\frak{q}}$ is minimal over $0$. That is, $\frak{q}$ is minimal over $(y_2,...,y_n)$.
	\end{proof}
	The inductive step also proves a converse:
	\begin{cor}
		If a prime ideal $\frak{p}$ has height $n$, then there exists $a_1,...,a_n \in \frak{p}$ such that $\frak{p}$ is minimal amongst all prime ideals containing $(a_1,...,a_n)$.
	\end{cor}
	Application: If $A$ is a Noetherian, local ring with maximal ideal $\frak{m}$ and $a_1,...,a_n$ are elements of $A$ whose images under $A \lto \frak{m}/\frak{m}^2$ form a basis for this vector space, then if we denote by $N$ the submodule of $A$ generated by $a_1,...,a_n$ we have
	\[N + \frak{m}^2 = \frak{m}\]
	so by Nakayama's Lemma, $N = \frak{m}$. This shows:
	\begin{cor}
		Let $A$ be Noetherian, local with maximal ideal $\frak{m}$. Then
		\[\operatorname{dim}_k\frak{m}/\frak{m}^2 \geq \operatorname{dim}A\]
	\end{cor}
	\begin{remark}
		In the above discussion we have used that the vector space dimension agrees with Krull dimension, and the Dimension Theorem.
	\end{remark}
	\begin{cor}
		Let $A$ be Noetherian, local with maximal ideal $\frak{m}$, and $\hat{A}$ the $\frak{m}$-adic completion. Then
		\[\operatorname{dim}A = \operatorname{dim}\hat{A}\]
	\end{cor}
	\begin{proof}
		$A/\frak{m}^n \cong \hat{A}/\hat{\frak{m}}^n$, so $d(A) = d(\hat{A})$.
	\end{proof}
	\section{Discrete valuation rings}
	What \emph{is} the integral closure? An answer can be provided once a theory of \emph{valuation rings} has been developed:
	\begin{thm}
		\label{thm:discrete_valuation_intersection}
		Let $B$ be a subring of a field $K$. Then the integral closure $\bar{B}$ of $B$ in $K$ is the intersection of all subrings of $\operatorname{Frac}B$ containing $B$ which are \emph{discrete valuation rings}.
	\end{thm}
	\begin{defn}
		A \textbf{valuation ring} $B$ is an integral domain satisfying: for all $x\neq 0 \in \operatorname{Frac}B$ either $x \in B$ or $x^{-1} \in B$ (or both).
		
		A valuation ring is \textbf{discrete} if the quotient group $(\operatorname{Frac}B)^{\times}/B^\times$ is isomorphic to $\bb{Z}$, here, the superscript $\times$ denotes the group of units (the intuition behind this Definition comes from Lemma \ref{})
	\end{defn}
	It is clear that discrete valuation rings exist, any field provides an example, but there are more interesting examples involving homomorphisms into algebraically closed fields. First we provide some properties:
	\begin{lemma}
		If $B$ is a valuation ring, then
		\begin{enumerate}
			\item\label{lem:discrete_valuation_local} $B$ is a local ring,
			\item\label{lem:discrete_valuation_between} if $B'$ is a ring such that $B \subseteq B' \subseteq \operatorname{Frac}B$ then $B'$ is also a discrete valuation ring,
			\item\label{lem:discrete_valuation_integrally_closed} $B$ is integrally closed.
		\end{enumerate}
	\end{lemma}
	\begin{proof}
		\ref{lem:discrete_valuation_local}: Let $\frak{m}$ be the set of all non-units of $B$, we show that this is an ideal. Let $b,x  \in B$. If $bx$ is a unit then $\exists r \in B, rbx = 1$ which implies $x$ is a unit. Thus if $x \in \frak{m}$ then $bx \in \frak{m}$. If $x,y \in \frak{m}$ then since $B$ is a discrete valuation ring, either $x^{-1}y \in B$ or $xy^{-1} \in B$. In the first case we have $x + y = (1 + x^{-1}y)x \in B\frak{m} \subseteq \frak{m}$.\\\\
		%
		\ref{lem:discrete_valuation_between}: Let $x \in \operatorname{Frac}B$ and say $x \not\in B'$. Then $x \not\in B$ and so $x^{-1} \in B$ which implies $x^{-1} \in B'$.\\\\
		%
		\ref{lem:discrete_valuation_integrally_closed}: Let $\alpha \in \operatorname{Frac}B$ be integral over $B$, write
		\[\alpha^n + b_1\alpha^{n-1} + \hdots + b_{n-1}\alpha + b_n = 0\]
		We have that $\alpha \in B$ or $\alpha^{-1} \in B$, in the first case we are done, in the second we have
		\[\alpha = b_n \alpha^{1-n}  - b_{n-1}\alpha^{2-n} - \hdots - b_2\alpha^{-1} - b_1\]
		where the expression on the right is an element of $B$. Thus in either case we have $\alpha \in B$.
	\end{proof}
	Our next goal is to prove:
	\begin{proposition}
		\label{prop:maximal_elt_DVR}
		Let $K$ be a field and $\Omega$ an algebraically closed field. Define $\Sigma_K^\Omega$ to be the set of pairs $(C,f)$ where $C \subseteq K$ is a subring and $f: C \lto \Omega$ a homomorphism;
		\[\Sigma_K^\Omega := \lbrace (C,f)\mid C \subseteq K, f: C \lto \Omega\text{ a homomorphism}\rbrace\]
		We endow $\Sigma_K^\Omega$ with the following partial order: $(C,f) \prec (C',f')$ if $C \subseteq C'$ and $f'\restriction_{C} = f$, then $\Sigma_K^\Omega$ satisfies the ascending chain condition and so by zorn's Lemma admits at least one maximal element $(B,g)$. This ring $B$ is a discrete valuation ring.
	\end{proposition}
	\begin{remark}
		Note: we do not exclude the possibility that $\Omega$ is taken to be the trivial field $(0)$. In this case, the unique maximal element given $(B,g)$ is $(K,0)$, where $0$ denotes the zero map. In this situation, it is clear that $K$ is a valuation ring. In what follows, we consider the case where $K \neq B$.
	\end{remark}
	First we prove a simpler result:
	\begin{lemma}
		\label{lem:local_kernel} $B$ is a local ring, and if $K \neq B$ then the unique maximal ideal is $\frak{m} := \ker g$.
	\end{lemma}
	\begin{proof}
		Notice first that $\frak{m}$ is at least prime. As $B$ is a subring of a field it is an integral domain, thus there is an injection $B \lto B_\frak{m}$. We have that for all $x \not\in \frak{m}$ that $g(x) \neq 0$ (by definition of $\frak{m}$) so by the universal property of localisation we obtain a homomorphism $B_\frak{m} \lto \Omega$ which extends $B$. By maximality of $B$ we obtain $B = B_\frak{m}$ which implies the statement.
	\end{proof}
	\begin{remark}
		Some authors (for example, Hartshorne) do not consider the set $\Sigma_K^\Omega$ but instead consider the set
		\begin{equation}
			\Gamma := \lbrace R \subseteq K \mid R\text{ is local}\rbrace
		\end{equation}
		and endow this set with a partial order given by \textbf{domination}, $R \prec S$ if $S \subseteq R$ and $\frak{m}_R \cap S = \frak{m}_S$, with $\frak{m}_T$ denoting the unique maximal ideal of $T$.
		
		This is equivalent to our presentation as since $(B,g)$ is local, it suffices to consider only local rings in $\Sigma_K^\Omega$, and all such local rings have unique maximal element given by the inverse of $\lbrace 0 \rbrace$ which renders the condition on the preorder given to $\Sigma_K^\Omega$ equivalent to the domination condition.
	\end{remark}
	
	
	
	Before proving Proposition \ref{prop:maximal_elt_DVR} we need the following narky lemma:
	\begin{lemma}
		\label{lem:narky} Let $x \neq 0 \in K$, then either $\frak{m}B[x] \neq B[x]$ or $\frak{m}B[x^{-1}] \neq B[x^{-1}]$.
	\end{lemma}
	\begin{proof}
		Say both $\frak{m}B[x] = B[x]$ and $\frak{m}B[x^{-1}] = B[x^{-1}]$. Then we have equations:
		\begin{equation}
			\label{eq:one_x}
			1 = m_nx^{n} + \hdots m_1x + m_0
		\end{equation}
		\begin{equation}
			\label{eq:one_x_inv}
			1 = m'_kx^{-k} + \hdots m'_1x^{-1} + m'_0
		\end{equation}
		with $m_j,m_j' \in \frak{m}$. We assume that these expressions are such that $n$ is minimal. Say $k < n$ and multiply \eqref{eq:one_x_inv} by $x^k$ we get:
		\begin{equation}
			\label{eq:one_x_inv_modified}
			(1 - m_0')x^k = m'_k + m'_{k-1}x^{1} + \hdots + m_1'x^{k-1}
		\end{equation}
		Since $m_0' \in \frak{m}$ we have $1 - m_0'$ is a unit and so we can divide through and multiply by $x^{n - k}$ to write \eqref{eq:one_x} with a smaller power of $n$, contradicting minimality.
	\end{proof}
	\begin{proof}[Proof of Proposition \ref{prop:maximal_elt_DVR}]
		Let $x \neq 0 \in \operatorname{Frac}B$ and assume $\frak{m}B[x] \neq B[x]$ (if in fact $\frak{m}B[x] = B[x]$ then replace $x$ by $x^{-1}$ in the following argument). Let $\frak{n}$ be a maximal ideal containing $\frak{m}B[x]$ in $B[x]$. Then $\frak{n} \cap B$ contains $\frak{m}$ and $\frak{m}$ is maximal, thus $\frak{n} \cap B = \frak{m}$, we thus have a homomorphism $B/\frak{m} \lto B[x]/\frak{n}$. Also, the homomorphism $g: B \lto \Omega$ induces $B/\frak{m} \lto \Omega$. We thus have the following commutative diagram of solid arrows
		\begin{equation}
			\label{eq:extension}
			\begin{tikzcd}
				B\arrow[r]\arrow[d] & B/\frak{m}\arrow[r]\arrow[d] & \Omega\\
				B[x]\arrow[r] & B[x]/\frak{n}\arrow[ur,dashed]
			\end{tikzcd}
		\end{equation}
		We have that $B[x]/\frak{n} \cong B/\frak{m}[\bar{x}]$ where $\bar{x}$ is the image of $x$ under $B/\frak{m} \lto B[x]/\frak{n}$ and so $B/\frak{m} \lto B[x]/\frak{n}$ is a finite and thus algebraic field extension. Thus we have the dashed arrow in \eqref{eq:extension} (here we crucially use that $\Omega$ is algebraically closed). By maximality, it then follows that $B = B[x]$, that is, $x \in B$.
	\end{proof}
	Theorem \ref{thm:discrete_valuation_intersection} now follows as a Corollary:
	\begin{proof}[Proof of Theorem \ref{thm:discrete_valuation_intersection}]
		Let $D$ denote the intersection of all discrete valuation rings of $K$. Since all discrete valuation rings are integrally closed it follows that $\bar{B} \subseteq D$.
		
		Conversely, say $x \in K$ and is not integral over $B$. Then $x$ is not contained in the ring $B[x^{-1}]$. Thus $x^{-1}$ is a non-unit inside $B[x^{-1}]$ and so is contained inside a maximal ideal $\frak{m}$. Let $\Omega$ be an algebraic closure of the field $B[x^{-1}]/\frak{m}$, we then have a homomorphism \[B \lto B[x^{-1}] \lto B[x^{-1}]/\frak{m} \lto \Omega\]
		and so by Proposition \ref{prop:maximal_elt_DVR} extends to a discrete valuation ring not containing $x$.
	\end{proof}
	We give an alternative Definition of a valuation ring, which explains the name:
	\begin{defn}
		Let $K$ be a field. and $G$ a totally ordered abelian group. A \textbf{valuation} is a function $v: K\setminus\lbrace 0 \rbrace \lto G$ satisfying:
		\begin{enumerate}
			\item $v(xy) = v(x) + v(y)$,
			\item $v(x + y) \geq \operatorname{min}\lbrace v(x), v(y)\rbrace$
		\end{enumerate}
		Given a valuation $v$, the set $R_{K,v} := \lbrace x \in K \mid v(x) \geq 0\rbrace$ is a local ring with maximal ideal $\lbrace x \in K \mid v(x) > 0 \rbrace$. We call this local ring the \textbf{valuation ring of $v$}. 
		
		A \textbf{valuation ring} is an integral domain which is the valuation ring of $v$ for some valuation $v: K \lto G$.
	\end{defn}
	We prove equivalence of these Definitions:
	\begin{lemma}
		A ring $R$ is a valuation ring if and only if it is a valuation ring.
	\end{lemma}
	\begin{proof}
		Say $R$ is such that for all $x \neq 0 \in \operatorname{Frac}R$ we have $x \in R$ or $x^{-1} \in R$. Let $K$ denote $\operatorname{Frac}R$ and consider the group $\Gamma := K^{\times}/R^\times$ (where $\times$ denotes the group of units) and endow it with the order $a \geq b$ if $a - b \in \operatorname{im}(R\setminus\lbrace 0 \rbrace \lto \Gamma)$.We take $v: K \lto \Gamma$ to be the natural projection. 
		
		Notice that $v(xy) = v(x) + v(y)$ is clearly satisfied, we are simply writing multiplication in the group $\Gamma$ additively.
		
		Next we show $v(x + y) \geq \operatorname{min}(v(x),v(y))$. We notice that
		\begin{equation}
			v(a + b) - v(b) = [a + b]_\Gamma - [b]_\Gamma = [ab^{-1} + 1]_\Gamma = [ab^{-1}]_\Gamma = v(a) - v(b)
		\end{equation}
		\textcolor{red}{Something wrong here, come back to it.}
	\end{proof}
	%
	%
	%
	%
	%
	%
	%
	\section{Graded rings, modules, and algebras}\label{Sec:Graded}
	\subsection{General Theory}
	\begin{defn}
		Let $G$ be a totally ordered group. A \textbf{$G$-graded ring} is a ring $A$ along with a \textbf{$G$-grading}, ie, a group isomorphism
		\begin{equation}
			A \cong \bigoplus_{g \in G}A_g
		\end{equation}
		for some collection of subgroups $\lbrace A_g \subseteq A \rbrace_{g \in G}$. Furthermore, $A$ is required to be such that $A_gA_h \subseteq A_{g + h}$ for all $g,h \in G$.
		
		An element $a \in A$ such that $a \in A_g$ is \textbf{homogeneous of degree $g$}. An ideal which can be generated by homogeneous elements is a \textbf{homogeneous ideal}.
		
		Let $A$ be a $G$-graded ring, a \textbf{$G$-graded $A$-module} $M$ is an $A$-module along with a \textbf{$G$-grading}, ie a group isomorphism
		\begin{equation}
			M \cong \bigoplus_{g \in G} M_g
		\end{equation}
		for some collection of subgroups $\lbrace M_g \subseteq M\rbrace_{g \in G}$. Furthermore, $M$ is required to be such that $A_gM_h \subseteq M_{g + h}$ for all $g,h \in G$.
	\end{defn}
	\begin{fact}
		An ideal $I$ is homogeneous if and only if
		\begin{equation}
			I = \bigoplus_{g \in G}(A_g \cap I)
		\end{equation}
	\end{fact}
	\begin{example}
		The canonical example is a polynomial ring $k[x_1,...,x_n]$ which is $\bb{Z}$-graded. The subgroup of degree $m$ elements is generated by all degree $m$ monomials.
		
		This ring also admits a $\bb{Z}^n$-grading, where the subgroup of degree $(m_1,...,m_n)$ elements is generated by the polynomial $x^{m_1}_1...x^{m_n}_n$.
	\end{example}
	\begin{example}\label{ex:quotient_of_homog_by_homog}
		If $A \cong \bigoplus_{g \in G}A_g$ is a graded algebra and $I \subseteq A$ is a homogeneous ideal, then $A/I$ is graded as per:
		\begin{equation}
			A/I \cong \bigoplus_{g \in G}A_g/\bigoplus_{g \in G}(A_g \cap I) \cong \bigoplus_{g \in G}A_g/A_g \cap I
		\end{equation}
	\end{example}
	
	The most important case will be when $G = \bb{Z}$, we now focus on this case (although there is no particular reason have to, other than commutativity sakes).
	\begin{defn}
		Let $A$ be a $\bb{Z}$-graded ring and $M,N$ two $\bb{Z}$-graded $A$-modules. A \textbf{morphism of $\bb{Z}$-graded $A$-modules of degree $i (\in \bb{Z})$} is an $A$-module homomorphism $\varphi: A \lto B$ subject to
		\begin{equation}
			\forall j \in \bb{Z}, f(A_j) \subseteq B_{j + i}
		\end{equation}
		we denote the $A$-module of such morphisms by $\operatorname{Hom}(A,B)$.
		
		This gives rise to a $\bb{Z}$-graded module
		\begin{equation}
			\operatorname{Hom}(A,B) := \bigoplus_{i \in \bb{Z}}\operatorname{Hom}(A,B)_i
		\end{equation}
		Moreover, the tensor product is naturally a $\bb{Z}$-graded module with grading:
		\begin{equation}
			A \otimes B \cong \bigoplus_{\substack{i \in \bb{Z}\\ n + m = i}}A_n \otimes B_m
		\end{equation}
	\end{defn}
	What if $A,B$ are $\bb{Z}$-graded \emph{algebras}? All the definitions go through as expected except for the tensor product which has multiplication defined by
	\begin{equation}\label{eq:tensor}
		(a_1 \otimes b_1)(a_2 \otimes b_2) = (-1)^{\operatorname{deg}a_2\operatorname{deg}b_1}(a_1a_2 \otimes b_1b_2)
	\end{equation}
	This multiplication law is necessary for the differential cases in order to make $\operatorname{Hom}(A,B) \otimes A \lto B$ given on pure tensors by $f \otimes a \longmapsto f(a)$ a \emph{morphism of chain complexes (see \cite{intro_hom_alg} for introductory material on chain complexes)}, a statement we now explain.
	\begin{defn}\label{def:diff_graded}
		Let $A$ be a ring, a \textbf{differential, $\bb{Z}$-graded $A$-module} is a $\bb{Z}$-graded $A$-module $M$ along with a \textbf{differential}, ie, a linear map $d: A \lto A$ such that
		\begin{equation}
			\forall m \in \bb{Z}, \forall m \in M, \operatorname{deg}f(m) = \operatorname{deg}m - 1
		\end{equation}
		A \textbf{morphism of differential, $\bb{Z}$-graded $A$-modules} $M,N$ is a morphism of $\bb{Z}$-graded modules $\varphi: M \lto N$ such that for all $i \in \bb{Z}$
		the following diagram commutes:
		\begin{equation}
			\begin{tikzcd}
				M_i\arrow[r, "\varphi"]\arrow[d,"{d_M}"] & N_i\arrow[d,"{d_N}"]\\
				M_{i - 1}\arrow[r,"{\varphi}"] & N_{i - 1}
			\end{tikzcd}
		\end{equation}
	\end{defn}
	We often say ``graded" in place of $\bb{Z}$-graded.
	
	In accordance with Definition \ref{def:diff_graded}, every differential, graded module is naturally a chain complex.
	\begin{defn}
		Let $(A,d_A),(B,d_B)$ be differential, graded $k$-algebras (for some commutative ring $k$), the tensor product is naturally equipped with the following differential:
		\begin{equation}
			d_{A \otimes B}(a \otimes b) = d_A(a) \otimes b + (-1)^{\operatorname{deg}a}a \otimes d_B(b)
		\end{equation}
		Similarly, $\operatorname{Hom}(A,B)$ is naturally equipped with the following differential:
		\begin{equation}
			d_{H}(f)= d_B(f) - (-1)^{\operatorname{deg}f}f(d_A)
		\end{equation}
		
		
		
	\end{defn}
	\begin{remark}\label{rmk:useless}
		Let $\psi: \operatorname{Hom}(A,B) \otimes A \lto B$ be the evaluation map, ie, the map given on pure tensors by $\psi(f\otimes a) = f(a)$. We claim this is a chain map. We require commutativity of the following diagram:
		\begin{equation}
			\begin{tikzcd}
				(\operatorname{Hom}(A,B) \otimes A)_n\arrow[r,"{\psi}"]\arrow[d,"{d_{H\otimes A}}"] & B_n\arrow[d,"{d_B}"]\\
				(\operatorname{Hom}(A,B) \otimes A)_{n-1}\arrow[r,"{\psi}"] & B_{n-1}
			\end{tikzcd}
		\end{equation}
		Unpacking definitions, for all pure tensors $f \otimes a \in (\operatorname{Hom}(A,B) \otimes A)_n$ we have
		\begin{equation}
			d_B(\psi)(f \otimes a) = d_B(f(a))
		\end{equation}
		and
		\begin{align*}
			\psi d_{H\otimes A}(f\otimes a) &= \psi (d_Hf \otimes a + (-1)^{\operatorname{deg}f}f \otimes d_A(a))\\
			&= d_Hf(a) + (-1)^{\operatorname{deg}f}f(d_A(a))\\
			&= d_B(f(a)) - (-1)^{\operatorname{deg}f}f(d_A(a)) + (-1)^{\operatorname{deg}f}f(d_A(a))\\
			&= d_B(f(a))
		\end{align*}
		so indeed we have a morphism of differential, graded algebras.
	\end{remark}
	\begin{remark}
		Notice that Remark \ref{rmk:useless} only explains why we put a minus sign in $d_H$ and absolutely nothing else.
	\end{remark}
	
	Consider the $\bb{Z}$-graded ring $S := k[x_0,...,x_n]$. We can define a ring homomorphism $\varphi: S \lto S$ given by multiplication by $x_0$, strictly speaking though this fails to be a morphism of $\bb{Z}$-graded rings as, for example, the degree $0$ element $1$ is mapped to the degree $1$ element $x_0$.
	
	There is an obvious fix to this, we simply shift the grading of the first copy of $S$, to this end we define:
	\begin{defn}
		Let $A$ be a $G$-graded ring. We denote by $A(g)$ the graded ring which is identical as a ring to $A$, but with the grading shifted by $g$, more concretely, if for an arbitrary $G$-graded ring $B$ we denote by $B_g$ the subgroup generated by the degree $g$ elements, then we have
		\begin{equation}
			A(g)_h = A_{g + h}
		\end{equation}
		In the special case where $G = \bb{Z}$, the differential denoted $d_{A(n)}$ is given by $d_{A(n)}(a) = (-1)^nd_A(a)$.
	\end{defn}
	\begin{example}
		We have a well defined morphism of graded rings
		\begin{equation}
			S(-1) \stackrel{(x_0)}{\lto}S
		\end{equation}
	\end{example}
	We conclude this Section with one last chain complex constructor: let $M$ be an $R$-module and $y \in R$ an arbitrary element of $R$.  Let $\scr{G}$ be a chain complex, we denote by $K(y)$ (see Definition \ref{def:koszul_complex} for a justification of this choice of notation) the following chain complex:
	\begin{equation}
		0 \lto R \stackrel{y}{\lto} R \lto 0
	\end{equation}
	We define:
	\begin{defn}
		The \textbf{mapping cone} of multiplication $\scr{G} \stackrel{y}{\lto} \scr{G}$ is the tensor product:
		\begin{equation}
			K(y) \otimes \scr{G}
		\end{equation}
	\end{defn}
	The usefulness of the mapping cone comes from the following property:
	\begin{proposition}\label{prop:mapping_exact_sequence}
		Let $\scr{G}$ be a chain complex of $R$-modules and let $y\in R$ be an arbitrary element of $R$. Then there exists a long exact sequence of homology groups:
		\begin{equation}\label{eq:mapping_exact_sequence}
			\hdots \lto H_{i-1}(\scr{G}) \stackrel{y}{\lto} H_{i-1}(\scr{G}) \lto H_{i}(K(y) \otimes \scr{G})\lto H_i(\scr{G}) \stackrel{y}{\lto} H_{i}(\scr{G}) \lto \hdots
		\end{equation}
		where the connecting morphisms are multiplication by $y$.
	\end{proposition}
	\begin{proof}
		Construct the following short exact sequence of chain complexes:
		\begin{equation}
			\begin{tikzcd}
				R(-1)\arrow[d] & 0\arrow[r]\arrow[d] & 0\arrow[r]\arrow[d] & R\arrow[r]\arrow[d] & 0\arrow[d]\\
				K(y)\arrow[d] & 0\arrow[r]\arrow[d] & R\arrow[r,"{y}"]\arrow[d] & R\arrow[r]\arrow[d] & 0\arrow[d]\\
				R &0\arrow[r] & R\arrow[r] & 0\arrow[r] & 0
			\end{tikzcd}
		\end{equation}
		We can tensor this entire diagram with $\scr{G}$ to obtain the following short exact sequence:
		\begin{equation}
			\begin{tikzcd}
				\scr{G}(-1)\arrow[d] & \hdots\arrow[r] & G_{i-2}\arrow[r]\arrow[d] & G_{i-1}\arrow[r]\arrow[d] & G_i\arrow[r]\arrow[d] & \hdots\\
				K(y)\otimes \scr{G}\arrow[d] & \hdots\arrow[r] & G_{i-2} \oplus G_{i-1}\arrow[r]\arrow[d] & G_{i-1} \oplus G_i\arrow[r]\arrow[d]\arrow[r] & G_{i} \oplus G_{i+1}\arrow[r]\arrow[d] & \hdots\\
				\scr{G} & \hdots\arrow[r] & G_{i-1}\arrow[r] & G_i\arrow[r] & G_{i+1}\arrow[r] & \hdots
			\end{tikzcd}
		\end{equation}
		which induces the exact sequence \eqref{eq:mapping_exact_sequence}.
	\end{proof}
	
	\subsection{Exterior algebra}
	Throughout, $R$ is a commutative ring with unit and $M$ a left $R$-module.
	\begin{defn}
		The \textbf{exterior algebra} associated to $M$ is the pair $(\bigwedge M, \iota: M \lto \bigwedge M)$ satisfying the following universal property: if $N$ is an $R$-algebra, and $f: M \lto N$ is an $R$-module homomorphism such that for all $m \in M, f(m)^2 = 0$ then there exists a unique $R$-algebra homomorphism $g: \bigwedge M \lto N$ making the following diagram commute:
		\begin{equation}
			\begin{tikzcd}
				M\arrow[r,"{\iota}"]\arrow[dr,swap,"{f}"] & \bigwedge M\arrow[d,"g"]\\
				& N
			\end{tikzcd}
		\end{equation}
		Moreover, if $N$ is graded and $f(M) \subseteq N_1$ then $g$ is a morphism of graded modules.
	\end{defn}
	\begin{remark}\label{rmk:existence_wedge}
		Existence of the exterior algebra is given by taking $\bigwedge M$ to be, where $m$ ranges over all $m \in M$:
		\begin{equation}
			\bigwedge M := \bigotimes M/m \otimes m
		\end{equation}
	\end{remark}
	\begin{remark}
		If $M$ is free and of finite rank, and $v_1,...,v_n$ is a basis for $M$, then a basis for $\bigwedge M$ as a vector space is given by
		\begin{equation}
			\lbrace v_{i_1} \wedge \hdots \wedge v_{i_d} \mid 1 \leq d \leq n, 1 \leq i_1 < \hdots < i_d \leq n\rbrace
		\end{equation}
		which is a set of size $2^n$.
	\end{remark}
	\begin{proposition}\label{prop:wedge_morphism}
		Let $\varphi: M \lto N$ be an $R$-module homomorphism. Then there exists a unique morphism $\wedge \varphi: \wedge M \lto \wedge N$ such that the following diagram commutes:
		\begin{equation}
			\begin{tikzcd}
				M\arrow[r,"{\varphi}"]\arrow[d] & N\arrow[d]\\
				\wedge M\arrow[r,"{\wedge \varphi}"] & \wedge N
			\end{tikzcd}
		\end{equation}
	\end{proposition}
	\begin{defn}
		As per Example \ref{ex:quotient_of_homog_by_homog} we have that the exterior algebra is $\bb{Z}$-graded. We denote the degree $d$ elements of $\bigwedge M$ by $\bigwedge^d M$.
	\end{defn}
	There are two canonical operators on the exterior algebra, which we now explain.
	\begin{defn}
		Let $x \in \bigwedge M$ be an arbitrary element. We define
		\begin{align*}
			x \wedge \und{0.2}: \bigwedge M &\lto \bigwedge M\\
			x_1 \wedge \hdots \wedge x_n &\longmapsto x \wedge x_1 \wedge \hdots \wedge x_n
		\end{align*}
	\end{defn}
	The second map is a bit harder to explain. We begin with some preliminary observations.
	\begin{lemma}
		Let $M$ be free and of finite rank. Then
		\begin{equation}
			\bigwedge^d M^{\ast} \cong (\bigwedge^d M)^\ast
		\end{equation}
	\end{lemma}
	\begin{proof}
		Let $\lambda_1,...,\lambda_n$ be elements of $M^\ast$. Define the following functional:
		\begin{align*}
			M^d &\lto R\\
			(m_1,...,m_d) &\longmapsto \operatorname{det}\big((\lambda_i m_j)_{ij}\big)
		\end{align*}
		This indeed is bilinear and so induces a map $M^{\otimes d} \lto R$ and moreover is such that any pure tensor with repeated elements maps to $0$, thus we obtain a map
		\begin{equation}
			\bigwedge M \lto R
		\end{equation}
		We have thus described a homomorphism $M^{\ast d} \lto R$ which indeed is bilinear and maps tuples with repeated elements to $0$, thus we have described a function
		\begin{equation}
			\varphi: \bigwedge^d M^{\ast} \lto \big(\bigwedge^d M\big)^{\ast}
		\end{equation}
		It remains to show that this is an isomorphism, and for this we use for the first time that $M$ is free of finite rank. Let $v_{i_1},...,v_{i_d} \in M$ be a basis. One can show
		\begin{equation}
			\varphi(v_{i_1} \wedge \hdots \wedge v_{i_d}) = (v_{i_1} \wedge \hdots \wedge v_{i_d})^{\ast}
		\end{equation}
		and so $\varphi$ maps onto a basis for $\big(\bigwedge^d M\big)^\ast$ so in particular $\varphi$ is surjective. Since $\varphi$ is a surjective map between vector spaces of the same, finite dimension, it must therefore also be injective.
	\end{proof}
	\begin{remark}
		Another simple but important observation is that $\bigwedge^d \und{0.2}$ is a functor.
	\end{remark}
	We can now define the second canonical map.
	\begin{defn}
		Assume that $M$ is free of finite rank. Let $\eta \in M^\ast$. There is the following sequence of compositions
		\begin{equation}\label{eq:contraction}
			\begin{tikzcd}
				{\bigwedge^d M}\arrow[r] & {\bigwedge^d M^{\ast\ast}}\arrow[r] & {\big(\bigwedge^d M^\ast\big)^\ast}\arrow[d,"{(\eta \wedge \und{0.2})^\ast}"]\\
				{\bigwedge^{d-1} M} & {\bigwedge^{d-1}M^{\ast\ast}}\arrow[l]& {\big(\bigwedge^{d-1} M^\ast\big)^{\ast}}\arrow[l]
			\end{tikzcd}
		\end{equation}
		The resulting map $\bigwedge^d M \lto \bigwedge^{d-1}M$ is \textbf{contraction} and is denoted by $\eta \lrcorner$.
		
		For an element $x \in M$ we often denote $x \wedge \und{0.2}$ by $x$ and $x^\ast \lrcorner$ by $x^\ast$.
	\end{defn}
	\begin{remark}
		We can follow the sequence of homomorphism \eqref{eq:contraction} to obtain an explicit formula for the contraction map. To this end, let $v_1,...,v_n$ be a basis for $M$ and observe the following calculation:
		\begin{align*}
			v_{i_1} \wedge \hdots \wedge v_{i_d} &\longmapsto v_{i_1}^{\ast\ast} \wedge \hdots \wedge v_{i_d}^{\ast\ast}\\
			&\longmapsto (v_{i_1}^\ast \wedge \hdots \wedge v_{i_d}^\ast)^\ast\\
			&\longmapsto (v_{i_1}^\ast \wedge \hdots \wedge v_{i_d}^\ast)^\ast\circ(\eta \wedge \und{0.2})
		\end{align*}
		We then have for any basis vector $(v_{j_1}^\ast \wedge \hdots \wedge v_{j_{d-1}}^\ast)^\ast \in (\bigwedge^{d-1}M^\ast)^\ast$ that
		\begin{align}
			(v_{i_1}^\ast \wedge \hdots \wedge v_{i_d}^\ast)^\ast&\circ(\eta \wedge \und{0.2})(v_{j_1}^\ast \wedge \hdots \wedge v_{j_{d-1}}^\ast)\\
			\label{eq:dual_half_way}&= (v_{i_1}^\ast \wedge \hdots \wedge v_{i_d}^\ast)^\ast(\eta \wedge v_{j_1}^\ast \wedge \hdots \wedge v_{j_{d-1}}^\ast)
		\end{align}
		By writing $\eta = \eta(v_1)v_1^\ast + \hdots + \eta(v_n)v_n^\ast$ we have
		\begin{align*}
			\eta \wedge v_{j_1}^\ast \wedge \hdots \wedge v_{j_{d-1}}^\ast &= (\eta(v_1)v_1^\ast+ \hdots + \eta(v_n)v_n^\ast)\wedge v_{j_1}^\ast \wedge \hdots \wedge v_{j_{d-1}}^\ast\\
			&= \sum_{k = 1}^n \eta(v_k)v_k^\ast \wedge v_{j_1}^\ast \wedge \hdots \wedge v_{j_{d-1}}^\ast
		\end{align*}
		so returning to \eqref{eq:dual_half_way}, we have
		\begin{align*}
			(v_{i_1}^\ast \wedge \hdots \wedge v_{i_d}^\ast)^\ast(\sum_{k = 1}^n \eta(v_k)v_k^\ast \wedge v_{j_1}^\ast \wedge \hdots \wedge v_{j_{d-1}}^\ast)
		\end{align*}
		which, if there exists $l \in \lbrace 1,...,d\rbrace$ such that $(i_1,...,i_{\hat{l}},...,i_d) = (j_1,...,j_{d-1})$ is equal to $(-1)^{l-1}\eta(v_{i_l})$. Hence, traversing the other direction of \eqref{eq:contraction} we see that this corresponds to the element
		\begin{equation}
			\eta_{\lrcorner}(v_{i_1} \wedge \hdots \wedge v_{i_d}) = \sum_{j = 1}^d(-1)^{j-1} \eta(v_j) v_{i_1} \wedge \hdots \wedge \hat{v_{i_j}} \wedge \hdots \wedge v_{i_d}
		\end{equation}
	\end{remark}
	\begin{remark}\label{rmk:contraction_differential}
		Notice that from \eqref{eq:contraction} and the fact that $\eta \wedge \eta \wedge \und{0.2} = 0$ it follows that contraction is a differential. Thus there is a chain complex
		\begin{equation}
			L(M) := \hdots \wedge^2 M \stackrel{\eta\lrcorner}{\lto} M \stackrel{\eta}{\lto} R \lto 0
		\end{equation}
	\end{remark}
	In fact, more can be said, we return to this after considering some category theoretic facts about the exterior algebra.
	\subsubsection{Category theoretic properties of the exterior algebra}
	The exterior algebra admits some properties which are described well using the language of category theory.
	\begin{defn}
		A \textbf{super algebra} is a graded, commutative algebra $A$ (over a ring $R$ say) with the following properties:
		\begin{itemize}
			\item for all $a,b \in A$ we have $ab = (-1)^{\operatorname{deg}a\operatorname{deg}b}ba$,
			\item if $a \in A$ is homogeneous of odd degree, then $a^2 = 0$.
		\end{itemize}
	\end{defn}
	\begin{example}
		The exterior algebra $\bigwedge M$ of a module $M$ is a super algebra.
	\end{example}
	\begin{observation}
		The wedge product $\wedge(\und{0.2})$ is a functor. This follows from Remark \ref{rmk:existence_wedge} and Proposition \ref{prop:wedge_morphism}.
	\end{observation}
	\begin{notation}
		We let $\text{mod}_R$ denote the category of commutative, left $R$-modules, and $\text{sAlg}_R$ the category of $R$-super algebras.
		
		We denote by $(\und{0.2})_1: \text{sAlg}_R \lto \text{mod}_R$ the functor which takes a super algebra to its degree $1$ component.
	\end{notation}
	\begin{observation}\label{obs:wedge_adjunction}
		The functor $\wedge(\und{0.2})$ is left adjoint to $(\und{0.2})_1$. This follows from Proposition \ref{prop:wedge_morphism}.
	\end{observation}
	We now use these observations to prove that there is a canonical isomorphism $\wedge(M) \otimes \wedge(N) \lto \wedge(M \oplus N)$.
	\begin{proposition}\label{prop:sum_tensor_wedge}
		For any pair of $R$-algebras $M,N$ there is an isomorphism
		\begin{align*}
			\Psi: \wedge ( M \oplus N ) &\lto \wedge M \otimes \wedge N\\
			\psi(m, n) &= m \otimes 1 + 1 \otimes m
		\end{align*}
	\end{proposition}
	\begin{proof}
		By Observation \ref{obs:wedge_adjunction} and that the tensor product acts as a coproduct in the category of $\text{Alg}_R$ of commutative $R$-algebras, we have the following commutative diagram, where the horizontal arrows are composition and all vertical arrows are natural isomorphisms, note also we simply write $H$ in place of $\operatorname{Hom}$:
		\begin{equation}
			\begin{tikzcd}
				H(\wedge(M \oplus N), \wedge M \otimes \wedge N) \times H(\wedge M \otimes \wedge N, \wedge(M \oplus N))\arrow[r]\arrow[d] & H(\wedge(M \oplus N), \wedge(M \oplus N))\arrow[ddd]\\
				H(M \oplus N, (\wedge M \otimes \wedge N)_1) \times H(\wedge M, \wedge(M \oplus N)) \times H(\wedge N, \wedge(M \oplus N))\arrow[d]\\
				H(M \oplus N, M \oplus N) \times H(M, M\oplus N) \times H(N, M \oplus N)\arrow[d]\\
				H(M \oplus N, M \oplus N) \times H(M \oplus N, M \oplus N)\arrow[r] & H(M \oplus N, M \oplus N)
			\end{tikzcd}
		\end{equation}
		Since the image of $\operatorname{id}_{M \oplus N}$ under
		\begin{equation}
			H(M \oplus N, M \oplus N) \times H(M \oplus N, M \oplus N)\lto H(M \oplus N, M \oplus N) \lto H(\wedge(M \oplus N), \wedge(M \oplus N))
		\end{equation}
		is $\operatorname{id}_{\wedge(M \oplus N)}$ it follows that there are canonical morphisms $\psi: \wedge(M \oplus N) \lto \wedge M \otimes \wedge N$ and $\psi': \wedge M \otimes \wedge N \lto \wedge (M \oplus N)$ such that $\psi' \psi = \operatorname{id}_{\wedge(M \oplus N)}$. A similar argument shows $\psi \psi' = \operatorname{id}_{\wedge M \otimes \wedge N}$.
	\end{proof}
	\section{Regular and quasi-regular sequences}
	This Section requires Section \ref{Sec:Graded} as a prerequisite.
	\subsection{Regular sequences and the Koszul complex}
	Throughout, all rings are commutative, associative, and unital.
	\begin{defn}\label{def:regular_sequence}
		Let $M$ be a left $R$-module. A sequence $(x_1,...,x_n)$ where each $x_i \in R$ is \textbf{regular} if
		\begin{itemize}
			\item for all $i = 1,...,n$ the element $f_i$ is a nonzerodivisor of $M/(x_1,...,x_{i-1})M$
			\item the module $M/(x_1,...,x_n)M$ is non-zero.
		\end{itemize}
	\end{defn}
	For now we focus on regular sequences of a \emph{ring}, which of course obeys the same definition as \ref{def:regular_sequence} where the ring is considered as a module over itself.
	\begin{example}
		Let $k$ be a field, the sequence $(x,y(1-x),z(1-x))$ is regular in $k[x,y,z]$
	\end{example}
	\begin{proof}
		\begin{itemize}
			\item $x$ is clearly a nonzerodivisor of $k[x,y,z]$.
			\item Say $m \in k[x,y,z]/(x)$ satisfied $m(y(1-x)) = 0$, then $y$ is a zero divisor in $k[x,y,z]/(x) \cong k[y,z]$ which is a contradiction.
			\item A similar argument shows that $z(1-x)$ is not a zero divisor of $k[x,y,z]/(x,y)$
			\item Lastly, $1 \neq 0 \in k[x,y,z]/(x,y,z)$.
		\end{itemize}
	\end{proof}
	\begin{remark}
		It is \emph{not} necessarily the case that for a regular sequence $(f_1,...,f_n)$ in a ring $R$, $f_j$ is a non zero divisor of $R/(f_1,...,f_{j-2})$. For instance, the sequence $(x,y)$ is a regular sequence in $k[x,y,w_1,w_2,...]/I$, where $k$ is a field and $I$ is the ideal generated by all $yw_i$ and all $w_i - xw_{i+1}$, even though $y$ is a zero divisor.
	\end{remark}
	One way of thinking about regular sequences is that they ``cut $R$ down" as much as possible at each stage of modding out. More precisely, if $r$ is a non zero divisor of $R$ then the map $R \to R$ given by multiplication by $r$ is injective. In this sense we ``kill just as much, if not more of $R$" by modding out by $(r)$ than if we had modded out by $(r')$, where $r' \in R$ is a zero divisor.
	%
	\begin{defn}\label{def:koszul_complex}
		Let $M$ be a left $R$-module and $x \in M$ an element. The \textbf{Koszul complex} $K(x)$ is the following chain complex
		\begin{equation}
			0 \lto R \lto M \lto \wedge^2 M \lto \wedge^3 M \lto \hdots \lto \wedge^n M \stackrel{d_x^n}{\lto} \wedge^{n+1}M \lto \hdots
		\end{equation}
		where $d_x^n: \wedge^n M \lto \wedge^{n+1}M$ is defined by the rule $m \longmapsto x \wedge m$.
		
		In the special case where $M = R^m$ and $x = (x_1,...,x_m)$ we write $K(x_1,...,x_n)$ for $K(x)$.
	\end{defn}
	\begin{example}\label{ex:K(x,y)}
		Let $M = R^2$ and let $x,y \in R$. Then $K(x,y)$ is the following chain complex:
		\begin{equation}
			0 \lto R \lto R^2 \lto \wedge^2 R^2 \lto \wedge^3 R^2 \lto 0 \lto \hdots
		\end{equation}
		which is such that the following diagram commutes, with vertical arrows isomorphisms
		\begin{equation}
			\begin{tikzcd}[row sep = huge]
				0\arrow[r]\arrow[d] & R\arrow[r,"{d_{(x,y)}^1}"]\arrow[d] & R^2\arrow[r,"{d_{(x,y)}^2}"]\arrow[d] &\wedge^2 R^2\arrow[r,"{d_{(x,y)}^3}"]\arrow[d] & \wedge^3 R^2\arrow[d]\arrow[r] &  \hdots\\
				0\arrow[r] & R\arrow[r,"{\begin{pmatrix}
						x\\
						y
					\end{pmatrix}
				}"] & R^2\arrow[r,"{\begin{pmatrix}
						y \text{  } -x
					\end{pmatrix}
				}"]  & R\arrow[r] & 0\arrow[r] & \hdots
			\end{tikzcd}
		\end{equation}
		so we obtain a simple special case.
		
		Further, in the setting where $M = R$ and $x \in R$, the Koszul complex $K(x)$ is simply multiplication by $x$:
		\begin{equation}
			0 \lto R \stackrel{x}{\lto} R
		\end{equation}
	\end{example}
	We can use the simple description given in Example \ref{ex:K(x,y)} to solve an exercise:
	\begin{exercise}
		Show that if
		\begin{equation}
			M := \begin{pmatrix}
				a & b\\
				c & d
			\end{pmatrix}
		\end{equation}
		is a matrix of elements in $R$ such that $M$ has determinant given by a unit in $R$, then $K(x,y) \cong K(ax + by, cx + dy)$.
	\end{exercise}
	\begin{proof}
		We construct the following diagram:
		\begin{equation}
			\begin{tikzcd}
				0\arrow[r] & R\arrow[r]\arrow[d,"{\operatorname{id}_R}"] & R \oplus R\arrow[r]\arrow[d,"{M}"] & R\arrow[d,"{\operatorname{det}M}"]\arrow[r] & 0\\
				0\arrow[r] & R\arrow[r] & R \oplus R\arrow[r] & R\arrow[r] & 0
			\end{tikzcd}
		\end{equation}
		which is invertible by the assumptions on $M$.
	\end{proof}
	We will now relate the homology of the Koszul complex to lengths of maximal regular sequences. In the following we make use of the notation:
	\begin{notation}
		For ideals $I,J$, denote:
		\begin{equation}
			(I:J) := \lbrace f \in R \mid fJ \subseteq I\rbrace
		\end{equation}
	\end{notation}
	\begin{observation}\label{obs:koszul_reg}
		The Koszul complex $K(x,y)$ admits $K(x)$ as a subcomplex, which then pushes forward to a cokernel, yielding the following commutative diagram where the vertical sequences are exact:
		\begin{equation}\label{eq:sub_quotient}
			\begin{tikzcd}[row sep = huge]
				0\arrow[r] & R\arrow[r,"{x}"] & R\arrow[r] & 0\\
				0\arrow[r] & R\arrow[u,"{\operatorname{id}_R}"]\arrow[r,"{\begin{pmatrix}
						x\\
						y
					\end{pmatrix}
				}"] & R \oplus R\arrow[r,"{\begin{pmatrix}
						y \text{  } -x
					\end{pmatrix}
				}"]\arrow[u,"{\pi_2}"] & R\arrow[u]\\
				& 0\arrow[u]\arrow[r] & R\arrow[r,"{-x}"]\arrow[u,"{\iota_1}"] & R\arrow[u,swap,"{\operatorname{id}_R}"]
			\end{tikzcd}
		\end{equation}
		and so we obtain a long exact sequence of homology:
		\begin{equation}
			0\lto H^0(K(x)) \stackrel{\delta}{\lto} H^0(K(x)) \lto H^1(K(x,y)) \lto H^1(K(x))\lto 0
		\end{equation}
		where the connecting morphism $\delta$ is multiplication by $y$ (as can easily be checked).
		
		Notice that if $H^1(K(x,y))=0$ then
		\begin{equation}
			H^0(K(x))/yH^0(K(x)) \cong 0
		\end{equation}
		Under the further assumption that $R$ is a Noetherian local and $y$ is an element of the maximal ideal, we obtain from Nakayama's Lemma that $H^0(K(x)) \cong 0$.
		
		So what is the consequence of this? Since $H^0(K(x)) \cong 0$, we have that $x$ is a nonzerodivisor, as follows straight from the definition. Now we investigate $H^1(K(x,y)) \cong 0$. Since $x$ is a nonzerodivisor, if we have $a,b \in R$ such that $-ax + by = 0$, then $a$ is uniquely determined by $b$, we let $k_a$ denote this $b$. In fact, we obtain an isomorphism
		\begin{align*}
			\gamma: (x:y) &\lto \operatorname{ker}(x\text{ }y)\\
			a &\longmapsto (a,-k_a)
		\end{align*}
		Moreover, the image of $R \lto R \oplus R$ is isomorphic to $(x)$, so we have
		\begin{equation}
			H^1(K(x,y)) \cong (x:y)/(x)
		\end{equation}
		So $H^1K(x,y) \cong 0$ implies $(x:y) = (x)$. In other words, if $f \in R$ is such that $fy \in (x)$ then $f \in (x)$. That is to say that $y$ is a nonzerodivisor of $R/(x)$.
	\end{observation}
	Thus we have proved (the first part of):
	\begin{proposition}\label{prop:Zero_KozHom_reg}
		If $R$ is a Noetherian local ring, and $x,y$ are elements of the maximal ideal, then $H^1(K(x,y)) \cong 0$ if and only if $x,y$ is a regular sequence of $R$.
	\end{proposition}
	Do regular sequences remain regular if the elements are permuted? In general, no, as Example \ref{ex:permute_reg} shows, but Observation \ref{obs:koszul_reg} can be used to provide a setting where permuting elements of a regular sequence \emph{does} result in a regular sequence (see Proposition \ref{prop:reg_permute}).
	\begin{example}\label{ex:permute_reg}
		Consider the ring $R := k[x,y,z]/(xz)$ along with the sequence $(x-1,xy)$. This sequence is regular as $x-1$ is not a zerodivisor of $R$ and $R/(x-1) \cong k[y] \not\cong 0$ inside which $y$ is not a zerodivisor. However, the sequence $(xy, x-1)$ is not regular as $xy$ is a zero divisor in $R$.
	\end{example}
	\begin{proposition}\label{prop:reg_permute}
		Let $R$ be a Noetherian local ring with maximal ideal $\frak{m}$ and let $(x_1,...,x_n)$ be a regular sequence with each $x_i$ an element of $\frak{m}$. Then any for any permutation $\rho: \lbrace 1,...,n \rbrace \lto \lbrace 1,...,n \rbrace$ the sequence $(x_{\rho(1)},...,x_{\rho(n)})$ is regular.
	\end{proposition}
	\begin{proof}
		First we prove the case when $n = 2$. We have already seen that the sequence $(x_1,x_2)$ is regular if and only if $H^1(K(x_1,x_2)) \cong 0$ (in the context given by the hypotheses). We then observe the following isomorphism $K(x_1,x_2) \cong K(x_2,x_1)$, where $s: R \oplus R \lto R \oplus R$ is the swap map $s(r_1,r_2) = (r_2,r_1)$.
		\begin{equation}
			\begin{tikzcd}
				0\arrow[r] & R\arrow[r]\arrow[d,"{\operatorname{id}_R}"] & R \oplus R\arrow[r]\arrow[d,"s"] & R\arrow[r]\arrow[d,"{-\operatorname{id}_R}"] & 0\\
				0\arrow[r] & R\arrow[r] & R \oplus R\arrow[r] & R\arrow[r] & 0
			\end{tikzcd}
		\end{equation}
		Now we abstract to the general setting. Let $(x_1,...,x_n)$ be regular, it suffices to show that $(x_1,...,x_{i+1},x_i,..., x_n)$ is regular. In turn, it suffices to show that $x_{i+1},x_i$ is regular in $R/(x_1,...,x_{i-2})$ which then follows from the first part of this proof.
	\end{proof}
	The Koszul complex can sometimes provide information about when a sequence is regular or not.
	\begin{thm}\label{thm:exact_reg}
		Let $M$ be a finitely generated module over a local ring $(R,\frak{m})$. Suppose $x_1,...,x_n \in \frak{m}$. If for some $k$ we have:
		\begin{equation}
			H^k(M \otimes K(x_1,...,x_n)) \cong 0
		\end{equation}
		then
		\begin{equation}
			\forall j \leq k,\quad H^j(M \otimes K(x_1,...,x_n)) \cong 0
		\end{equation}
		Moreover, if $H^{n-1}(M \otimes K(x_1,...,x_n)) \cong 0$ then $(x_1,...,x_n)$ is regular.
	\end{thm}
	We will need the following Lemma to prove Theorem \ref{thm:exact_reg}.
	\begin{lemma}\label{lem:sum_tens}
		Let $N \cong N' \oplus N''$ be a module and $x = (x',x'')$ an element of $N$. We have
		\begin{equation}
			K(x) \cong K(x') \otimes K(x'')
		\end{equation}
	\end{lemma}
	\begin{proof}
		We have from Proposition \ref{prop:sum_tensor_wedge} that there exists an isomorphism of graded algebras $\wedge N \cong \wedge N' \otimes \wedge N''$, hence it suffices to check commutativity of the following diagram, in what follows we write $\Psi^n$ for the homomorphism $\Psi$ restricted to $\wedge^n N$.
		\begin{equation}
			\begin{tikzcd}
				\hdots\arrow[r] & \wedge^n N\arrow[r]\arrow[d,"{\Psi^n}"] & \wedge^{n+1} N\arrow[r]\arrow[d,"{\Psi^{n+1}}"] & \hdots\\
				\hdots\arrow[r] & (\wedge N' \otimes \wedge N'')^{n}\arrow[r] & (\wedge N' \otimes \wedge N'')^{n+1}\arrow[r] & \hdots 
			\end{tikzcd}
		\end{equation}
		To check commutativity of this, we consider an arbitrary element $y \in \wedge N$ which maps under $\Psi$ to a pure tensor $y_1 \otimes y_2$, indeed it suffices to consider such elements. We calculate:
		\begin{align*}
			x \wedge y &\longmapsto (x_1 \otimes 1 + 1 \otimes x_2) \wedge (y_1 \otimes y_2)\\
			&= x_1 \wedge y_1 \otimes y_2 + (-1)^{y_1}y_1 \otimes x_2 \wedge y_2
		\end{align*}
		On the other hand, we have
		\begin{align*}
			(d_{x_1} \otimes d_{x_2})(y_1 \otimes y_2) &= d_{x_1}(y_1) \otimes y_2 + (-1)^{y_2}y_1 \otimes d_{x_2}(y_2)\\
			&= x_1 \wedge y_1 \otimes y_2 + (-1)^{y_1}y_1 \otimes x_2 \wedge y_2
		\end{align*}
		The result follows.
	\end{proof}
	\begin{remark}\label{rmk:where_mult}
		We touch on a subtle point. Notice that Definition \ref{def:koszul_complex} defined the Koszul complex in a general setting where the differential is given by multiplication by an element of the \emph{module} $M$ (as apposed to multiplication by an element of the \emph{ring} $R$). We wish to relate the Koszul complex $K(x_1,...,x_n)$ to regularity of the sequence $(x_1,...,x_n)$ however in Definition \ref{def:regular_sequence} we required that $x_1,...,x_n$ be elements of $R$. Hence, in order to relate the Koszul complex to regularity of a sequence, we will chiefly be concerned with the special case of the Koszul complex where multiplication is by an element in the \emph{ring} $R$. In this case, for $x \in R$ we have $M \otimes K(x) \cong K(x\cdot 1_R)$ which follows from the isomorphism $R \otimes M \cong M$.
	\end{remark}
	\begin{remark}\label{rmk:long_exact_sequence}
		Recall that we established in a general setting the existence of a long exact sequence given a chain complex $\scr{G}$ over a ring $R$ along with an element $y \in R$ (Proposition \ref{prop:mapping_exact_sequence}). If $M$ is a module, $x_1,x_2 \in R$, we let $\scr{G}$ be $M \otimes K(x_1)$ and $y = x_2$, we first note that:
		\begin{align*}
			K(x_2) \otimes (M \otimes K(x_1)) &= M \otimes K(x_1,x_2)
		\end{align*}
		and hence we obtain the following long exact sequence.
		\begin{equation}
			0 \lto H^0(M \otimes K(x_1)) \stackrel{x_2}{\lto} H^0(M \otimes K(x_1)) \lto H^1(M \otimes K(x_1,x_2)) \lto 0
		\end{equation}
		In fact, more can be said. Recall the following identity which holds for all $1 \leq m \leq n$.
		\begin{equation}
			{n-1 \choose m} + {n-1 \choose m-1} = {n \choose m}
		\end{equation}
		Hence there exists an isomorphism:
		\begin{equation}
			\wedge^{m}R^{n-1} \oplus \wedge^{m-1}R^{n-1} \cong \wedge^m R^n
		\end{equation}
		which in turn implies the existence of an isomorphism:
		\begin{equation}
			\Psi_m: (M \otimes \wedge^{m}R^{n-1}) \oplus (M \otimes \wedge^{m-1}R^{n-1}) \cong M \otimes \wedge^mR^n
		\end{equation}
		This can be used to show that $K(x_n) \otimes (M \otimes K(x_1,...,x_{n-1})) \cong M \otimes K(x_1,...,x_n)$, simply observe the following isomorphism of chain complexes, where the top row is $M \otimes K(x_1,...,x_n)$ and the bottom row is $K(x_n) \otimes (M \otimes \wedge^{m-1}R^{n-1})$.
		\begin{equation}
			\begin{tikzcd}[column sep = small]
				0\arrow[r] & M\arrow[r]\arrow[d] & M \otimes R^{n}\arrow[r]\arrow[d,"{\psi_1}"] & M \otimes \wedge^2 R^n\arrow[r]\arrow[d,"{\psi_2}"] & \hdots\arrow[r] & M \otimes \wedge^n R^n\arrow[r]\arrow[d,"{\psi_n}"] & 0\\
				0\arrow[r] & M\arrow[r] & M\otimes(R \oplus R^{n-1} )\arrow[r] & M \otimes ( R^{n-1} \oplus \wedge^2 R^{n-1})\arrow[r] & \hdots\arrow[r] & M \otimes (\wedge^{n}R^{n-1} \oplus \wedge^{n-1}R^{n-1})\arrow[r] & 0
			\end{tikzcd}
		\end{equation}
		Again, using Proposition \ref{prop:mapping_exact_sequence} we obtain a long exact sequence.
		\begin{align*}
			\hdots &\lto H^i(M \otimes K(x_1,...,x_{n-1})) \stackrel{x_n}{\lto} H^i(M \otimes K(x_1,...,x_{n-1})) \lto H^{i+1}(M \otimes K(x_1,...,x_n))\\
			&\lto H^{i+1}(M \otimes K(x_1,...,x_{n-1})) \stackrel{x_n}{\lto} \hdots
		\end{align*}
	\end{remark}
	We are nearly in a position to prove Theorem \ref{thm:exact_reg}, however we need one more result. Proposition \ref{prop:kozology_explicit} writes $H^i(M \otimes K(x_1,...,x_n))$ out in an explicit form. We adopt the following notation, where $I$ is an ideal of $R$ and $M,N$ are $R$-modules:
	\begin{equation}
		(N:IM) := \lbrace m \in M \mid Jm \subseteq N\rbrace
	\end{equation}
	Notice that $(N:IM)$ is itself an $R$-module.
	\begin{proposition}\label{prop:kozology_explicit}
		Let $M$ be finitely generated and $(x_1,...,x_n)$ is a regular sequence. Then:
		\begin{equation}\label{eq:explicit}
			H^i(M \otimes K(x_1,...,x_n)) \cong \big((x_1,...,x_i)M:(x_1,...,x_n)\big)/(x_1,...,x_i)M
		\end{equation}
	\end{proposition}
	\begin{proof}
		We proceed by induction on $n$. The base case, when $n = 2$, is proved in an exactly similar way to what was done in Observation \ref{obs:koszul_reg}. Now we assume that $n > 3$ and the result holds for $n-1$. Consider the following.
		\begin{align*}
			H^i(M \otimes K(x_1,...,x_{n-1})) &\cong \big((x_1,...,x_i)M:(x_1,...,x_{n-1})\big)/(x_1,...,x_i)M\\
			&\cong 0
		\end{align*}
		where the first $\cong$ follows from the inductive hypothesis and the second $\cong$ follows from the fact that $(x_1,...,x_n)$ is a regular sequence. Using the long exact sequence of Remark \ref{rmk:long_exact_sequence} we infer that the kernel of the endomorphism on the following module given by multiplication by $x_n$.
		\begin{equation}
			H^{i}(M \otimes K(x_1,...,x_{n-1}))
		\end{equation}
		is isomorphic to $H^{i}(M \otimes K(x_1,...,x_n))$. We now use the inductive hypothesis to infer that $H^i(M \otimes K(x_1,...,x_n))$ is isomorphic to the kernel of the endomorphism on the following module given by multiplication by $x_n$.
		\begin{equation}
			\big((x_1,...,x_i)M:(x_1,...,x_{n-1})\big)/(x_1,...,x_i)M
		\end{equation}
		The proof is then complete once it is shown that the kernel of this map is isomorphic to the module given in Equation \ref{eq:explicit}. Indeed, an isomorphism is given by the rule $m \longmapsto m$, one checks that the defining conditions of both modules are equivalent.
	\end{proof}
	\begin{proof}[Proof of Theorem \ref{thm:exact_reg}]
		We proceed by induction on $n$. The base case, that $K(M \otimes K(x_1,x_2)) \cong 0$ implies that $(x_1,x_2)$ is a regular sequence follows exactly similarly to what was shown in Observation \ref{obs:koszul_reg}. Now we proceed with the inductive step, assume that $n >2$ and assume the result holds true for all $2 \leq i < n$. We first consider the endomorphism on $H^{n-1}(M \otimes K(x_1,...,x_{n-1}))$ given by multiplication by $x_n$. Since $H^i(M \otimes K(x_1,...,x_n)) \cong 0$, it follows from the long exact sequence of Remark \ref{rmk:long_exact_sequence} that the endomorphism $x_n$ is surjective. Hence, by Nakayama's Lemma, we have that $H^{n-1}(M \otimes K(x_1,...,x_{n-1})) \cong 0$. By the inductive hypothesis, this implies that $(x_1,...,x_{n-1})$ is a regular sequence, and it remains to show that $x_n$ is not a zero divisor of $M/(x_1,...,x_{n-1})M$.
		
		To do this, we invoke Proposition \ref{prop:kozology_explicit}. Indeed,$(x_1,...,x_n)$ is regular and so:
		\begin{equation}
			\big((x_1,...,x_{n-1})M:(x_1,...,x_n)\big)/(x_1,...,x_i)M \cong 0
		\end{equation}
		completing the proof.
	\end{proof}
	The following two results are bonus, and are not relevant to the core point of this Section. Indeed, these results are used in \cite[\S17.3]{view} as part of an investigation into what happens when $R$ is not local.
	\begin{proposition}\label{prop:in_gen_ideal}
		Let $x_1,...,x_n \in R$ be elements of $R$ and $I$ the ideal they generate. Assume $y_1,...,y_r \in I$ are elements of $I$, then there is an isomorphism
		\begin{equation}
			K(x_1,...,x_n,y_1,...,y_r) \cong K(x_1,...,x_n) \otimes \wedge R^r
		\end{equation}
	\end{proposition}
	\begin{proof}
		First, write $y_i = \sum_{j = 1}^n a_{ij}x_j$ for each $y_i$ and let $A$ denote the matrix $(a_{ij})$. Then there is an automorphism of $R^n \oplus R^r$ given by the matrix
		\begin{equation}
			F:= \begin{pmatrix}
				I & 0\\
				-A & I
			\end{pmatrix}
		\end{equation}
		indeed an inverse is given by
		\begin{equation}
			\begin{pmatrix}
				I & A\\
				0 & I
			\end{pmatrix}
		\end{equation}
		Notice that $F$ is such that $F(x_1,...,x_n,y_1,...,y_r) = (x_1,...,x_n,0,...,0)$.  We state without proof that the Koszul complex is functorial, and so we thus have
		\begin{equation}
			K(x_1,...,x_n,y_1,...,y_r) \cong K(x_1,...,x_n,0,...,0)
		\end{equation}
		Moreover, using Lemma \ref{lem:sum_tens} we have $K(x_1,...,x_n,0,...,0) \cong K(x_1,...,x_n) \otimes K(0,...,0) \cong K(x_1,...,x_n) \otimes \wedge R^r$. .
	\end{proof}
	\begin{cor}
		Let $M$ be any $R$-module, and $x_1,...,x_n,y_1,...,y_r$ as in Proposition \ref{prop:in_gen_ideal}. Then
		\begin{equation}
			H^\ast (M \otimes K(x_1,...,x_n,y_1,...,y_r))\cong H^\ast(M \otimes K(x_1,...,x_n)) \otimes \wedge R^r
		\end{equation}
		and so for each $i$,
		\begin{equation}
			H^i(M \otimes K(x_1,...,x_n,y_1,...,y_r)) \cong \sum_{i = j + k}H^k(M \otimes K(x_1,...,x_n)) \otimes \wedge^j R^r
		\end{equation}
		Thus
		\begin{equation}
			H^i(M \otimes K(x_1,...,x_n,y_1,...,y_r)) = 0
		\end{equation}
		if and only if
		\begin{equation}
			H^k(M \otimes K(x_1,...,x_n)) \cong 0\text{ for all }k\text{ such that }i-r \leq k \leq i
		\end{equation}
	\end{cor}
	\begin{proof}
		The first statement follows from flatness of $\wedge^j R^r$ (indeed, it is free), the rest are obvious.
	\end{proof}
	
	
	
	\subsection{Regular sequences are quasi-regular}
	Now, let $(f_1,...,f_n)$ be regular in some ring $R$ and denote by $J$ the ideal generated by these elements. For any $m \geq 0$ the scalar multiplication by $R$ on $J^m/J^{m+1}$ descends to one of $R/J$, thus rendering $J^m/J^{m+1}$ an $R/J$-module. Moreover, these scalars can be extended to $(R/J)[x_1,...,x_n]$ by defining $x_i\cdot [r]_J = [f_i r]_J = [0]_J$. There is then an $(R/J)[x_1,...,x_n]$-module homomorphism
	\begin{equation}
		\label{quasi}
		(R/J)[x_1,...,x_n] \to \bigoplus_{m \geq 0}J^m/J^{m+1}
	\end{equation}
	defined by the rule
	\[x_1^{i_1}...x_n^{i_n} \mapsto f_1^{i_1}...f_n^{i_n}\operatorname{mod}J^{i_1 + ... + i_n + 1}\]
	which is surjective.
	\begin{defn}
		Such a sequence is \textbf{quasi-regular} if the above map is an isomorphism.
	\end{defn}
	Indeed this is to be thought of as a weakening of the notion of regular sequences, as justified by the following Lemma:
	\begin{lemma}
		If a sequence $(f_1,...,f_n)$ of $R$ is regular, it is quasi-regular.
	\end{lemma}
	\begin{proof}
		Throughout, the notation $|I|$ where $I$ is a sequence of natural numbers will mean $\sum_{i \in I}i$.\\\\
		%
		We proceed by induction on $n$. When $n = 0$ notice that the composite
		\[(R/J) \stackrel{(\ref{quasi})}{\longrightarrow}\bigoplus_{m \geq 0}J^m/J^{m+1} \cong R/J\]
		is the identity map, so the result clearly holds for the base case.\\\\
		%
		Now say $n \geq 1$. Let $\sum_{|I| = m}[\alpha_I]_{J} [f^I]_{J^{m+1}} = [0]_{J^m}$, in other words, say $\sum_{|I| = m}\alpha_I f^I$ as an element of $R$ is in $J^{m+1}$. Let $\sum_{|I| = m}\alpha_I f^I = \sum_{|I'| = m + 1}\beta_{I'}f^{I'}$. By substituting each $\beta_{I'}$ by $\hat{\beta}_I := \beta_{I'}f_{i_1}$, we have $\sum_{|I| = m}\alpha_I f^I = \sum_{|I| = m}\hat{\beta}_If^I$, where each $\hat{\beta}_I \in J$. That is to say, $\sum_{|I| = m}\hat{\alpha}_If^I = 0$ where $\hat{\alpha}_I = \alpha_I - \hat{\beta}_I$. Thus we may assume that in fact $\sum_{|I| = m} \alpha_If^I = 0$. It remains to show that each $\alpha_I \in J$.\\\\
		%
		Next we rewrite $\sum_{|I| = m} \alpha_If^I$ as a sum where each occurrence of $f_n$ in $f^I$ has been factored out. We let $m'$ denote the largest integer such that a summand of $\sum_{|I| = m} \alpha_If^I$ contains $m'$ factors of $f_n$ in the product $f^I$:
		\[\sum_{|I| = m} \alpha_If^I = \sum_{j = 0}^{m'}\Big(\sum_{|I'| = m - j} \alpha_{I,j}f^{I',j}\Big)f_n^j = 0\]
		the relabelling of $\alpha_I$ by $\alpha_{I,j}$ is for clarity later on. We now prove that in such a setting, we have that $\alpha_I \in J$ by induction on $m'$.\\\\
		%
		Denote the ideal $(f_1,...,f_{n-1})$ by $J'$. If $m'$ = 0 then $\sum_{|I'| = m} \alpha_If^{I'} = 0$ where $f^{I'} \in (f_1,...,f_{n-1})^m$ and so each $\alpha_I \in J$ by the hypothesis of induction on $n$.\\\\
		%
		Now say $m' \geq 1$. Then (and this is the step which takes advantage of reducing the proof to the case when $\sum_{|I| = m} \alpha_If^I = 0$):
		\[\big(\sum_{|I'| = m - m'} \alpha_{I,m'}f^{I',j}\Big)f_n^{m'} = -\Big(\sum_{j = 0}^{m' - 1}\big(\sum_{|I'| = m - j} \alpha_{I,j}f^{I',j}\big)f_n^j\Big) \in (J')^{m-m'+1}\]
		That is to say, $\Big(\sum_{|I'| = m - m'}[\alpha_{I,m'}]_{J}[f^{I'}]_{(J')^{m-m'+1}}\Big)[f_n^{m'}]_{(J')^{m-m'+1}} = [0]_{(J')^{m - m' + 1}}$. It follows by the hypothesis of induction on $n$ that $f_n^{m'}\alpha_I \in J'$. Now we make use of the hypothesis that $(f_1,...,f_n)$ is regular, and indeed this is the key moment in the proof. Since $f_n^{m'}$ is not a zero divisor of $R/J'$, we deduce that $\alpha_{I,m'} \in J' \subseteq J$. It now remains to show that the remaining $\alpha_{I,j} \in J$.\\\\
		%
		For this, we write:
		\[\sum_{j = 0}^{m'}\Big(\sum_{|I'| = m-j}\alpha_{I,j}f^{I',j}\Big)f_n^j = \sum_{|I'| = m - j}\big(\alpha_{I,m'-1}f^{I',j} + f_n\alpha_{I,m'}f^{I',j}\big)f_n^{m'-1} + \sum_{j = 0}^{m' - 2}\Big(\sum_{|I'| = m-j}\alpha_{I,j}f^{I',j}\Big)f_n^{j} = 0\]
		so by the hypothesis of induction on $m'$ we have that $\alpha_{I,m'-1} + f_n\alpha_{I,m'} \in J$ and $\alpha_{I,j} \in J$ for all $j \leq m' - 2$. The final observation to make is that since $f_n\alpha_{I,m'} \in J$ it follows that $\alpha_{I,m'-1} \in J$.
	\end{proof}
	%
	\section{Clifford algebras}
	\subsection{Bilinear/Quadratic forms}
	Throughout $V$ is a finite dimensional $k$-vector space. 
	
	This Section considers vector spaces equipped with either a bilinear form or a quadratic form (which due to \ref{prop:bil_quad_form} amounts, in the case where $k$ is of characteristic not equal to 2,  to the same thing).
	\begin{defn}
		A bilinear map $B:V \times V \lto k$ is sometimes called a \textbf{bilinear form}. If $v_1,...,v_n$ is a basis for $V$ then for any $u = u_1v_1 + \hdots u_n v_n,w = w_1v_1 + \hdots w_n v_n \in V$ the value $B(u,w)$ can be calculated by
		\begin{equation}
			\begin{bmatrix}
				w_1 & \hdots & w_n
			\end{bmatrix}
			\begin{bmatrix}
				B(v_1,v_1) & \hdots & B(v_1, v_n)\\
				\vdots & \ddots & \vdots\\
				B(v_n,v_1) & \hdots & B(v_n,v_n)
			\end{bmatrix}
			\begin{bmatrix}
				u_1\\
				\vdots\\
				u_n
			\end{bmatrix}
		\end{equation}
		and so given a choice of basis for $V$ there exists an isomorphism between the vector space of bilinear forms and the vector space of $n \times n$ matrices with entries in $k$. If $\scr{B}$ is a basis for $V$, the matrix corresponding to $B$ is denoted $[B]_{\scr{B}}$.
		
		A bilinear form $B: V \times V \lto k$ is \textbf{symmetric} if for all $v,u \in V$ we have $B(v,u) = B(u,v)$.
	\end{defn}
	\begin{defn}
		A \textbf{quadratic form} is a function $Q: V \lto k$ satisfying the following properties:
		\begin{itemize}
			\item for all $a \in k$ and $v \in V$, we have $Q(av) = a^2Q(v)$,
			\item the function $B: V \times V \lto k$ given by $B(v,u) = Q(v + u) - Q(v) - Q(u)$ is bilinear.
		\end{itemize}
	\end{defn}
	\begin{proposition}\label{prop:bil_quad_form}
		Let $B: V \times V \lto k$ be a symmetric bilinear form and $k$ a field of characteristic not equal to $2$. Then the function $Q_B: V \lto k$ given by $Q_B(v) = B(v,v)$ is a quadratic form.
		
		Also, given a quadratic form $Q: V \lto k$, the function $B_Q: V \times V \lto k$ given by $B_Q(v,u) = \frac{1}{2}\big(Q(v + u) - Q(v) - Q(u)\big)$ is a bilinear form.
	\end{proposition}
	\begin{proof}
		Easy.
	\end{proof}
	\begin{defn}\label{def:associated_forms}
		In the notation of Proposition \ref{prop:bil_quad_form}, $B_Q$ is the \textbf{bilinear form associated to $Q$} and $Q_B$ is the \textbf{quadratic form associated to $B$}. Notice that $B_Q$ is symmetric.
		
		We say that a bilinear form $B$ is \textbf{diagonalisable} if there exists a basis $\scr{B}$ for $V$ rendering $[B]_{\scr{B}}$ diagonal, similarly, we say that $Q$ is \textbf{diagonalisable}.
		
	\end{defn}
	\begin{proposition}\label{prop:diagonalisable_symmetric}
		Every finite dimensional bilinear form $B: V \times V \lto k$ is diagonalisable if and only if it is symmetric.
	\end{proposition}
	\begin{proof}
		The bilinear form $B$ is symmetric if and only if there exists a basis with respect to which the matrix representation of $B$ is symmetric (which would imply the matrix representation with respect to \emph{any} basis is symmetric). So since $B$ is diagonalisable we have that $B$ is symmetric.
		
		Now we prove the converse. If $B$ maps everything to zero then the result is obvious so assume this is not the case. We first prove that there exists a vector $v$ such that $Q_B(v) = B(v,v) \neq 0$. Let $u_1,u_2 \in V$ be such that $B(u_1,u_2) \neq 0$. If $B(u_1,u_1) \neq 0$ or $B(u_2,u_2) \neq 0$ then we could take $v$ to be one of $u_1,u_2$, so assume $B(u_1,u_1) = B(u_2,u_2) = 0$. We have
		\begin{equation}
			Q(u_1 + u_2) = B(u_1 + u_2, u_1 + u_2) = B(u_1,u_2) + B(u_2,u_1) = 2B(u_1,u_2) \neq 0
		\end{equation}
		where we have used both the assumptions that $B$ is symmetric and that the characteristic of $k$ is not 2. We can thus take $v$ to be $u_1 + u_2$.
		
		We proceed by induction on the dimension of $V$, with the base case $\operatorname{dim}V = 1$ being trivial.
		
		Say $\operatorname{dim}V = n > 1$. Consider the map $\varphi_v: V \lto k$ given by $\varphi_v(u) = B(u,v)$. Since $B(v,v) \neq 0$ we have that $\operatorname{im}\varphi_v = k$ and so $\operatorname{ker}\varphi_v = \operatorname{dim}_kV - 1$. We have by the inductive hypothesis that $B\restriction_{\operatorname{ker}\varphi_v \times \operatorname{ker}\varphi_v}$ is diagonalisable. Fix a basis $\scr{B} := \lbrace v_1,...,v_{n-1}\rbrace$ of $\operatorname{ker}\varphi_v \times \operatorname{ker}\varphi_v$ so that the top left $n-1 \times n-1$ minor of the matrix representation of $B$ with respect to this basis is diagonal. We extend $\scr{B}$ to a basis $\scr{B}'$ for $V$ by taking $\scr{B} := \scr{B} \bigcup \lbrace v_n\rbrace$ with $v$ and notice that $B(v_i,v) = B(v,v_i) = 0$ for all $i = 1,...,n-1$ (using that the top left minor of $V$ is diagonal and symmetry of $B$). We thus have a basis $\lbrace v_1,...,v_{n-1},v\rbrace$ with respect to which the matrix representation of $V$ is diagonal.
	\end{proof}
	\begin{proposition}\label{prop:one_negative_one}
		Say $V$ is finite dimensional of dimension $n$. By Proposition \ref{prop:diagonalisable_symmetric} the quadratic form $Q$ is diagonalisable, in fact, more can be said:
		\begin{itemize}
			\item if $k = \bb{R}$ then there exists a basis for $V$ and $0 \leq r \leq n$ such that $Q$ with respect to this basis has diagonal entries
			\begin{equation}
				\lambda_1 = \hdots = \lambda_r = 1, \qquad \lambda_{r+1} = \hdots = \lambda_n = -1
			\end{equation}
			\item if $k = \bb{C}$ then there exists a basis for $V$ such that $Q$ with respect to this basis has diagonal entries
			\begin{equation}
				\lambda_1 = \hdots = \lambda_n = 1
			\end{equation}
		\end{itemize}
	\end{proposition}
	\begin{proof}
		Let $v_1,\hdots,v_n$ be a basis with respect to which $Q$ is diagonal with diagonal entries $\lambda_1,\hdots,\lambda_n$. We proceed by induction on $n$. Say $n = 1$ and let $e$ be the chosen basis vector of $V$,and say $k = \bb{R}$,  we have
		\begin{equation}
			B_Q(v_1,v_2) = v_2e\cdot \lambda_1 \cdot v_1 e =
			\begin{cases}
				v_2 \sqrt{\lambda_1}e\cdot 1 \cdot v_1\sqrt{\lambda_1} e,&\lambda_1 \geq 0,\\
				v_2 \sqrt{-\lambda_1}e\cdot -1 \cdot v_1\sqrt{-\lambda_1} e,& \lambda_2 < 0
			\end{cases}
		\end{equation}
		so we can replace the basis $e$ by either $\sqrt{\lambda_1}e$ or $\sqrt{-\lambda_1}e$ and we are done. In the case when $k = \bb{C}$, there always exists a square root of $\lambda_1$.
		
		The logic of the inductive step is exactly similar.
	\end{proof}
	\begin{proposition}\label{prop:signature}
		Say $V$ is a real vector space of dimension $n$. By Proposition \ref{prop:one_negative_one} there exists a basis of $V$ for which $[B]_{\scr{B}}$ is diagonal with all entries equal to either 1 or $-1$. The triple $(n_{+},n_{-},n_0)$ consisting of the number $n_{+}$ of positive entries, the number $n_{-}$ of negative entries, and the number $n_{0}$ of entries equal to zero in a $[B]_{\scr{B}}$ is independent of the choice of diagonalising basis $\scr{B}$. 
	\end{proposition}
	\begin{proof}
		Write
		\begin{equation}
			[B]_{\scr{B}} = 
			\begin{bmatrix}
				I_{p} & &\\
				& -I_{q} &\\
				& & 0_{r}
			\end{bmatrix}
		\end{equation}
		Denote by $W \subseteq V$ the largest subspace such that $B\restriction_{W \times W}$ is positive definite, ie, $B(w,w) > 0$ for all $w\in W$.  Letting $w = w_1v_1 + \hdots w_nv_n$ and calculating $B(w,w)$ using $[B]_{\scr{B}}$ we have
		\begin{equation}
			w^T[B]_{\scr{B}}w = w_1^2 + \hdots w_p^2 - w_{p+1}^2 - \hdots - w_{p+q}^2
		\end{equation}
		and so $w^t[B]_{\scr{B}}w >0$ if and only if $w_{p+1} = \hdots = w_{p+q} = 0$. We thus have $$W \subseteq \operatorname{Span}(v_1,...,v_p)$$ Letting $W'$ denote this span, we clearly also have $W' \subseteq W$, implying $p = \operatorname{dim}W$. Thus $p$ has been related to a value which is basis independent and so $p$ is an invariant. The remaining invariances follow from the rank-nullity Theorem.
	\end{proof}
	
	\begin{defn}
		In the notation of Proposition \ref{prop:signature}, the triple $(n_{+},n_{-},n_0)$ is the \textbf{signature} of $B$.
		
		If $n_0 = 0$ then the bilinear form is \textbf{nondegenerate}.
	\end{defn}
	
	\begin{remark}\label{rmk:signature_complex}
		The number of entries equal to $1$ in a matrix representation of a symmetric bilinear form on a finite dimensional complex vector space is also an invariant, this follows directly from the rank-nullity Theorem.
	\end{remark}
	
	\subsection{Clifford algebras}
	Throughout, we denote by $(V,Q)$ a quadratic form, consisting of a $k$-vector space $V$ and a quadratic form $Q: V \lto k$ on $V$. The field $k$ is assumed to have characteristic not equal to 2.
	\begin{defn}
		A pair $(C_Q,j)$ consisting of a $k$-algebra $C_Q$ and a linear transformation $j: V \lto C_Q$ such that
		\begin{equation}
			\forall v \in V, j(v)^2 = Q(v)\cdot 1
		\end{equation}
		is a \textbf{clifford algebra for $(V,Q)$} if it is universal amongst such maps. That is, for every pair $(D,k)$ consisting of a $k$-algebra $D$ and a linear transformation $k: V \lto D$ satisfying
		\begin{equation}
			\forall v \in V, k(v)^2 = Q(v)\cdot 1
		\end{equation}
		there exists a unique $k$-algebra homomorphism $m: C_Q \lto D$ such that the following diagram commutes
		\begin{equation}
			\begin{tikzcd}
				V\arrow[r,"j"]\arrow[rd,"k"] & C_Q\arrow[d,"m"]\\
				& D
			\end{tikzcd}
		\end{equation}
	\end{defn}
	\begin{proposition}\label{prop:clifford_construction}
		A Clifford algebra for $(V,Q)$ always exists and is essentially unique (unique up to unique isomorphism).
	\end{proposition}
	\begin{proof}[Proof (sketch)]
		We construct the tensor algebra
		\begin{equation}
			T(V) := \bigoplus_{i \geq 0}V^{\otimes i}
		\end{equation}
		(where $V^{\otimes 0} := k$) quotiented by the ideal $I$ generated by the set $\lbrace v \otimes v - Q(v)\cdot 1\rbrace_{v \in V}$. The map $j: V \lto C_Q$ is the inclusion $V \lto T(V)$ composed with the projection $T(V) \lto T(V)/I$.
	\end{proof}
	Notice that $j$ given in the proof of Proposition \ref{prop:clifford_construction} is injective.
	\begin{example}\label{ex:clifford_exterior}
		Say $Q: V \lto k$ maps everything to zero. Then the associated Clifford Algebra $(C_Q,l)$ is such that $C_Q \cong \bigwedge V$.
		
		To see this, define $\varphi: V \lto \bigwedge V$ by $v \mapsto v$. This is such that $\varphi(v)^2 = 0$ and so by the universal property of $(C_V,l)$ there exists a $k$-algebra homomorphism $\overline{\varphi}: C_Q \lto \bigwedge V$. This is clear as the definitions of $C_Q$ and $\bigwedge V$ are the same.
	\end{example}
	Example \ref{ex:clifford_exterior} shows that when $Q$ is the 0 quadratic form then the associated Clifford algebra is isomorphic to the exterior algebra, in fact, if $Q$ is \emph{not} the zero quadratic form then the associated Clifford algebra is \emph{not} isomorphic to the exterior algebra as an \emph{algebra} but whatever $Q$ is, $C_Q$ is always \emph{linearly} isomorphic (isomorphic as a vector space) to the exterior algebra, as the next Proposition states:
	\begin{proposition}\label{prop:linear_iso}
		The underlying vector spaces of $C_Q$ and $\bigwedge V$ are isomorphic.
	\end{proposition}
	Proposition \ref{prop:linear_iso} will follow from a series of observations which cover a broader scope of theory, which we now present.
	\begin{defn}
		The vector space $\operatorname{End}V$ is naturally a space equipped with a quadratic form.
	\end{defn}
	
	Consider the linear map $k: V \lto C_Q$ given by $k(v) = -j(v)$ which clearly satisfies $k(v)^2 = Q(v)\cdot 1$. There is thus an induced morphism $\beta: C_Q \lto C_Q$ rendering the following diagram commutative:
	\begin{equation}
		\begin{tikzcd}
			V\arrow[r,"j"]\arrow[dr,"k"] & C_Q\arrow[d,"\beta"]\\
			& C_Q
		\end{tikzcd}
	\end{equation}
	We have that $\beta^2 = \operatorname{id}_{C_Q}$. 
	
	\begin{defn}
		The involution $\beta$ is the \textbf{involution associated with the Clifford Algebra $(C_Q,j)$}.
	\end{defn}
	Recall that for an arbitrary involution $f: V \lto V$ (where $V$ is a vector space over a field of characteristic not equal to 2) we have
	\begin{equation}
		\forall v \in V, v = 1/2(f(v) + v) + v - 1/2(f(v) + v) = 1/2(f(v) + v) + 1/2(v - f(v))
	\end{equation}
	where we notice
	\begin{equation}
		f(1/2(f(v) + v)) = 1/2(f(v) + v),\quad \text{and}\quad f(1/2(v - f(v))) = 1/2(f(v) - v)
	\end{equation}
	and so
	\begin{equation}
		V = E_1 + E_{-1}
	\end{equation}
	where $E_i$ is the $i^\text{th}$ Eigenspace of $f$. 
	
	Applying this observation to the situation of Clifford algebras, we have:
	\begin{equation}
		C_Q^0 := \lbrace v \in C_Q^0 \mid \beta(v) = v\rbrace, \qquad C_Q^1 := \lbrace v \in C_Q^1 \mid \beta(v) = -v\rbrace
	\end{equation}
	and
	\begin{equation}
		C_Q = C_Q^0 \oplus C_Q^1
	\end{equation}
	Thus the Clifford algebra $(C_Q,j)$ associated to a quadratic form $Q: V \lto k$ is naturally a $\bb{Z}_2$-graded algebra.
	\begin{proposition}\label{prop:sum_tensor}
		For quadratic forms $Q_1: V_1 \lto k, Q_2: V_2 \lto k$ we have
		\begin{equation}
			C_{Q_1 \oplus Q_2} \cong C_{Q_1} \otimes C_{Q_2}
		\end{equation}
	\end{proposition}
	\begin{proof}
		Consider the linear transformation
		\begin{align*}
			T: V_1 \oplus V_2 &\lto C_{Q_1} \otimes C_{Q_2}\\
			(v_1,v_2) &\longmapsto v_1 \otimes 1 + 1 \otimes v_2
		\end{align*}
		We have:
		\begin{align*}
			T(v_1,v_2)^2 &= (v_1 \otimes 1 + 1 \otimes v_2)^2\\
			&= (v_1 \otimes 1 + 1 \otimes v_2)(v_1 \otimes 1 + 1 \otimes v_2)\\
			&= v_1^2 \otimes 1 + v_1 \otimes v_2 - v_1 \otimes v_2 + 1 \otimes v_2^2\\
			&= Q_{V_1}(v_1) \otimes 1 + 1 \otimes Q_{V_2}(v_2)\\
			&= (Q_{V_1}(v_1) + Q_{V_2}(v_2))(1 \otimes 1)\\
			&= Q_{V_1 \oplus V_2}(v_1,v_2)(1 \otimes 1)
		\end{align*}
		So by the universal property of the Clifford algebra $(C_Q,j)$ there exists a $k$-algebra homomorphism $\hat{T}: C_{Q_1 \oplus Q_2} \lto C_{Q_1} \otimes C_{Q_2}$. First we prove surjectivity, it is sufficient to prove that every pure tensor $x \otimes y \in C_{Q_1} \otimes C_{Q_2}$ is mapped onto by some element by $\hat{T}$. Write $x \otimes y = v_1\hdots v_n \otimes u_1 \hdots u_m$ for some $u_1,...,u_n \in C_{Q_1}, v_1,...,v_m \in C_{Q_2}$. Since
		\begin{equation}
			v_1\hdots v_n \otimes u_1 \hdots u_m = (v_1 \otimes 1)\hdots(v_n \otimes 1)(1 \otimes u_1)\hdots (1 \otimes u_m)
		\end{equation}
		it suffices to show that for all pairs $(v,u) \in V_1 \times V_2$ that $v \otimes u \in C_{Q_1} \otimes C_{Q_2}$ is mapped onto by some element by $\hat{T}$. Indeed:
		\begin{equation}
			T(v \otimes 1 + 1 \otimes u)T(1 \otimes u + v \otimes 1) = 2v\otimes u
		\end{equation}
		Surjectivity follows.
		
		\textcolor{red}{Injectivity?}
	\end{proof}
	\begin{defn}
		A bilinear form or a quadratic form is \textbf{finite dimensional} if $V$ is.
	\end{defn}
	For the next result, recall that a finite dimensional bilinear form is diagonalisable if and only if it is symmetric (Proposition \ref{prop:diagonalisable_symmetric}):
	
	We are now in a position to describe a basis for $C_Q$ given one for $V$:
	\begin{proposition}\label{prop:decomp_even_odd}
		Let $v_1,...,v_n$ be a basis for $V$. The set:
		\begin{equation}
			\scr{B} := \lbrace v_{i_1}...v_{i_m} \mid m \leq n, v_{j} \in V, 0 \leq i_1 < \hdots < i_m \leq n\rbrace
		\end{equation}
		forms a basis for $C_Q$. In particular,
		\begin{equation}\label{eq:dimension_clifford}
			\operatorname{dim}_kC_Q = 2^{\operatorname{dim}_kV}
		\end{equation}
	\end{proposition}
	\begin{proof}
		This set clearly linearly generates $C_Q$ and so it suffices to show that \eqref{eq:dimension_clifford} holds.
		
		By Proposition \ref{prop:diagonalisable_symmetric} we have that $Q = Q_1 \oplus \hdots \oplus Q_n$ and by Proposition \ref{prop:sum_tensor} it follows that $C_{Q_1 \oplus \hdots \oplus Q_n} \cong C_{Q_1} \otimes \hdots \otimes C_{Q_n}$. Thus it suffices to prove the case when $\operatorname{dim}_kV = 1$. This can be directly analysed; we know
		\begin{equation}
			C_Q \cong C^0_Q \oplus C^1_Q
		\end{equation}
		and $C^0_Q = k, C^1_Q = k\cdot e$, where $e \neq 0$ is some non-zero element of $V$. Thus the dimension of $C_Q$ in this case is 2.
	\end{proof}
	\begin{proposition}
		Say $V$ is finite dimensional and $v_1,...,v_n$ is a basis such that $B(v_i,v_j) = 0$ for all $i \neq j$. Then the Clifford algebra $C_Q$ is multiplicatively generated by $v_1,...,v_n$ which satisfy the relations
		\begin{equation}\label{eq:clifford_relations}
			v_i^2 = Q(v_i),\qquad v_iv_j + v_jv_i = 0, i \neq j
		\end{equation}
	\end{proposition}
	\begin{proof}
		The only non-obvious part follows from the calculation
		\begin{align*}
			(v_i + v_j)^2 &= Q(v_i + v_j)\\
			&= B(v_i + v_j, v_i + v_j)\\
			&= B(v_i,v_i) + 2B(v_i,v_j) + B(v_j,v_j)\\
			&= Q(v_i) + Q(v_j)\\
			&= v_i^2 + v_j^2
		\end{align*}
		which implies
		\begin{equation}
			v_iv_j + v_jv_i = 0, i \neq j
		\end{equation}
	\end{proof}
	Thus we may think of a Clifford algebra with respect to a finite quadratic form as the free algebra on $\operatorname{dim}_kV$ elements subject to the relations \eqref{eq:clifford_relations}.
	
	\subsection{Clifford algebras of real or complex bilinear forms}
	In this Section we think of the Clifford algebra as associated to a symmetric bilinear form, rather than a quadratic form.  All mentions of quadratic forms should be of as induced by a symmetric bilinear form (Proposition \ref{prop:bil_quad_form}). There is no difficult difference, but we note that the correct universal property of $(C_B,j)$ is:
	\begin{equation}
		\forall v_1,v_2 \in V, j(v_1)j(v_2) + j(v_2)j(v_1) = 2B(v_1,v_2)\cdot 1
	\end{equation}
	We also introduce new notation; the Clifford algebra associated to a bilinear form $B: V \times V \lto k$ is denoted $C(V,B)$.
	\begin{defn}
		Let $Q_i: V_i \lto k$ be quadratic forms for $i = 1,2$. Let $f: V_1 \lto V_2$ be a linear map, by composing with the inclusion $l: V_2 \lto C_{V_2}$ there is an induced map $\varphi: V_1 \lto C_{V_2}$ such that for all $v \in V_1$ we have
		\begin{equation}
			\varphi(v)^2 = f(v)^2 = Q_2(f(v))\cdot 1
		\end{equation}
		and so if $Q_2(f(v)) = Q_1(v)$ for all $v \in V$ we have by the universal property of $C_{Q_1}$ that there exists a unique morphism $C_{Q_1} \lto C_{Q_2}$ which we denote by $C(f)$.
	\end{defn}
	\begin{lemma}
		The map $C(f)$ is an isomorphism if $f$ is.
	\end{lemma}
	\begin{proof}
		Easy.
	\end{proof}
	We can restate Remark \ref{rmk:signature_complex} in terms of Clifford algebras:
	\begin{cor}\label{cor:signature_determines}
		Let $k\in \lbrace \bb{R},\bb{C}\rbrace$. All Clifford algebras of quadratic forms over finite dimensional, $k$-vector spaces which admit the same signature are isomorphic.
	\end{cor}
	\begin{notation}
		We denote:
		\begin{itemize}
			\item the Clifford algebra associated to the quadratic form $(\bb{R}^n, -x_1^2 - \hdots - x_n^2)$ by $C_n$,
			\item the Clifford algebra associated to the quadratic form $(\bb{R}^n, x_1^2 + \hdots + x_n^2)$ by $C_n'$,
			\item the Clifford algebra associated to the quadratic form $(\bb{C}^n, z_1^2 + \hdots + z_n^2)$ by $C_n^{\bb{C}}$.
		\end{itemize}
	\end{notation}
	Throughout this Section, $V$ is assumed to be a vector space over $k$ with $k \in \lbrace \bb{R},\bb{C}\rbrace$, and $B: V \times V \lto k$ is a bilinear form.
	Given a real algebra $A$, the \emph{complexification} is the $\bb{C}$-algebra $A \otimes_\bb{R} \bb{C}$ with multiplication given by 
	\begin{equation}
		\big((x \otimes z),(y\otimes w)\big)\longmapsto (xy \otimes zw)
	\end{equation}
	Also, given a bilinear form $B: V \times V \lto k$ where $V$ is a real vector space, we define the \emph{complexification} of $B$ as $B_{\bb{C}}: V \otimes_{\bb{R}} \bb{C} \lto \bb{C}$ given by
	\begin{equation}
		B_{\bb{C}}\big((v_1 \otimes z_1),(v_2 \otimes z_2)\big) = B(v_1,v_2)z_1z_2
	\end{equation}
	The following Proposition shows that the Clifford algebra of a complexification behaves well:
	\begin{proposition}
		We have
		\begin{equation}
			C(V \otimes_{\bb{R}} \bb{C}, B_{\bb{C}}) \cong C(V,B) \otimes_{\bb{R}} \bb{C}
		\end{equation}
	\end{proposition}
	\begin{proof}
		Consider the map $\varphi: V \otimes_{\bb{R}}\bb{C} \lto C(V,B) \otimes_{\bb{R}} \bb{C}$ given by $\varphi(v \otimes z) = v \otimes z$. This is such that
		\begin{equation}
			\varphi(v \otimes z)^2 = (v \otimes z)^2 = v^2 \otimes z^2 = B(v,v)z^2 \cdot 1 \otimes 1 = B_{\bb{C}}\big((v\otimes z),(v\otimes z)\big)\cdot 1
		\end{equation}
		So $\varphi$ induces a map $\hat{\varphi}: C(V \otimes_{\bb{R}}\bb{C}) \lto C(V,B) \otimes_{\bb{R}} \bb{C}$ which is an isomorphism with inverse induced by the bilinear map $C(V,B) \times \bb{C} \lto C(V \otimes_{\bb{R} }\bb{C}, B_{\bb{C}})$ given by $(x,z) \longmapsto x \otimes z$.
	\end{proof}
	\begin{lemma}\label{lem:complexification}
		We have
		\begin{equation}
			C_n^{\bb{C}} \cong C_n \otimes_{\bb{R}} \bb{C} \cong C_n' \otimes_{\bb{R}} \bb{C}
		\end{equation}
	\end{lemma}
	\begin{proof}
		Denote by $B^{C_n}, B^{C_n'}$ the bilinear form associated to $C_n,C_{n}'$ respectively. We have that $\bb{R}^n \otimes_{\bb{R}}\bb{C} \cong \bb{C}^n$ and so by Corollary \ref{cor:signature_determines} it suffices to show that $B^{C_n}_{\bb{C}}$ and $B^{C_n'}_{\bb{C}}$ are non-degenerate, which is easy to show.
	\end{proof}
	\begin{example}\label{ex:em_two}
		We have $C_2^{\bb{C}} \cong C_2 \otimes_{\bb{R}}\bb{C}$, and the latter algebra is generated by $e_1,e_2$ satisfying
		\begin{equation}\label{eq:clifford_c_2}
			e_1^2 = e_2^2 = -1,\qquad e_1e_2 + e_2e_1 = 0
		\end{equation}
		On the other hand, the underlying vector space of the complex algebra $M_2(\bb{C})$ has a basis
		\begin{equation}
			I = \begin{bmatrix}
				1 & 0\\
				0 & 1
			\end{bmatrix}
			, g_1 = 
			\begin{bmatrix}
				i & 0\\
				0 & -i
			\end{bmatrix}, g_2 =
			\begin{bmatrix}
				0 & i \\
				i & 0
			\end{bmatrix}, T = 
			\begin{bmatrix}
				0 & -i\\
				i & 0
			\end{bmatrix}
		\end{equation}
		satisfying:
		\begin{equation}\label{eq:generators_matrices}
			g_1^2 = g_2^2 = -I,\qquad g_1g_2 + g_2g_1 = 0,
		\end{equation}
		which implies $C_2^{\bb{C}} \cong M_2(\bb{C})$.
	\end{example}
	\begin{remark}
		We present a more precise account of Example \ref{ex:em_two}. Let $B:\bb{R} \times \bb{R} \lto \bb{R}$ denote the bilinear form $B(x,y) = -x^2 - y^2$. We have:
		\begin{equation}
			C_2 = \bigotimes \bb{R}^2/\big((x_1,y_1)(x_2,y_2) + (x_2,y_2)(x_1,y_1) = -x_1x_2 - y_1y_2\big)
		\end{equation}
		Denote the standard basis vectors $(1,0),(0,1)$ respectively by $e_1,e_2$. We see that $C_2$ is the free algebra generated by $e_1,e_2$ subject to the relations, where $i,j = 1,2$
		\begin{equation}
			e_i e_j + e_j e_i = -\delta_{ij}
		\end{equation}
		which is exactly the relations \eqref{eq:clifford_c_2}.
		
		Thus, to obtain the isomorphism claimed by Example \ref{ex:em_two}, it suffices to show that $I,g_1,g_2,T$ constitute a basis for $M_2(\bb{C})$ as a vector space, the elements $g_1,g_2$ generate $M_2(\bb{C})$ multiplicatively, (which follows from the observation that $I,g_1,g_2,T$ are a basis and that $g_1g_2 = -iT$) and that all generators correspond to those of $C_2$. Since all possible products between the generators have been considered in \eqref{eq:generators_matrices}, we have achieved exactly this.
	\end{remark}
	
	
	
	
	Two final isomorphisms (Proposition \ref{prop:complex_plus_two}, Lemma \ref{lem:case_one}) allows for a structure Theorem (Theorem \ref{thm:structure_clifford})
	\begin{proposition}\label{prop:complex_plus_two}
		We have
		\begin{equation}
			C_{n+2} \cong C_n' \otimes_{\bb{R}}C_2,\qquad C_{n+2}' \cong C_n \otimes_{\bb{R}}C_2'
		\end{equation}
		Here the tensor product is the usual one for algebras.
	\end{proposition}
	\begin{proof}
		We satisfy ourselves with a proof sketch. The key Definition is the following:
		\begin{equation}
			u: \bb{R}^2 \lto C_n' \otimes_{\bb{R}}C_2
		\end{equation}
		defined on basis vectors $e_1,e_2 \in \bb{R}^{n+2}$ as:
		\begin{equation}
			u(e_1) = 1 \otimes e_1,\quad u(e_2) = 1 \otimes e_2,\quad u(e_j) = e_{j-2} \otimes e_1e_2, j = 3,...,n+2
		\end{equation}
		and the key calculation is
		\begin{align*}
			u(e_j)^2 &= (e_{j - 2} \otimes e_1e_2)^2\\
			&= e_{j-2}^2 \otimes e_1e_2e_1e_2\\
			&= 1 \otimes -e_1^2e_2^2\\
			&= -1
		\end{align*}
		Notice that had we mapped $u$ into $C_n \otimes_{\bb{R}} C_2$ instead of into $C_n' \otimes_{\bb{R}}C_2$ then $u(e_{j})^2 = 1$ which would not induce a map $C_{n+2} \lto C'_n \otimes_{\bb{R}}C_2$.
	\end{proof}
	\begin{remark}
		In Proposition \ref{prop:complex_plus_two}, one might suggest (INCORRECTLY!) defining $u: C_{n+2} \lto C_n \otimes_{\bb{R}}C_2$ by
		\begin{equation}
			u(e_1) = 1 \otimes e_1,\quad u(e_2) = 1 \otimes e_2,\quad u(e_j) = e_{j-2} \otimes 1, j = 3,...,n+2
		\end{equation}
		but this does not work as then (for example)
		\begin{align*}
			u(e_1)u(e_3) + u(e_3)u(e_1) &= (1 \otimes e_1)(e_1 \otimes 1) + (e_1 \otimes 1)(1 \otimes e_1)\\
			&= 2e_1 \otimes e_1 \neq 0
		\end{align*}
		So in fact, this ``twisting" $C_{n+2} \lto C'_n \otimes_{\bb{R}}C_2$ is necessary.
	\end{remark}
	\begin{cor}
		We have
		\begin{equation}
			C_{n+2}^{\bb{C}} \cong C_n^{\bb{C}} \otimes_{\bb{C}}M_2(\bb{C})
		\end{equation}
		given explicitly by the following ($g_1,g_2$ are as in Example \ref{ex:em_two})
		\begin{equation}
			e_1 \longmapsto 1 \otimes e_1,\quad e_2 \longmapsto 1 \otimes e_2,\quad e_j \longmapsto ie_{j-2} \otimes g_1g_2, j = 3,...,n+2
		\end{equation}
	\end{cor}
	\begin{proof}
		This follows from an algebraic manipulation:
		\begin{align*}
			C_{n+2}^{\bb{C}} &\cong C_{n+2} \otimes_{\bb{R}}\bb{C}\\
			&\cong (C'_n \otimes_{\bb{R}} C_2) \otimes_{\bb{R}}\bb{C}\\
			&\cong (C'_{n}\otimes_{\bb{R}}\bb{C}) \otimes_{\bb{C}} (C_2 \otimes_{\bb{R}} \bb{C})\\
			&\cong C_{n}^{\bb{C}} \otimes_{\bb{C}}C_2^{\bb{C}}\\
			&\cong C_{n}^{\bb{C}} \otimes_{\bb{C}}M_2(\bb{C})
		\end{align*}
		We note that for $j > 2$, the element $e_j$ is mapped along these isomorphisms in the following way:
		\begin{align}
			e_j &\longmapsto e_j \otimes_{\bb{R}} 1\\
			&\longmapsto (e_{j-2} \otimes_{\bb{R}} e_1e_2) \otimes_{\bb{R}} 1\\
			&\longmapsto (e_{j-2} \otimes_{\bb{R}} 1) \otimes_{\bb{C}} (e_1e_2 \otimes_{\bb{R}} 1)\\
			&\label{eq:factor_of_i}\longmapsto ie_{j-2} \otimes_{\bb{C}} e_1e_2\\
			&\longmapsto ie_{j-2} \otimes_{\bb{C}} g_1g_2
		\end{align}
	\end{proof}
	\begin{remark}
		In yet more detail, concentrating on $C_n' \otimes_{\bb{C}} \cong C_n^{\bb{R}}\bb{C}$, we asserted this in Lemma \ref{lem:complexification} and justified it by saying that all non-degenerate bilinear forms over $\bb{C}^n$ are equivalent. Unwinding this, we see that the bilinear form corresponding to $C_n'$ is $-I$ when written with respect to the basis $e_1,...,e_n$ of $\bb{C}^n$. Writing this with respect to the basis $ie_1,...,ie_n$, the matrix representation is $I$,which explains where the factor of $i$ in \eqref{eq:factor_of_i} comes from.
	\end{remark}
	\begin{lemma}\label{lem:case_one}
		We have:
		\begin{equation}
			C_1^{\bb{C}} \cong C_1 \otimes_{\bb{R}}\bb{C} \cong \bb{C} \oplus \bb{C}
		\end{equation}
	\end{lemma}
	\begin{proof}
		The vector space $\bb{R}$ is 1-dimensional, so we are in the same situation as the proof of Proposition \ref{prop:decomp_even_odd}, we use similar logic. We have that $C_1 \cong C_1^0 \oplus C_1^1$ where $C_1^0 \cong \bb{R}$ and $C_1^1 \cong \bb{R}\cdot e$ where $e$ is some formal variable. This is subject to the single relation $e^2 = -1$ and so $C_1 \cong \bb{C}$. The result follows.
	\end{proof}
	\begin{thm}\label{thm:structure_clifford}
		\begin{itemize} There is the following decomposition:
			\item If $n = 2k$ is even,
			\begin{equation}
				C_n^{\bb{C}} \cong M_2(\bb{C}) \otimes \hdots \otimes M_2(\bb{C}) \cong \operatorname{End}(\bb{C}^2 \otimes \hdots \otimes \bb{C}^2) \cong \operatorname{End}(\bb{C}^{2^k})
			\end{equation}
			given explicitly by the following, we make use of the function
			\begin{align*}
				\alpha(j) = \begin{cases}
					1,& j\text{ odd},\\
					2, & j\text{even}
				\end{cases}
			\end{align*}
			\begin{equation}
				e_j \longmapsto I \otimes \hdots \otimes I \otimes g_{\alpha(j)} \otimes T \otimes \hdots \otimes T
			\end{equation}
			\item if $n = 2k+1$ is odd,
			\begin{equation}
				C_n^{\bb{C}} \cong \operatorname{End}(\bb{C}^{2^k}) \oplus \operatorname{End}(\bb{C}^{2^k})
			\end{equation}
		\end{itemize}
	\end{thm}
	
	\section{Matrix factorisations}
	\begin{defn}
		A morphism of $\bb{Z}_2$-graded algebras $\varphi:X \lto Y$ is \textbf{odd} if $\varphi(X_i) \subseteq Y_{i+1}$ (where subscripts are taken modulo $2$).
	\end{defn}
	\begin{defn}
		Let $R$ be a ring and $W \in R$ some element. A \textbf{matrix factorisation of $(R,W)$} consists of a free $\bb{Z}_2$-graded algebra $X$ along with an odd morphism $\partial: X \lto X$ such that
		\begin{equation}
			\partial^2 = W\operatorname{id}_R
		\end{equation}
	\end{defn}
	\begin{example}
		Let $R$ be the ring $\bb{C}[x_1,x_2]$ and $W$ the polynomial $x_1^2 + x_2^2$. A matrix factorisation is given by the ring $R \oplus R$ along with the morphism
		\begin{equation}
			\partial := 
			\begin{pmatrix}
				0 & x_1 - ix_2\\
				x_1 + ix_2 & 0
			\end{pmatrix}
		\end{equation}
		notice that $\partial^2 = (x_1^2 + x_2^2)\operatorname{id}_R$.
	\end{example}
	
	
	
	
	
	
	
	
	
	
	
	\begin{thebibliography}{9}
		\bibitem{hartshorne} Robin Hartshorne, \emph{Algebraic Geometry}, Springer-Verlag New York 1977
		
		\bibitem{stacksproject} Stacks Project \url{https://stacks.math.columbia.edu/}
		
		\bibitem{murfet_hensel} Hensel's Lemma \url{http://therisingsea.org/notes/HenselsLemma.pdf}
		
		\bibitem{Zariski} Commutative Algebra, \emph{O. Zariski, P. Samuel} D. Van Nostrand Company (Canada), LTD 1958
		
		\bibitem{atiyah_macdonald} Commutative Algebra, \emph{Atiyah, MacDonald} Addison-Wesley Publishing Company 1969
		
		\bibitem{intro_hom_alg} \emph{Introduction to Homological Algebra (note)}, W. Troiani.
		
		\bibitem{Friedrich},\emph{Dirac Operations in Riemannian Geometry} Friedrich
		
	\end{thebibliography}
	
\end{document}
