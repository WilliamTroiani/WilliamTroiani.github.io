\documentclass[12pt]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{units}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{nicefrac}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{color}
\usepackage{tensor}
\usepackage{tipa}
\usepackage{bussproofs}
\usepackage{ stmaryrd }
\usepackage{ textcomp }
\usepackage{leftidx}
\usepackage{afterpage}
\usepackage{varwidth}
\usepackage{physics}

\newcommand\blankpage{
    \null
    \thispagestyle{empty}
    \addtocounter{page}{-1}
    \newpage
    }

\graphicspath{ {images/} }

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[subsection] % reset theorem numbering for each chapter
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{fact}[thm]{Fact}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers
\newtheorem{notation}[thm]{Notation}
\newtheorem{remark}[thm]{Remark}
\newtheorem{condition}[thm]{Condition}
\newtheorem{question}[thm]{Question}
\newtheorem{construction}[thm]{Construction}
\newtheorem{exercise}[thm]{Exercise}
\newtheorem{example}[thm]{Example}
\newtheorem{observation}[thm]{Observation}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\scr}[1]{\mathscr{#1}}
\newcommand{\call}[1]{\mathcal{#1}}
\newcommand{\psheaf}{\text{\underline{Set}}^{\scr{C}^{\text{op}}}}
\newcommand{\und}[1]{\underline{\hspace{#1 cm}}}
\newcommand{\adj}[1]{\text{\textopencorner}{#1}\text{\textcorner}}
\newcommand{\comment}[1]{}
\newcommand{\lto}{\longrightarrow}
\newcommand{\im}{\operatorname{im}}
\newcommand{\spec}{\operatorname{Spec}}

\usepackage[margin=1cm]{geometry}

\title{Notes on Linear Algebra}
\author{Will Troiani}
\date{\today}

\begin{document}

\maketitle

\section{Trace}

\begin{lemma}[Gram-Schmidt process]\label{lem:gram_schmidt}
Every Hilbert space $\bb{H}$ admits an orthonormal basis. Moreover, there is an algorithm which converts a (not necessarily orthonormal) basis for $\bb{H}$ into an orthonormal basis.
\end{lemma}
\begin{proof}
Every Hilbert space admits a basis so it suffices to prove the second claim.

Let $\ket{v_1},...,\ket{v_n}$ be a basis for $\bb{H}$ and inductively define the following set of vectors:
\begin{equation}
\ket{u_1} := \ket{v_i}/||\ket{v_i}||,\qquad
\ket{u_i} := \frac{\ket{v_i} - \sum_{k = 1}^{i-1}\bra{u_k}\ket{v_i}\ket{u_k}}{||\ket{v_i} - \sum_{k = 1}^{i-1}\bra{u_k}\ket{v_i}\ket{u_k}||}
\end{equation}
These are clearly normal vectors which span all of $\bb{H}$, we prove orthogonality by induction on $j$, the key calculation is the following: let $i < j$
\begin{align*}
||\ket{u_j}|| ||\ket{u_i}||\bra{u_j}\ket{u_i} &= (\bra{v_i} - \sum_{k = 1}^{i-1}\overline{\bra{u_k}\ket{v_i}}\bra{u_k})\ket{u_i}\\
&= \bra{v_i}\ket{u_i} - \sum_{k = 1}^{i-1}\overline{\bra{u_k}\ket{v_i}}\bra{u_k}\ket{u_i}\\
&= \bra{v_i}\ket{u_i} - \overline{\bra{u_i}\ket{v_i}}\\
&= 0
\end{align*}
\end{proof}



\section{Matrix algebra}
Observe the following calculation, where all entries are elements of some ring $R$:
\begin{equation}
    \begin{bmatrix}
    a_{11} & \hdots & a_{1n}\\
    \vdots & \ddots & \vdots\\
    a_{m1} & \hdots & a_{mn}
    \end{bmatrix}
    \begin{bmatrix}
    x_1\\
    \vdots\\
    x_n
    \end{bmatrix}
    =
    \begin{bmatrix}
    a_{11}x_1 + \hdots + a_{1n}x_n\\
    \vdots\\
    a_{m1}x_1 + \hdots + a_{mn}x_n
    \end{bmatrix}
    =
    x_1
    \begin{bmatrix}
    a_{11}\\
    \vdots\\
    a_{m1}
    \end{bmatrix}
    + \hdots +
    x_n
    \begin{bmatrix}
    a_{1n}\\
    \vdots\\
    a_{mn}
    \end{bmatrix}
\end{equation}
it follows that the image of a linear transformation $T$ is given by the span of the columns of the matrix representing $T$. This is where the importance of the \emph{column space} comes from.

Here is another important calculation: let a matrix $M$ have columns $c_1,...,c_n$ and assume these are linearly dependent, write
\begin{equation}\label{eq:linearly_dependent}
    \alpha_1c_1 + \hdots + \alpha_nc_n = 0
\end{equation}
where not all $\alpha_i$ are zero. If we multiply row $i$ of $M$ by a scalar $\lambda$ then
\begin{equation}
    \lambda \alpha_1 c_{1i} + \hdots + \lambda \alpha_n c_{ni} = \lambda (\alpha_1c_{1i} + \hdots + \alpha_nc_{ni}) = 0
\end{equation}
and so linear dependence is preserved. Also, if we interchange two rows $i \neq j$ then linear dependence is still preserved. Finally, if we replace row $i$ with $\lambda R_j + R_i$, where $R_k$ denotes the $k^{\text{th}}$ row of $M$, we have
\begin{equation}
    \alpha_1 (\lambda c_{1j} + c_{1i}) + \hdots + \alpha_n(\lambda c_{nj} + c_{ni}) = \lambda (\alpha_1c_{1j} + \hdots + \alpha_nc_{nj}) + \alpha_1c_{1i} + \hdots + \alpha_nc_{ni} = 0
\end{equation}
We have proved the first half of:
\begin{proposition}\label{prop:row_ops_lin_dep}
Elementary row operations preserve linear dependence and independence of columns.
\end{proposition}
\begin{proof}
Easy.
\end{proof}
\begin{proposition}\label{prop:number_independence}
Let $M$ be a matrix, the largest number of linearly independent columns is equal to the largest number of linearly independent rows.
\end{proposition}
\begin{proof}
Let $M'$ be a matrix is row echelon form which is equivalent under row reduction to $M$. Then the number of non-zero rows of $M'$ is equal to the number of columns with leading 1s which in turn is equal to the number of linearly independent columns of $M'$. Clearly, the number of linearly independent rows of $M$ and the number of linearly independent rows of $M'$ agree, and that the same claim holds for the columns of $M$ and $M'$ follows from Proposition \ref{prop:row_ops_lin_dep}.
\end{proof}
\begin{defn}
By Proposition \ref{prop:number_independence}, the greatest integer $r$ such that a matrix $M$ has $r$ linearly independent rows/columns is a well defined notion, which we call the \textbf{rank of $M$}, denoted $\operatorname{rank}M$.
\end{defn}
\begin{example}
Let $Y \subseteq \bb{A}^n$ be an affine variety, $P \in Y$ a point, and $f_1,...,f_m$ generators for the ideal $IY$. The linear transformation
\begin{align*}
    \theta: IY &\lto k^n\\
    f &\longmapsto \Big(\frac{\partial f}{\partial x_1}(P),...,\frac{\partial f}{\partial x_n}(P)\Big)
\end{align*}
is such that $\operatorname{dim}\operatorname{im}\theta = \operatorname{rank}||(\partial f_i/\partial x_j)(P)||$. This is because
\begin{equation}
    \operatorname{im}\theta = \operatorname{Span}(\theta f_1,...,\theta f_n)
\end{equation}
and consider the matrix $M$ whose $i^{\text{th}}$ row is $\theta f_i$ (ie, $M = ||(\partial f_i/\partial x_j)(P)||$). We can reduce $M$ to row echelon form (as $k$ is a field) which results in a matrix $M'$ whose number $d$ of non-zero rows is the dimension of $\operatorname{\im}\theta$. The last observation to make is that $d$ is exactly $\operatorname{rank}M$.
\end{example}
\begin{defn}
Given an $n \times m$ matrix $M$ and $r \leq \operatorname{min}\lbrace n,m\rbrace$, an \textbf{$r$-minor} of $M$ is an $r\times r$ submatrix of $M$ given by deleting rows and/or columns.
\end{defn}
\begin{cor}[Corollary of Proposition \ref{prop:number_independence}]\label{cor:det_minor}
The rank of a matrix $M$ is equal to the greatest integer $r$ for which there exists an $r$-minor $N$ of $M$ with $\operatorname{det}N \neq 0$.
\end{cor}
Corollary \ref{cor:det_minor} also uses the following observation:
\begin{observation}
Let $M$ be a matrix whose columns are linearly dependent, then $\operatorname{det}M = 0$.
\end{observation}
\begin{proof}
If $M'$ is equivalent to $M$ under row reduction then there is at least one zero row of $M'$ and so $\operatorname{det}M' = 0$ which implies $\operatorname{det}M = 0$.
\end{proof}
\subsection{Determinant}
We have:
\begin{proposition}
If $M$ is a square $n \times n$ matrix with entries in some field $k$ then
\begin{itemize}
\item for any $l \in k$:
\begin{equation}\label{eq:scalar_det}
\operatorname{det}(l M) = l^n \operatorname{det}M
\end{equation}
\item If $N$ is another such square matrix then:
\begin{equation}\label{eq:det_multiplicative}
\operatorname{det}(MN) = \operatorname{det}M\operatorname{det}N
\end{equation}
\end{itemize}
\end{proposition}
\begin{proof}
The first claim is obvious.

First say $M$ is non-invertible. Then $MN$ is also non-invertible and so both sides of \eqref{eq:det_multiplicative} are zero.

Now say that $M$ is invertible. We use the fact that $M$ can be written as a product of elementary matrices, this follows easily from the fact that the reduced row echelon form of an invertible matrix is the identity. It now suffices to show that for any elementary matrix $E$,
\begin{equation}
\operatorname{det}(EM) = \operatorname{det}E\operatorname{det}M
\end{equation}
The result then follows from a case analysis on $E$.
\end{proof}

\section{Diagonalisation}
Throughout, we assume $k$ is a field.
\begin{defn}\label{def:diagonalisable}
A square matrix $A$ with coefficients in $k$ is \textbf{diagonalisable} if there exists an invertible matrix $P$ such that $PAP^{-1}$ is diagonal.

Let $V$ be some $k$-vector space. A linear transformation $T: V \lto V$ is \textbf{diagonalisable} if there exists a basis with respect to which the matrix representation of $T$ is diagonal.
\end{defn}
\begin{remark}
A linear transformation $T: V \lto V$ being diagonalisable is equivalent to the statement that the matrix representation of $T$ (with respect to any basis) is diagonalisable. This is clumsy though because we are then saying $T$ with respect to any basis can be written diagonally if we change the basis...
\end{remark}
Clearly, we have:
\begin{lemma}\label{lem:diagonalisable}
Let $V$ be some $k$-vector space.  A linear transformation $T: V \lto V$ is diagonalisable if there exists a basis for $V$ consisting of eigenvectors for $T$. The diagonal entries will then be the eigenvalues.
\end{lemma}
We wish to prove the following:
\begin{proposition}\label{prop:linear_indep_dinstinct_eig}
Let $T: V \lto V$ be a linear transformation and $v_1,...,v_k$ a set of eigenvectors with eigenvalues $\lambda_1,...,\lambda_k$. If all of $\lambda_1,...,\lambda_k$ are distinct then the set $v_1,...,v_k$ is linearly independent.
\end{proposition}
\begin{proof}
Assume $v_1,...,v_k$ is linearly dependent, let $j$ be such that $v_1,...,v_{j}$ is linearly dependent but $v_1,...,v_{j-1}$ is linearly independent (reordering if necessary). Write
\begin{equation}\label{eq:zero_equation}
\alpha_1 v_1 + \hdots + \alpha_j v_j = 0
\end{equation}
Applying $T$ to both sides of \eqref{eq:zero_equation} we obtain:
\begin{equation}\label{eq:applying_T}
\alpha_1 \lambda_1 v_1 + \hdots + \alpha_j \lambda_j v_j = 0
\end{equation}
and multiplying \eqref{eq:zero_equation} by $\lambda_j$ we obtain
\begin{equation}\label{eq:mult_lambda}
\alpha_1\lambda_k v_1 + \hdots + \alpha_j \lambda_j v_j = 0
\end{equation}
Subtracting \eqref{eq:mult_lambda} from \eqref{eq:applying_T} we obtain
\begin{equation}
\alpha_1(\lambda_1 - \lambda_k)v_1 + \hdots + \alpha_{j-1}(\lambda_{j - 1} - \lambda_j)v_{j-1} = 0
\end{equation}
which by linear independence implies for all $i = 1,...,j-1$ that
\begin{equation}
\alpha_i(\lambda_1 - \lambda_i) = 0
\end{equation}
That all eigenvalues are distinct then implies $\alpha_i = 0$ for $i = 1,...,j-1$ which then implies $\alpha_j = 0$, a contradiction.
\end{proof}
\begin{remark}
The proof of Proposition \ref{prop:linear_indep_dinstinct_eig} is probably more naturally written as an inductive argument.
\end{remark}
\subsection{Jordan normal form}\label{sec:Jordan_normal_form}
Wikipedia described a process for constructing a linearly independent set of \emph{generalised eigenvectors} with respect to a linear transformation $T: V \lto V$. With respect to this basis, the matrix representation of $T$ consists of \emph{eigenblocks}. Corollaries of this includes that the product of eigenvalues is the determinant of the matrix, and the sum is the trace.  We also arrive at the fact that if the \emph{minimal polynomial} of $T$ splits into linear factors, then $T$ is diagonalisable. In particular, this implies that any \emph{involution} (a linear transformation $T: V \lto V$ such that $T^2 = \operatorname{id}_V$) is diagonalisable, as $T$ satisfies the polynomial $x^2 - 1 = (x+1)(x-1)$. Hence, any vector space $V$ which admits an involution $T$ decomposes $V \cong V_0 \oplus V_1$ where $V_0 := \lbrace v \in V \mid Tv = v\rbrace$ and $V_1 := \lbrace v \in V \mid Tv = -v\rbrace$.


\section{Quadratic forms}
Throughout, $V$ is a finite dimensional vector space over a field $k$.

In Linear Algebra, we study morphisms between vector spaces which (given a fixed basis) are given by matrix multiplication:
\begin{equation}
T: V \lto V,\qquad T(x_1,...,x_n) = 
    \begin{bmatrix}
    a_{11} & \hdots & a_{1n}\\
    \vdots& & \vdots\\
    a_{n1} & \hdots & a_{nn}
    \end{bmatrix}
    \begin{bmatrix}
    x_1\\
    \vdots\\
    x_n
    \end{bmatrix}
\end{equation}
A \textbf{bilinear form} is a bilinear map $T: V \times V \lto k$ (where $k$ is the bass field of $V$), which (again, given a fixed basis) can always be written in matrix form:
\begin{equation}
    T: V \times V \lto k,\qquad T\big((x_1,...,x_n),(y_1,...,y_n)\big) =
    \begin{bmatrix}
    y_1 & \hdots & y_n
    \end{bmatrix}
    \begin{bmatrix}
    a_{11} & \hdots & a_{1n}\\
    \vdots & & \vdots\\
    a_{n1} & \hdots & a_{nn}
    \end{bmatrix}
    \begin{bmatrix}
    x_1\\
    \vdots\\
    x_n
    \end{bmatrix}
\end{equation}
First we prove that such a representation always exists.

Let $V$ be finite dimensional and let $v_1,...,v_n$ a basis for $V$. Let $a_{ij} = T(v_i,v_j)$. Then for any pair of vectors $v = \sum_{i = 1}^na_{i}v_i$, $u = \sum_{i = 1}^nb_iv_i$ we have
\begin{align*}
    T(v,u) &= T(\sum_{i = 1}^n a_i v_i, \sum_{i = 1}^nb_i v_i)\\
    &= \sum_{i = 1}^n\sum_{j = 1}^na_ib_jT(v_i, v_j)\\
    & = \sum_{i = 1}^n\sum_{j = 1}^n a_ib_j a_{ij}
\end{align*}
and
\begin{align*}
    \begin{bmatrix}
    a_1 & \hdots & a_n
    \end{bmatrix}
    \begin{bmatrix}
    a_{11} & \hdots & a_{1n}\\
    \vdots & & \vdots\\
    a_{n1} & \hdots & a_{nn}
    \end{bmatrix}
    \begin{bmatrix}
    b_1\\
    \vdots\\
    b_n
    \end{bmatrix}
    &=
    \begin{bmatrix}
    a_1 & \hdots & a_n
    \end{bmatrix}
    \begin{bmatrix}
    \sum_{j = 1}^n a_1j b_j\\
    \vdots\\
    \sum_{j = 1}^n a_{nj} b_j
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
    \sum_{i = 1}^n\sum_{j = 1}^n a_ia_{ij}b_j
    \end{bmatrix}
\end{align*}
The result follows by commutativity.

\begin{defn}
A bilinear form $B: V \times V \lto k$ is \textbf{symmetric} if for all $v,u \in V$ we have $B(v,u) = B(u,v)$.
\end{defn}

\begin{defn}
A \textbf{quadratic form} is a function $Q: V \lto k$ satisfying the following properties:
\begin{itemize}
    \item for all $a \in k$ and $v \in V$, we have $Q(av) = a^2Q(v)$,
    \item the function $B: V \times V \lto k$ given by $B(v,u) = Q(v + u) - Q(v) - Q(u)$ is linear.
\end{itemize}
\end{defn}
\begin{example}
The prototypical example is given using the dot product: let $V = \bb{R}^n$ and define $Q(x_1,...,x_n) = \sum_{i = 1}^n x_i^2$.
\end{example}
Bilinear forms and quadratic forms are closely related, as is shown by the following Proposition:
\begin{proposition}
Let $B: V \times V \lto k$ be a symmetric bilinear form and $k$ a field of characteristic not equal to $2$. Then the function $Q_B: V \lto k$ given by $Q(v) = B(v,v)$ is a bilinear form.

Also, given a quadratic form $Q: V \lto k$, the function $B_Q: V \times V \lto k$ given by $B_Q(v,u) = \frac{1}{2}\big(Q(v + u) - Q(v) - Q(u)\big)$ is a bilinear form.

Furthermore, the maps $B \lto Q_B$ and $Q \lto B_Q$ are mutually inverse bijections.
\end{proposition}
\begin{proof}
We have
\begin{align*}
    B_{Q_B}(v,u) &= \frac{1}{2}\big(Q_B(v + u) - Q_B(v) - Q_B(u)\big)\\
    &= \frac{1}{2}\big(B(v+u,v+u) - B(v,v) - B(u,u)\big)\\
    &= \frac{1}{2}\big(B(v,u) + B(v,u)\big)\\
    &= B(v,u)
\end{align*}
where we use symmetry in the final line.

Also,
\begin{align*}
    Q_{B_Q}(v) &= B_Q(v,v)\\
    &= \frac{1}{2}\big(Q(v + v) - Q(v) - Q(v)\big)\\
    &= Q(v)
\end{align*}

The rest follows in a similar, simple fashion.
\end{proof}
\begin{example}
The assumption that $k$ have characteristic not equal to 2 is crucial: let $\bb{F}_2$ denote the field with two elements and consider the following quadratic form:
\begin{align*}
Q: \bb{F}^2_2 &\lto \bb{F}_2\\
(x,y) &\longmapsto xy
\end{align*}
we claim that this cannot arise as $Q_B$ for any symmetric bilinear form $B: \bb{F}^2_2 \times \bb{F}^2_2 \lto \bb{F}_2$. To see this, notice that
\begin{align*}
Q_B(x,y) &= B\big((x,y),(x,y)\big)\\
&= x^2B\big((1,0),(1,0)\big) + yxB\big((0,1),(1,0)\big) + xyB\big((1,0),(0,1)\big) + y^2B\big((0,1),(0,1)\big)\\
&= x^2B\big((1,0),(1,0)\big) + 2xyB\big((0,1),(1,0)\big) + y^2B\big((0,1),(0,1)\big)\\
&= x^2B\big((1,0),(1,0)\big) + y^2B\big((0,1),(0,1)\big)\\
&= x^2Q_B(1,0) + y^2Q_B(0,1)
\end{align*}
which does not fit the form of $Q$. Thus in the characteristic 2 case, there are quadratic forms which do not arise from symmetric bilinear forms.
\end{example}
\subsection{Clifford algebras}
\subsection{Graded, differential algebras}
We generalise the concept of polynomial algebras to \emph{differential, graded algebras}.
\begin{defn}
Let $G$ be a totally ordered, abelian group, a \textbf{$G$-graded algebra} is an algebra $A$ along with a decomposition
\begin{equation}
G \cong \bigoplus_{g \in G}A_g
\end{equation}
satisfying $A_gA_{h} \subseteq A_{g+h}$.

A nonzero element of $A_g$ is a \textbf{homogeneous element of degree $g$} and we write $\operatorname{deg}a = g$ for $a \in A_g$.
\end{defn}
\begin{defn}
A \textbf{differential $\bb{Z}$-graded algebra} is a $\bb{Z}$-graded algebra $A := \bigoplus_{i \in \bb{Z}}A_i$ along with a \textbf{differential} $d: A \lto A$ satisfying
\begin{equation}
\operatorname{deg}d(a) = \operatorname{deg}a - 1
\end{equation}
\end{defn}
The following Definitions are justified by Remark ??????

\begin{defn}
Let $A \cong \bigoplus_{i \in \bb{Z}}A_i,B \cong \bigoplus_{i \in \bb{Z}}B_i$ be graded $k$-algebras (where $k$ is some ring). The \textbf{graded tensor product} $A \otimes B$ is the normal tensor product of $k$-modules but with multiplication defined on pure tensors as:
\begin{equation}
(a_1\otimes_k b_1)(a_2 \otimes_k b_2) = (-1)^{\operatorname{deg}a_2\operatorname{deg}b_1}(a_1a_2 \otimes_k b_1b_2)
\end{equation}
The reason for this definition is given in Remark ?????

The $k$-algebra $\operatorname{Hom}_k(A,B)$ is also naturally a graded algebra. For $n \in \bb{Z}$ we define
\begin{equation}
\operatorname{Hom}_k(A,B)_n := \lbrace f \in \operatorname{Hom}_k(A,B) \mid \forall m \in \bb{Z}, \forall a \in A_m, \operatorname{deg}f(a) = n + \operatorname{deg}a\rbrace
\end{equation}

In fact this is a graded algebra with grading
\begin{equation}
A \otimes B \cong \bigoplus_{\substack{i \in \bb{Z}\\ n + m = i}}A_n \oplus B_m
\end{equation}
Furthermore, say $A,B$ were \emph{differential}, graded algerbas with respective differentials $d_A,d_B$.  Then $A \otimes B$ is also differential graded with differential given on pure tensors by
\begin{equation}
d_{A \otimes B}(a \otimes b) = d_A(a) \otimes b + (-1)^{\operatorname{deg}a\operatorname{deg}b}a \otimes d_B(b)
\end{equation}
\end{defn}







\end{document}































